{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc3463a9-8d3b-4be0-be98-15abc5dead94",
   "metadata": {},
   "source": [
    "## Social Distance Monitoring and Multiple People Localization with low resolution IR data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df23481-e5bd-4770-8cf5-1e8f5b879f9d",
   "metadata": {},
   "source": [
    "This notebook is designed to facilitate the use of low-resolution infrared data from www.kaggle.com/andreyperov/lowresir-detect-and-distance for non-invasive social monitoring and the localization of multiple individuals through deep learning techniques. The dataset was developed as part of a research project at Leuphana University of LÃ¼neburg, and is discussed in the conference paper titled \"Privacy-Preserving Localization and Social Distance Monitoring with Low-Resolution Thermal Imaging and Deep Learning,\" authored by Andrei Perov and Jens Heger."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c37d4a2-a73b-4f57-a609-4da4d0aca7cc",
   "metadata": {},
   "source": [
    "The dataset is split into:\n",
    "\n",
    "- Social Distance Violation Detection: 98,000 training frames and 25,500 testing frames are categorized into four classes based on the number of observed distance rule violations (less than 1.5 meters between two given individuals).\n",
    "- People Localization: 42,800 frames for training and 11,100 for testing. Each row contains the list of coordinates representing the location of the heads of the people on the frame.\n",
    "Data are stored in CSV format with each row corresponding to temperature readings for each frame. with 10% of the data presenting challenging scenarios for traditional methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d4883f-0e50-4e65-b81d-0531e59a5839",
   "metadata": {},
   "source": [
    "**Full code base with many different models and additional functions you can find in the github repository of the paper**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76ebec1-3988-4e53-9d9c-8ad1c580a330",
   "metadata": {},
   "source": [
    "**Please reference the repository in your codebase and include a citation in your written documentation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda7bc49-ac3c-4bc1-a203-a3e62e08a745",
   "metadata": {},
   "source": [
    "First lets import all the neccessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4a9348f5-2526-4045-ba69-2cbe5c02c968",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import datetime\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import torchvision.transforms.functional as TF\n",
    "import torch.optim as optim\n",
    "import pickle \n",
    "from sklearn.metrics import precision_recall_curve, auc, confusion_matrix, ConfusionMatrixDisplay\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils.data import TensorDataset, DataLoader, WeightedRandomSampler, Dataset, Subset\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import ast\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from sklearn.preprocessing import label_binarize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56e28fd-bbd7-45c7-b69b-abc31683b629",
   "metadata": {},
   "source": [
    "### 1. Social Distance Monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df349f0a-ff30-4665-b52f-657faffc6477",
   "metadata": {},
   "source": [
    "Download the datasets for social distance rule violation counting. The dataset exhi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38172056-c776-4779-bdb8-1e064ddf3fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('distance_test_11_06.csv')\n",
    "df_train = pd.read_csv('distance_train_09_06.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14958935-d537-45bc-9e77-7e76664d653d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff02337-4ac6-4fc3-a49e-0770942c8063",
   "metadata": {},
   "source": [
    "To process the corresponding data for subsequent modelling we need following functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d45664-227f-45c6-b00b-6b370b82059b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data_distancing(df, interpolate=True):\n",
    "    pixel = df.columns[0:64]\n",
    "    sessions_unique = df['session'].unique()\n",
    "    num_sessions = len(sessions_unique)\n",
    "\n",
    "    # Preallocate a list of lists\n",
    "    session_arrays = [None] * num_sessions\n",
    "    \n",
    "    for i, session_label in enumerate(sessions_unique):\n",
    "        df_temp = df[df['session'] == session_label]\n",
    "        # Preallocate list for tuples (image, label) for each image\n",
    "        pics_tuples = []\n",
    "\n",
    "        for idx, row in df_temp.iterrows():\n",
    "            # Extract pixel values and check if it can be reshaped to 8x8\n",
    "            pic = pd.to_numeric(row[pixel], errors='coerce').values\n",
    "            label = row['violations']\n",
    "            min_value = pic.min()\n",
    "            if pic.size == 64:\n",
    "                if interpolate:\n",
    "                    try:\n",
    "                        pic_reshaped = pic.reshape(1, 1, 8, 8)\n",
    "                    except Exception:\n",
    "                        continue\n",
    "                    pic_reshaped = torch.Tensor(pic_reshaped)\n",
    "                    pic_reshaped = F.interpolate(pic_reshaped, size=(16, 16), \n",
    "                                                 mode='bilinear', align_corners=None, \n",
    "                                                 recompute_scale_factor=None, antialias=False)\n",
    "                    pic_reshaped = pic_reshaped.cpu().detach().numpy().reshape(1, 16, 16)\n",
    "                else:\n",
    "                    pic_reshaped = pic.reshape((1, 8, 8))\n",
    "                pic_reshaped = pic_reshaped - min_value\n",
    "                pics_tuples.append((pic_reshaped, int(label)))\n",
    "            else:\n",
    "                raise ValueError(\"The image does not have exactly 64 pixels to reshape into 8x8\")\n",
    "\n",
    "        session_arrays[i] = pics_tuples\n",
    "        \n",
    "    return session_arrays\n",
    "\n",
    "\n",
    "def prepare_data_distancing(df, sequence=False, seq_len=8, interpolate = True):\n",
    "    data = format_data_distancing(df, interpolate = interpolate)\n",
    "    data_collection = []\n",
    "\n",
    "    if sequence:\n",
    "        for seq in data:\n",
    "            num_frames = len(seq)\n",
    "            for i in range(num_frames - seq_len + 1):\n",
    "                frames = np.concatenate([seq[j][0] for j in range(i, i + seq_len)])\n",
    "                label = seq[i + seq_len - 1][1]\n",
    "                data_collection.append((frames, label))\n",
    "    else:\n",
    "        for seq in data:\n",
    "            data_collection.extend(seq)\n",
    "    \n",
    "    return data_collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8b8edf-9de3-441b-abf7-f6f90a552312",
   "metadata": {},
   "source": [
    "Lets prepare the image data collection with the corresponding labels (amount of social distance rule violations present on the frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2688e962-6009-4e7e-917f-55387a13ae0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_train = prepare_data_distancing(df_train, interpolate = True)\n",
    "collection_test = prepare_data_distancing(df_test, interpolate = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2069d6a0-63ab-4b74-aa7d-689c3709e510",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is the path where you want to save the trained models\n",
    "path_to_save = \"D:/IR_blobs/IR_blobs_github/models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "982677ea-d5b8-4673-ac50-a806be44521a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing Torch datasets\n",
    "class SimpleTorchDataset(Dataset):\n",
    "    def __init__(self, data_collection):\n",
    "        self.data_collection = data_collection\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_collection)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        frames, label = self.data_collection[idx]\n",
    "        \n",
    "        # Ensure frames and label are numpy arrays of appropriate types\n",
    "        frames = np.array(frames, dtype=np.float32)\n",
    "        label = np.array(label, dtype=np.float32)\n",
    "        \n",
    "        # Convert to PyTorch tensors\n",
    "        frames = torch.tensor(frames, dtype=torch.float32)\n",
    "        label = torch.tensor(label, dtype=torch.float32)\n",
    "        \n",
    "        return frames, label\n",
    "\n",
    "\n",
    "def create_datasets(data_collection_train, data_collection_test):\n",
    "    # Creating PyTorch datasets\n",
    "    train_set = SimpleTorchDataset(data_collection_train)\n",
    "    test_set = SimpleTorchDataset(data_collection_test)\n",
    "    \n",
    "    return train_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d6c64bba-3fa7-4218-aa39-78c9a335d833",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = create_datasets(collection_train, collection_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3a1ca0cd-e9d7-457b-a89f-08f66b510cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the main hyperprameters \n",
    "batch_size = 64\n",
    "num_epochs = 200\n",
    "std = 0.02\n",
    "patience = 20\n",
    "learning_rate = 0.001\n",
    "class_number = 4\n",
    "code_word = f\"cnn_ir_bs{batch_size}__std{std}_lr{learning_rate}_pt{patience}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "114b63f7-30be-4a42-b714-f9fcff61ec9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#in the case of the class imbalance we oversample underrepresented classes\n",
    "def class_balancing(torch_dataset, batch_size, random_seed=42):\n",
    "    if random_seed is not None:\n",
    "        torch.manual_seed(random_seed)  # Fix the random seed for reproducibility\n",
    "\n",
    "    # Get the labels from the dataset\n",
    "    labels = np.array([int(torch_dataset[i][1].item()) for i in range(len(torch_dataset))])\n",
    "\n",
    "    # Compute class frequencies\n",
    "    class_counts = np.bincount(labels)\n",
    "\n",
    "    # Calculate class weights\n",
    "    class_weights = 1. / class_counts\n",
    "\n",
    "    # Assign weights to each sample based on its class\n",
    "    sample_weights = class_weights[labels]\n",
    "\n",
    "    # Create a WeightedRandomSampler\n",
    "    sampler = WeightedRandomSampler(sample_weights, len(sample_weights))\n",
    "\n",
    "    # Return a DataLoader with the WeightedRandomSampler\n",
    "    return DataLoader(torch_dataset, batch_size=batch_size, sampler=sampler)\n",
    "\n",
    "#create a Pytorch dataloader\n",
    "def dataset_to_loader(torch_dataset, batch_size, balancing = True, shuffle = True):\n",
    "    if balancing == True:\n",
    "        loader = class_balancing(torch_dataset, batch_size)   \n",
    "    else:\n",
    "        loader = DataLoader(torch_dataset, batch_size = batch_size, shuffle=shuffle)\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee647755-945f-4582-9647-8dae926a0b8d",
   "metadata": {},
   "source": [
    "Next we created test and training dataloaders. The dataloaders split the dataset to batches according to the rules we preditermined for it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "070b6bec-a347-474a-8f6f-3e18c609656c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = dataset_to_loader(train_set, batch_size, balancing = True, shuffle = True)\n",
    "test_loader = dataset_to_loader(test_set, batch_size, balancing = False, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089187d8-d451-4448-b243-25eaea870c40",
   "metadata": {},
   "source": [
    "Here we create a class with selected architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0edd5036-d2d3-4569-b63d-61607aeb43d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#big version CNN-IR with 3 by 3 kernel for the first and second layers and extra fc layer examined in the paper\n",
    "class CNN_IR_7by7_3fc(nn.Module):\n",
    "    def __init__(self, class_number):\n",
    "        super().__init__() \n",
    "        self.conv1 = nn.Conv2d(1, 16, 7, 1, 3)#in 16*16 - out 16*16 \n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 8, 7, 1, 2)# 16*16 - 14*14\n",
    "        self.bn2 = nn.BatchNorm2d(8)\n",
    "        self.conv3 = nn.Conv2d(8, 4, 5)# 14*14 - 10*10\n",
    "        self.bn3 = nn.BatchNorm2d(4)\n",
    "        self.conv4 = nn.Conv2d(4, 4, 5)# 10*10 - 6*6\n",
    "        self.bn4 = nn.BatchNorm2d(4)\n",
    "        self.fc1 = nn.Linear(36*4, 32) \n",
    "        self.fc2 = nn.Linear(32, 16)\n",
    "        self.fc3 = nn.Linear(16, class_number)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "60de8274-2408-4ca9-bd56-1727ee05f49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN_IR_7by7_3fc(class_number = class_number)\n",
    "# Move the model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a1575bee-3f2e-4cbc-906e-2ea9d3e97d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13588\n"
     ]
    }
   ],
   "source": [
    "# Calculate the number of parameters\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dab76dde-3093-4bd8-88f4-452ff8deeb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#determine the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fdaab4bf-dd61-45a8-affb-7dda506b89f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "#Data augmentations:\n",
    "def RandomHorizontalFlip(sample, prob, dim = 2):\n",
    "    number = random.random()\n",
    "    state = False\n",
    "    if number <= prob:\n",
    "        state = True\n",
    "        return torch.flip(sample, (dim,)), state\n",
    "    else:\n",
    "        return sample, state\n",
    "\n",
    "def RandomVerticalFlip(sample, prob, dim = 3):\n",
    "    number = random.random()\n",
    "    state = False\n",
    "    if number <= prob:\n",
    "        state = True\n",
    "        return torch.flip(sample, (dim,)), state\n",
    "    else:\n",
    "        return sample, state\n",
    "\n",
    "def AddValue(frame):\n",
    "    tensor_ones = torch.ones(frame.shape)\n",
    "    integer_ = random.randint(0, 2)\n",
    "    float_ = random.random()\n",
    "    return frame + tensor_ones.to(device)*(integer_ + float_)\n",
    "\n",
    "class AddGaussianNoise:\n",
    "    def __init__(self, mean=0., std=0.5):\n",
    "        self.std = std\n",
    "        self.mean = mean\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        return tensor + torch.randn(tensor.size()).to(device) * self.std + self.mean\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)\n",
    "\n",
    "\n",
    "\n",
    "def apply_augmentation(inputs, add_noise = True, dim = (2, 3), add_val = False, std = 0.05):\n",
    "    noise = AddGaussianNoise(mean=0., std=std)\n",
    "    inputs, state_hor = RandomHorizontalFlip(inputs, 0.5, dim[0])\n",
    "    inputs, state_ver = RandomVerticalFlip(inputs, 0.5, dim[1])\n",
    "    if add_noise:\n",
    "        inputs = noise(inputs)\n",
    "    if add_val:\n",
    "        inputs = AddValue(inputs)\n",
    "    return inputs, (state_hor, state_ver)\n",
    "\n",
    "#we need to flip labels for localization task only\n",
    "def flip_labels(labels, states):\n",
    "    state_hor, state_ver = states\n",
    "    if state_hor:\n",
    "        labels = torch.flip(labels, (1,))\n",
    "    if state_ver:\n",
    "        labels = torch.flip(labels, (2,))\n",
    "    return labels\n",
    "\n",
    "#The main training fuction for social distancing:\n",
    "def network_training(net, train_loader, optimizer, criterion, \n",
    "                     num_epochs, code_word, path_to_save, interpolate = True, patience=20, add_noise = True, factor=0.4, norm_grid = False, add_val = True, std = 0.01):\n",
    "    epoch_acc_max = 0\n",
    "    patience_counter = 0  # Early stopping counter\n",
    "    start_time = time.time()\n",
    "    loss_batch = []\n",
    "    loss_epoch = []\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=patience//2, factor=factor)\n",
    "    model_path = f'{path_to_save}/model_{code_word}'\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f'current learning rate:{current_lr}')\n",
    "    for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        running_loss_epoch = 0.0\n",
    "        net.train()\n",
    "        running_corrects = 0\n",
    "        #to log learning rate:\n",
    "        if patience_counter >= patience//2:\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            print(f'current learning rate:{current_lr}')       \n",
    "        #wandb.log({\"learning_rate\": current_lr})\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            inputs, _ = apply_augmentation(inputs.to(device), add_noise = add_noise, std = std)\n",
    "            if norm_grid:\n",
    "                inputs = optimized_gridwise_normalize(inputs)\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            # Ensure labels are of type LongTensor\n",
    "            labels = labels.long()\n",
    "            #zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            with torch.set_grad_enabled(True):\n",
    "                outputs = net(inputs).to(device)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                #print(outputs)\n",
    "                #print(labels)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            # print training statistics\n",
    "            running_loss += loss.item()\n",
    "            running_loss_epoch += loss.item()\n",
    "            running_corrects += torch.sum(preds == labels)\n",
    "            if i % 200 == 199:    # print every 200 mini-batches\n",
    "                loss_batch += [running_loss / 200]\n",
    "                running_loss = 0.0\n",
    "        epoch_acc = running_corrects.double() / len(train_loader.dataset)\n",
    "        scheduler.step(epoch_acc)\n",
    "        epoch_loss = running_loss_epoch/ len(train_loader.dataset)\n",
    "        loss_epoch.append(epoch_loss)\n",
    "        #wandb.log({'epoch': epoch, 'loss': epoch_loss})\n",
    "        running_loss_epoch = 0.0\n",
    "        if epoch_acc > epoch_acc_max:\n",
    "            epoch_acc_max = epoch_acc\n",
    "            patience_counter = 0\n",
    "            torch.save(net.state_dict(), model_path)\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        print(f'Epoch {epoch}: {epoch_acc}')\n",
    "        if patience_counter >= patience:\n",
    "            print(f'Early stopping triggered at epoch {epoch}.')\n",
    "            break  # Stop training if patience limit is reached\n",
    "    training_time = time.time() - start_time\n",
    "    print(f'time of training: {training_time}')\n",
    "    print(f'max accuracy: {epoch_acc_max}')\n",
    "    #log the model\n",
    "    #wandb.log_artifact(model_path, type=\"model\")\n",
    "    \n",
    "    return net, loss_batch, loss_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3271b6-c03e-4429-aa03-d7d22d87d964",
   "metadata": {},
   "source": [
    "Lets train our CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "41a45309-6457-4ade-898a-b651d1d367eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current learning rate:0.001\n",
      "Epoch 0: 0.6244984517966129\n",
      "Epoch 1: 0.8472704825814673\n",
      "Epoch 2: 0.8950606835990004\n",
      "Epoch 3: 0.9142464580491642\n",
      "Epoch 4: 0.9219227590876055\n",
      "Epoch 5: 0.9333525821642987\n",
      "Epoch 6: 0.93910731887016\n",
      "Epoch 7: 0.9447425800734774\n",
      "Epoch 8: 0.948366670317307\n",
      "Epoch 9: 0.9525483129063411\n",
      "Epoch 10: 0.9533149473809974\n",
      "Epoch 11: 0.9570983382948854\n",
      "Epoch 12: 0.9584922191578968\n",
      "Epoch 13: 0.9605133464092633\n",
      "Epoch 14: 0.9611107239219825\n",
      "Epoch 15: 0.9636595346429175\n",
      "Epoch 16: 0.9644958631607244\n",
      "Epoch 17: 0.9663975149095471\n",
      "Epoch 18: 0.9672537560111112\n",
      "Epoch 19: 0.9674130566811696\n",
      "Epoch 20: 0.9689761945061182\n",
      "Epoch 21: 0.9702306872828284\n",
      "Epoch 22: 0.9704795945797947\n",
      "Epoch 23: 0.9710371469249992\n",
      "Epoch 24: 0.9725405469986758\n",
      "Epoch 25: 0.9738448212347793\n",
      "Epoch 26: 0.9725505032905545\n",
      "Epoch 27: 0.9737751271916287\n",
      "Epoch 28: 0.9742729417855613\n",
      "Epoch 29: 0.9746811497525861\n",
      "Epoch 30: 0.9762144187018986\n",
      "Epoch 31: 0.975547347146029\n",
      "Epoch 32: 0.9761148557831121\n",
      "Epoch 33: 0.9766425392526807\n",
      "Epoch 34: 0.9770507472197055\n",
      "Epoch 35: 0.9768814902577684\n",
      "Epoch 36: 0.97760829956491\n",
      "Epoch 37: 0.9780862015750854\n",
      "Epoch 38: 0.9781558956182359\n",
      "Epoch 39: 0.9784545843745955\n",
      "Epoch 40: 0.9785939724608966\n",
      "Epoch 41: 0.9783351088720517\n",
      "Epoch 42: 0.9784446280827168\n",
      "Epoch 43: 0.979818596361971\n",
      "Epoch 44: 0.9801471539939665\n",
      "Epoch 45: 0.9810233076792879\n",
      "Epoch 46: 0.9806449685878991\n",
      "Epoch 47: 0.9805454056691126\n",
      "Epoch 48: 0.9800675036589372\n",
      "Epoch 49: 0.9814713408138273\n",
      "Epoch 50: 0.9803562361234182\n",
      "Epoch 51: 0.9817899421539442\n",
      "Epoch 52: 0.9816704666514003\n",
      "Epoch 53: 0.9824769262935712\n",
      "Epoch 54: 0.9817799858620655\n",
      "Epoch 55: 0.9824868825854499\n",
      "Epoch 56: 0.9821981501209689\n",
      "Epoch 57: 0.9823773633747847\n",
      "Epoch 58: 0.9828054839255668\n",
      "Epoch 59: 0.982656139547387\n",
      "Epoch 60: 0.9827955276336882\n",
      "Epoch 61: 0.9829548283037466\n",
      "Epoch 62: 0.9830245223468971\n",
      "Epoch 63: 0.9837812005296748\n",
      "Epoch 64: 0.9842292336642141\n",
      "Epoch 65: 0.9824271448341779\n",
      "Epoch 66: 0.9838508945728253\n",
      "Epoch 67: 0.9837214627784028\n",
      "Epoch 68: 0.9838409382809467\n",
      "Epoch 69: 0.9851651251008074\n",
      "Epoch 70: 0.9847170919662681\n",
      "Epoch 71: 0.9843088839992433\n",
      "Epoch 72: 0.9844283595017872\n",
      "Epoch 73: 0.9838409382809467\n",
      "Epoch 74: 0.9844383157936658\n",
      "Epoch 75: 0.9847071356743894\n",
      "Epoch 76: 0.9844184032099085\n",
      "Epoch 77: 0.9852945568952299\n",
      "Epoch 78: 0.9857425900297693\n",
      "Epoch 79: 0.9850456495982636\n",
      "Epoch 80: 0.9855733330678322\n",
      "Epoch 81: 0.9862304483318233\n",
      "Epoch 82: 0.985264688019594\n",
      "Epoch 83: 0.9851850376845648\n",
      "Epoch 84: 0.985264688019594\n",
      "Epoch 85: 0.9861308854130367\n",
      "Epoch 86: 0.9857724589054052\n",
      "Epoch 87: 0.9858819781160705\n",
      "Epoch 88: 0.9865490496719401\n",
      "Epoch 89: 0.9864196178775176\n",
      "Epoch 90: 0.9858122840729199\n",
      "Epoch 91: 0.9866585688826054\n",
      "Epoch 92: 0.9864494867531537\n",
      "Epoch 93: 0.9859317595754638\n",
      "Epoch 94: 0.9866287000069695\n",
      "Epoch 95: 0.9862105357480659\n",
      "Epoch 96: 0.9867382192176346\n",
      "Epoch 97: 0.9864693993369109\n",
      "Epoch 98: 0.987385378189747\n",
      "Epoch 99: 0.9864793556287896\n",
      "Epoch 100: 0.9866087874232121\n",
      "Epoch 101: 0.9869672139308436\n",
      "Epoch 102: 0.9872559463953245\n",
      "Epoch 103: 0.9873156841465964\n",
      "Epoch 104: 0.9872260775196886\n",
      "Epoch 105: 0.9871464271846594\n",
      "Epoch 106: 0.9867680880932705\n",
      "Epoch 107: 0.9873455530222324\n",
      "Epoch 108: 0.9869473013470863\n",
      "current learning rate:0.001\n",
      "Epoch 109: 0.9871663397684166\n",
      "current learning rate:0.0004\n",
      "Epoch 110: 0.9903423968777069\n",
      "Epoch 111: 0.9903623094614642\n",
      "Epoch 112: 0.9902129650832844\n",
      "Epoch 113: 0.9904618723802507\n",
      "Epoch 114: 0.9909795995579407\n",
      "Epoch 115: 0.9905216101315226\n",
      "Epoch 116: 0.990511653839644\n",
      "Epoch 117: 0.9909995121416979\n",
      "Epoch 118: 0.9909995121416979\n",
      "Epoch 119: 0.9911787253955137\n",
      "Epoch 120: 0.9907306922609743\n",
      "Epoch 121: 0.9907804737203676\n",
      "Epoch 122: 0.9910990750604844\n",
      "Epoch 123: 0.9906609982178237\n",
      "Epoch 124: 0.9911289439361204\n",
      "Epoch 125: 0.9912185505630283\n",
      "Epoch 126: 0.9913280697736935\n",
      "Epoch 127: 0.9906908670934597\n",
      "Epoch 128: 0.991168769103635\n",
      "Epoch 129: 0.9910692061848485\n",
      "Epoch 130: 0.9913579386493294\n",
      "Epoch 131: 0.9912982008980575\n",
      "Epoch 132: 0.9909795995579407\n",
      "Epoch 133: 0.9903423968777069\n",
      "Epoch 134: 0.9909198618066687\n",
      "Epoch 135: 0.9912882446061788\n",
      "Epoch 136: 0.9913081571899361\n",
      "Epoch 137: 0.9909198618066687\n",
      "Epoch 138: 0.99194535987017\n",
      "Epoch 139: 0.9916665836975677\n",
      "Epoch 140: 0.9923137426696801\n",
      "Epoch 141: 0.992373480420952\n",
      "Epoch 142: 0.9918856221188981\n",
      "Epoch 143: 0.9922739175021655\n",
      "Epoch 144: 0.9926920817610689\n",
      "Epoch 145: 0.9922440486265295\n",
      "Epoch 146: 0.9927418632204622\n",
      "Epoch 147: 0.9921843108752576\n",
      "Epoch 148: 0.9919652724539273\n",
      "Epoch 149: 0.9924730433397385\n",
      "Epoch 150: 0.9920449227889565\n",
      "Epoch 151: 0.9927617758042194\n",
      "Epoch 152: 0.9922042234590149\n",
      "Epoch 153: 0.9923635241290734\n",
      "Epoch 154: 0.9925327810910105\n",
      "Epoch 155: 0.9925029122153745\n",
      "Epoch 156: 0.9920250102051992\n",
      "Epoch 157: 0.9927717320960981\n",
      "Epoch 158: 0.9931002897280936\n",
      "Epoch 159: 0.9932297215225161\n",
      "Epoch 160: 0.9928812513067633\n",
      "Epoch 161: 0.9924133055884666\n",
      "Epoch 162: 0.9926522565935543\n",
      "Epoch 163: 0.9922838737940441\n",
      "Epoch 164: 0.9930007268093072\n",
      "Epoch 165: 0.9927119943448262\n",
      "Epoch 166: 0.9925925188422824\n",
      "Epoch 167: 0.992343611545316\n",
      "Epoch 168: 0.9930106831011858\n",
      "Epoch 169: 0.9924929559234958\n",
      "current learning rate:6.400000000000001e-05\n",
      "Epoch 170: 0.992602475134161\n",
      "current learning rate:2.5600000000000006e-05\n",
      "Epoch 171: 0.9928115572636127\n",
      "current learning rate:2.5600000000000006e-05\n",
      "Epoch 172: 0.9922241360427723\n",
      "current learning rate:2.5600000000000006e-05\n",
      "Epoch 173: 0.9929011638905206\n",
      "current learning rate:2.5600000000000006e-05\n",
      "Epoch 174: 0.9926124314260397\n",
      "current learning rate:2.5600000000000006e-05\n",
      "Epoch 175: 0.9931898963550015\n",
      "current learning rate:2.5600000000000006e-05\n",
      "Epoch 176: 0.9926821254691902\n",
      "current learning rate:2.5600000000000006e-05\n",
      "Epoch 177: 0.9931799400631229\n",
      "current learning rate:2.5600000000000006e-05\n",
      "Epoch 178: 0.9934985414032398\n",
      "Epoch 179: 0.9932496341062734\n",
      "Epoch 180: 0.9929310327661566\n",
      "Epoch 181: 0.9925925188422824\n",
      "Epoch 182: 0.9931799400631229\n",
      "Epoch 183: 0.9929509453499139\n",
      "Epoch 184: 0.9927219506367049\n",
      "Epoch 185: 0.9929708579336711\n",
      "Epoch 186: 0.9922540049184082\n",
      "Epoch 187: 0.9934288473600892\n",
      "Epoch 188: 0.9931202023118509\n",
      "current learning rate:2.5600000000000006e-05\n",
      "Epoch 189: 0.9929409890580352\n",
      "current learning rate:1.0240000000000004e-05\n",
      "Epoch 190: 0.9931699837712442\n",
      "current learning rate:1.0240000000000004e-05\n",
      "Epoch 191: 0.9934686725276038\n",
      "current learning rate:1.0240000000000004e-05\n",
      "Epoch 192: 0.9931301586037297\n",
      "current learning rate:1.0240000000000004e-05\n",
      "Epoch 193: 0.9931998526468802\n",
      "current learning rate:1.0240000000000004e-05\n",
      "Epoch 194: 0.9927717320960981\n",
      "current learning rate:1.0240000000000004e-05\n",
      "Epoch 195: 0.9933790659006959\n",
      "current learning rate:1.0240000000000004e-05\n",
      "Epoch 196: 0.9930604645605791\n",
      "current learning rate:1.0240000000000004e-05\n",
      "Epoch 197: 0.9932197652306375\n",
      "current learning rate:1.0240000000000004e-05\n",
      "Epoch 198: 0.9926223877179183\n",
      "Early stopping triggered at epoch 198.\n",
      "time of training: 1377.2763624191284\n",
      "max accuracy: 0.9934985414032398\n"
     ]
    }
   ],
   "source": [
    "model, loss_batch, loss_epoch = network_training(model, train_loader, optimizer, criterion, \n",
    "                                                 num_epochs, code_word, path_to_save, interpolate = True, patience=patience, add_noise = True, std = std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bc51f23c-106a-4336-a39e-e1d3ad99d36c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x126e0bb15e0>]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d85e441-0696-4787-afe7-b7fc41f79949",
   "metadata": {},
   "source": [
    "Training went smoothly, the step after 100 epochs accured due to learning rate change "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66b88a4-cc5a-42cb-a60b-db98f8f805c2",
   "metadata": {},
   "source": [
    "Lets evaluate the performance of our model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7b00f8ad-26c9-4a31-9ba5-c86cd0916750",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_of_net(net, test_loader, class_number, interpolate=False, norm_grid = False ):\n",
    "    net.eval()\n",
    "    labels = []\n",
    "    predictions = []\n",
    "    misclassified_samples = []\n",
    "    misclassified_count = 0\n",
    "    probas = []\n",
    "\n",
    "    for inputs, label in test_loader:\n",
    "        if interpolate:\n",
    "            inputs = interpolate(inputs, size=(16, 16), mode='bilinear')\n",
    "        inputs = inputs.to(device)\n",
    "        label = label.to(device)\n",
    "        outputs = net(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        # Aggregate outputs for PR-AUC calculation\n",
    "        probas.extend(outputs.cpu().detach().numpy())\n",
    "\n",
    "        # Extend labels and predictions for the entire batch\n",
    "        labels.extend(label.cpu().numpy())\n",
    "        predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "        # Batch processing for misclassified samples\n",
    "        misclassified_indices = np.where(predicted.cpu().numpy() != label.cpu().numpy())[0]\n",
    "        misclassified_count += len(misclassified_indices)\n",
    "        for index in misclassified_indices:\n",
    "            misclassified_samples.append((label[index].item(), predicted[index].item(), inputs[index].cpu().numpy()))\n",
    "\n",
    "    correctly_classified = (len(labels) - misclassified_count) * 100 / len(labels)\n",
    "    print(f'- accuracy: {correctly_classified:.2f} %')\n",
    "    print(f'- misclassification: {100 - correctly_classified:.2f} %')\n",
    "\n",
    "    # Confusion Matrix and Class-specific Accuracies\n",
    "    cm = confusion_matrix(labels, predictions)\n",
    "    if class_number == 2:\n",
    "        TN, FP, FN, TP = cm.ravel()\n",
    "        # Calculate sensitivity and specificity\n",
    "        sensitivity = TP / (TP + FN)\n",
    "        specificity = TN / (TN + FP)\n",
    "    \n",
    "        # Calculate Balanced Accuracy\n",
    "        balanced_accuracy = (sensitivity + specificity) / 2\n",
    "        print(f'- Balanced Accuracy: {balanced_accuracy * 100:.2f} %')\n",
    "        # F1 Score Calculation\n",
    "        f1 = f1_score(labels, predictions)\n",
    "        print(f'- F1 Score: {f1:.2f}')\n",
    "    \n",
    "    class_accuracies = cm.diagonal() / cm.sum(axis=1)\n",
    "    average_class_accuracy = class_accuracies.sum()/ class_accuracies.shape[0]\n",
    "    print(f'- average class accuracy: {average_class_accuracy * 100:.5f} %')\n",
    "    for i, acc in enumerate(class_accuracies):\n",
    "        print(f\"Class {i} Accuracy: {acc:.2f}\")\n",
    "\n",
    "    # PR-AUC Calculation\n",
    "    y_real = label_binarize(labels, classes=[i for i in range(class_number)])\n",
    "    y_proba = np.array(probas)\n",
    "\n",
    "    precision = dict()\n",
    "    recall = dict()\n",
    "    pr_auc = dict()\n",
    "    for i in range(class_number):\n",
    "        precision[i], recall[i], _ = precision_recall_curve(y_real[:, i], y_proba[:, i])\n",
    "        pr_auc[i] = auc(recall[i], precision[i])\n",
    "\n",
    "    # Averaging PR-AUC scores across classes\n",
    "    average_pr_auc = np.mean(list(pr_auc.values()))\n",
    "    print(f'- average PR-AUC: {average_pr_auc}')\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    plt.rcParams.update({'font.size': 16})\n",
    "    mpl.rc('image', cmap='cividis')\n",
    "    cmp = ConfusionMatrixDisplay(cm, display_labels=[i for i in range(class_number)])\n",
    "    cmp.plot(ax=ax)\n",
    "    plt.show()\n",
    "\n",
    "    return correctly_classified, average_class_accuracy , pr_auc, labels, predictions, misclassified_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d328afb9-c843-4754-bba6-070c21d5b382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- accuracy: 95.35 %\n",
      "- misclassification: 4.65 %\n",
      "- average class accuracy: 90.32975 %\n",
      "Class 0 Accuracy: 0.98\n",
      "Class 1 Accuracy: 0.95\n",
      "Class 2 Accuracy: 0.88\n",
      "Class 3 Accuracy: 0.80\n",
      "- average PR-AUC: 0.915739821977966\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0sAAAKzCAYAAADP8pa2AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAACfjUlEQVR4nOzdd3wUdf7H8fdueg8QeuhVqiIKCogCgoCIgAJyiPHwkJ9nBU+JiuVEOM/DhpUTBT2RjjRBCCoqWEAQjJRQAgQCpJG6qbv7+2NNYN0JhAU27fV8POZx7sx89/vdYW8zn/l85zMmu91uFwAAAADAibm8BwAAAAAAFRHBEgAAAAAYIFgCAAAAAAMESwAAAABggGAJAAAAAAwQLAEAAACAAYIlAAAAADBAsAQAAAAABrzLewCeZLPZlJiYqJCQEJlMpvIeDgAAQLVnt9uVlZWlBg0ayGyuXNfx8/LyVFBQUC59+/r6yt/fv1z6rk6qVbCUmJioRo0alfcwAAAA8CcJCQmKjIws72GUWV5enpo1CdbJJGu59F+vXj3Fx8cTMF1m1SpYCgkJkSQd2d5UocGV68oFKpfhHbuW9xBQDdjz88t7CABw0YpUqO/1Rcl5WmVRUFCgk0lWHfmlqUJDPHtemZllU5OrD6ugoIBg6TKrVsFS8dS70GCzx7/UqF68TT7lPQRUA3aTrbyHAAAXz+74n8p6i0RwiEnBIZ4du02V81hVRkQMAAAAAGCAYAkAAAAADFSraXgAAADApWS122S1e75PeAaZJQAAAAAwQGYJAAAAcJNNdtnk2dSSp/urzsgsAQAAAIABMksAAACAm2yyydN3EHm+x+qLzBIAAAAAGCBYAgAAAAADTMMDAAAA3GS122W1e7bggqf7q87ILAEAAACAATJLAAAAgJsoHV61kVkCAAAAAAMESwAAAABggGl4AAAAgJtsssvKNLwqi8wSAAAAABggswQAAAC4iQIPVRuZJQAAAAAwQGYJAAAAcBMPpa3ayCwBAAAAgAGCJQAAAAAwwDQ8AAAAwE22PxZP9wnPILMEAAAAAAbILAEAAABuspbDQ2k93V91RmYJAAAAAAwQLAEAAACAAabhAQAAAG6y2h2Lp/uEZ5BZAgAAAAADZJYAAAAAN1E6vGojswQAAAAABsgsAQAAAG6yySSrTB7vE55BZgkAAAAADBAsAQAAAIABpuEBAAAAbrLZHYun+4RnkFkCAAAAAANklgAAAAA3WcuhwIOn+6vOyCwBAAAAgAGCJQAAAAAwwDQ8AAAAwE1Mw6vayCwBAAAAgAEySwAAAICbbHaTbHbPZno83V91RmYJAAAAAAyQWQIAAADcxD1LVRuZJQAAAAAwQLAEAAAAAAaYhgcAAAC4ySqzrB7OP1g92lv1RmYJAAAAAAyQWQIAAADcZC+H0uF2Sod7DJklAAAAADBAsAQAAAAABpiGBwAAALiJ5yxVbWSWAAAAAMAAmSUAAADATVa7WVa7h0uH2z3aXbVGZgkAAAAADJBZAgAAANxkk0k2D+cfbCK15ClklgAAAADAAMESAAAAABhgGh4AAADgJkqHV21klgAAAADAAJklAAAAwE3lUzqcAg+eQmYJAAAAAAwQLAEAAACAAabhAQAAAG5yPGfJswUXPN1fdUZmCQAAAAAMkFmq4BIO+Gn7phDt/y1Q+3cF6Oh+f9msJt3zxAmNefSUYZutX4Xo+zXhOvh7gFJP+igr3UvePnbVb1Kga/pmasSEJIXVsrq0G3dtO5065nveMd39+AmNnXSm751bgvXEHS3P2eahfyXo1nGpTuvSU7y19esQbfs6VHE7A5Wc6COzSardsEBX987S8PuTVa9RwXnHg/Jz09AUXX1DhppfYVHN2oUKDrMqP9esY/H+2vJlDa2YV1d5Fq+S/U0mu9pela2uvTN05XWZatQyV4HBNuVkeeng7kBtWBKhr1fUks5xxaxlhxyN+r9Edbg2S0EhVqUl+einr8I1f1ZDZaT6eOBTo6Lw8rarY/dsdb0pS52uy1bDZvnyD7Qp87S39v0aqC8+qaWfN4a6tKvdoEDX9MlSq04WteqUqyZt8uTrZ9fa+TX1+uONyuGToDKb/NpR9R91+pz73NqsowrzuT5dVdlkltXD+QebKPDgKQRLFdzqjyP0+Qe1L6jNV8tq6KtlNdWgWb6atMlTWK0iZZ320r5fA7VwVl19+VlNvbz4oJq2yXNq13NwujJPG38lsk576ccNYZKkzj2yDfepUbtQXW/KMtwW2SLfZd37zzfQV8tqymy2q0nbPHXvn6F8i1n7fg3Uig9r68sFNfXsnHhd3du4P5S/W/+SpCuuzlbCgQAd+D1IWeleCo8o0hVdstWmc47635msf4y+QmlJjiC8XuN8vbZ0jyQp87SX9v8WpKwMb9VvnK8uPTPVpWemeg9J07T/a6miQtc/PD0HpmnKGwfl7WPXvp1BOpngp1YdczT0niT1GnRak++8QieO+Hv0GKD8dLouW/9aeEiSlHrKW7//HKS8XLMat8rXdf0zdV3/TK35pKbefDJSZwfgPQdlaOI/E8tp1KiqYn8OVGK8n+E2m5UpU0BlVeGDpcWLF+vtt9/Wzp07VVBQoJYtW+ovf/mLHnvsMfn4VP2ryE3b5OqOiUlq0cGilh1ztWBWXW1cUvOcbe6YmKy/PZuomnWKnNbn5pg1c1Ijfbeqhl5/vJFeX7XfafuE50o/eVj0dh39uCFMkc3z1LFbjuE+jVrm6/HXj5bxk0kh4Vbd/fgJ3XJXmiLqFzqN8/XHG+mbFTU044Gm+mjzHoWEu2bCUP5mT2+s4/H+ys5w/ikJCS/Uc7P3q8M12Zrw9FH965E/Mo92acfmUC2ZXU87vg+TzXbmBKJjt0z9c06cuvdN18iJJzR/VkOn96xZp0CP/+eQvH3seuOpplr7WR1Jktls1+T/HFLfYama8sZBPXJ7O50rM4Wqw2aTvlsdps8/iFDsz8FO23rfdlpPvnVUg+9O0+6tQYo563fzZIKvPp8ToQO/BejAbwG6YUi6xjya5Onho4pZN7+WNiw6999nAJVPhc4JP/rooxo5cqQ2b96sa6+9VrfccouOHj2qJ598Un369FFubm55D/GyG/iXNP3t2UT1GZ6uxq3yZS7Dv1iLDrkugZIkBQTZNOFZR0C055cg5WSV/Z//ywWOPwD9R6eVuc35PDDtuMZOOuUUKBWP87GZRxUYbFXWaW/DaTSoGPb9GuwSKElSVrqPPnrFMZ2pS6+MkvUnjvoremxb/fJtuFOgJEm//RSqRe/VlyT1G57i8p7D/npS/oE2bf8+tCRQkiSbzaRZzzRVdqaX2nTO0dU3ZLi0RdW0c3OIpk1o6hIoSdKmlTW0/o8T1353Ok+R+uHLML07taE2LKqp+D0BsnLVH8BFKH7OkqeXC7Vv3z7NmjVLUVFR6tixo7y9vWUymTRt2jTD/W02m7Zs2aJnn31WPXv2VK1ateTj46OIiAjdfPPN+vTTT2U/z/OefvnlF915552qW7eu/P391axZMz300ENKSjr3BapTp07pwQcfVLNmzeTn56e6devqzjvv1Pbt28/ZrqCgQC+//LI6d+6soKAg1ahRQzfeeKOWLFly7oNzDhU2WPr888/1xhtvKDg4WD/99JO+/PJLLV26VPv371fHjh31/fffa+rUqeU9zErHy9vxpTab7fL2Ltt8199/DtKxg/7y8rbr5pGXLlg6F/9Ae8nUveTEqp9BrIpsf8TrhQVl/5k5+HuQJCmivuu9atf3d5zwOu5pcpZn8dKPMeGSpB4Dzn3vAKqPg7EBkqSIBoXn2RMAqr53331XDz/8sObNm6fY2FhZreeetXPo0CH16NFDL774ovbs2aOuXbtqxIgRat68uWJiYjR27FjddtttKigwvr98yZIl6t69u5YsWaImTZpo6NChMpvNeuutt9SpUycdOHDAsF1cXJw6deqkt99+W2azWbfffruaNGmiJUuWqFu3blq+fLlhO4vFoptuuklTpkzR0aNHdcstt+jaa6/V5s2bdeedd+rxxx+/sAP2hwobLE2fPl2SNGXKFHXp0qVkfUREhN555x1J0ltvvaWMDK4il1VBvkkfzXBcub/qhiz5BZQtWCrOKl3TJ9MwY1XsdLK3/vdqXb3xRKTendpQq+bVUtIx9wKdokLpVILjPpdz9YmKKSDIqrGPHpekkiCmLBo0ddxHdzrZ+XsTEGRVw2aO4Hn/b0GGbYvXt2hvudDhoooq/s6knarwM85RBXS+PlsTnk3UI/9O0L3RJ3T9LRny8bWV97DgATaZy2W5UB06dNDjjz+uTz/9VHv27NHdd999zv1NJpP69OmjtWvXKikpSV9++aUWLFign3/+Wd98842CgoK0evVq/etf/3Jpm5iYqHvuuUdFRUV6//339fPPP2vhwoWKi4vT2LFjderUKY0ZM8YlM2W32zV69GglJSXp7rvvVlxcnBYuXKiff/5Z77//voqKijRu3DidPHnSpc+nnnpKW7ZsUceOHbV//34tXbpUX375pX788UcFBwdr5syZWr169QUftwr5F+T48ePaunWrJGnMmDEu23v27KlGjRopISFBX3zxhe666y5PD7FS2L8rQCs+rC27XcpI9Vbcr4HKSPNW6ytzNGlmQpneI89i1rerwiVJt9yVes59Ew7465P/1Hda9663XUP/mqz7nkmU1wV829Z9VksZad7y87fpmj6ZZW+IctGlV4Zuui1VJrNdNSIK1faqbAWF2LT1mzDNebls1cX8/K0aGuWosvj9Oud5/3UjzxQISUo0rtiY/Mf6s/dF9VWjdmFJJvz7L8LKeTSoDm4e6ZrVTj3prVcnNdK2b5hOjvJ33333Ob02n+fejhYtWmjjxo2G23r37q0pU6Zo6tSp+vjjj/Xss886bX/99ddlsVjUr18/TZgwoWS9l5eX3n33Xa1atUpbt27V+vXrNWDAgJLta9eu1Y4dOxQeHq533nlHXl5nKupOmDBBixYt0saNG/XGG29oxowZJdtOnz6td999V5IjgxYREVGy7eqrr9aTTz6pqVOn6qWXXtKtt956zs/9ZxUys7Rjxw5JUs2aNdWsWTPDfbp27eq0L1wlH/fVhkU1FbO4prZ+FaqMNG9d1StLT717xOU+odJsWhmu3Bwv1axTqGv7GgctQSFWDftbkv6zbL8++zVWKw7s1Hsb92r4hCSZTHYtm11Hs6Ijyzzu+D3++uDFBpKkMY+dVI3aZJYqusYtc3XzHSnqNzxVV9+QqaAQm776vJZm/qO5LFlli5IffPGI6jfOV8pJHy142znoDgg6M1Xg7FLkZ8v9Y31gMMVAqjuzl11PvnVUwWE2Hdrtry8+cZ26CVwqh3YH6J2pDTThxja6vVUHjezYTtGjm+v3rYGqVa9Iz889rE7XUdW1KrPaTeWylLerrrpKkpSQ4HoBvniqnFHSIzg4WLfddpskadmyZYbtbrvtNgUHu96PWvx+f273xRdfqKCgQI0bN1aPHj1Kbffjjz8qMfHCqqFWyGApPj5ektS4ceNS92nUqJHTvnB1/cAMfZn4q75I+FUf//y7HvvPUSUc8NP9N7XVd6vLdqX1y88cV/j73pFWamaoZcdcTXwhUR2756hmnSL5B9rV7Io83f98oqLfPSJJWvtpRMn9A+eSnOij5+5prtwcL3Xvn6FRD1KhqjL4/KN6uqXZtRrcqqvu7d1Js6c10jU3pmv2+l3qcO35M4NjHjqum+9IUX6eSdMfbKmsdO5Tg/sefvmYruqVrYw0L02b0NSwDD1wqSz/b22tmFNbR+L8lZvjpYxUH23/NkSThrbUlnWh8vG1a+I/j5f3MIFLbv9+R1Xl+vWdL3BmZWWV3I9UnNz4s9KSHsWvz9du//79ysnJKXO75s2bq2ZNxzntr7/+avyBSlEh/4JkZTme1RMUZHxvgqSSaDMzs/QTsfz8fGVmZjot1ZGXl1Q3slC3jEnTzM8PSCa7Zj7WWGlJ577if+ygn37f6jjOA9ysgtdzUEbJPSQ/bjj3NIS0JG9NGdlCp4756uobM/X0+4dlKv8LJ7gA1iKzThz117I59fXMvW0UHGbVE68ekq9f6fP2h48/oXGTjqsg36R/3t9Ku38JcdknN+dMNsk/0DhzFPDHeku2ceYJ1cPEfx7XwDFpyjrtpejRzXX8kPFzb4DLz6RP/lNPktSifZ5qN+Ah66g6LBaL3nzzTUnSiBEjnLYdPny45L9LS3yUlvQ4X8KkuJ3dbnfqpyyJlsjISMM+z6dCBkuXyowZMxQWFlayFB/g6qxeowJ17pGt3Bwvbf/W9aT0bMWFHdpfm61GLd2/D6RRK0fblBOlZwvSU7z15J0tdeyQv67qlaXnP4yXrx9Pp67M9v0arKP7A1SnYYFadTJ+Ntdt95zUhGcSVJBv0rT/a6Vfvg033C/p+JkT3jqlnHAUn4gkHePkuLqa8Gyiht2Xoqx0L0Xf1VwHYwPLe0io5o7uP/N7VNbp76h8rDKXyyLJJSmQn++Z+3YfeOABxcfHq0GDBnrqqaecthUnPaTSEx+lJT3OlzA5e2re2W0vVaLFSIUMlkJCHCfxZ6fX/iw72zH/NzS09GxFdHS0MjIyShajOZXVkX+A4yp/ekrpmSWrVSUPcbzlrosrF5512nGlPyDIOLuQnuqlJ+5soaP7/XVlzyy9MPeQfP0JlKqCvFzHT0x4LdeThCF3n9IDzx91BEoPtNTPX4eX+j6WbC8dj3ecdLTqaPy7ULz+wO+cIFdH459J1IiJycrOMOupu5pr/y6+Byh/oTXOZMIt2RXylAuVXKNGjZwSA2cXPbhcXnzxRc2bN0/+/v5atGiRatWq2veFVshqeE2bNpVkfMNYseJtxfsa8fPzk58fV5nPVpBv0u9bHVF3ZPPSrz5s3RiqtFM+Cgy26oYh6W73l3LCR7E/OSL5Nle5lnTOSPXSk3e21JF9AbqyZ5b+Oe9QmUuao2ILrVGo5lc4/s2Px/s7bRs0Jkl//+eRM4HSVzXO+35b1tfQnfef1E1DU7VhSW2nbf6BVnXrmy5J2vzl+d8LVctfn0rUyAccgVL06BaK20mghIqh99B0SVJOplnHDvqfe2dUWja7WTY3HhJ7cX06zpUSEhKcEgeX+7z31Vdf1bPPPis/Pz8tX77csJhCcdJDciQ+wsJc75MvLekREhKitLS0UhMmxe3+3PZSJVqMVMjLHMXVNVJTU0udV7ht2zZJcnoGExzZolXzaikny/WfNuWEj/79cGOlnvRV3Ub56nJDlsE7OBRPwes99LT8A8/9nIjlH0QoI9X1PpFDu/313D3NlJ9nVv2m+bpugPMzsTJPe+nJUS11eG+ArupFoFTZNG6Zq5uGphg+R6Rhs1w9/fYB+frZtWd7kA7vO3PyesvoJD344uELCpQkafmH9ZRnMatLz0zdMvpM4Q+z2a4HXzyskDCr9u0M0i/fUia6OrnniRMa9WCyY+odgRI8rHn7XHXvnyGzl/PfLpPJrgF3pereKSckSZ/PiZC1iJtwcemFhoY6LZczWJo1a5YmT54sX19fLV26VLfccovhfk2aNCn576NHjxruU1rSo/j1+dqZTCanfs7XTpKOHTtm2Of5VMjMUmRkpK655hpt3bpV8+fP19NPP+20/fvvv1dCQoL8/Pw0aNCgchqlZ+zfFaC3njpTdvvEYcf/CdZ8Uks/xZyJjJ+dE69adYuUl2vWW9GN9P5zDdW8fa7qRhbILikl0VcHfgtQYYFZteoV6LkP40ud6pae4q2fNzreuyxT8D75T33NfqGhWrTPVb3GBTKZ7DpxxE8HYwNks5lUp2GB/jnvkMs9SK8/3kjxuwNkMtkVEl6kWdHG95RdPyBD1w/k4cMVTXitQj35+iE9/NJhHdwdqJQTvvL2satOgwK16JAjLy/pyH5/TX+oZUmb5lfk6OGXDstslk4m+KnXwNPqNdD12SSSNPMfzZ1epyX5auY/mmvKGwf06IzDGjAyWaeO+al1pxw1aJKvtGQf/euRFpI4IakuuvfP0JhHHYFz4mFfDYlKMdwv87S3/vvPBiWva9Yp1LNzDpe8Lr6X5Lr+mWq6an/J+reeaqgDvxF8oXR1GxXo+Q8PK+u0lw7EBuh0sreCQq1q2jZPdSMd36uvl4frf6/WK+eRAhfn7bff1sMPP1wSKA0ePLjUfUNDQ9WyZUsdOHBA27ZtU8eOHV32KS3p0aVLF23fvr1ke2ntWrVq5XT/UvH7lNbu0KFDSktznNMWJ2XKqkIGS5LjKbzDhg3Tv/71Lw0cOLDkIKSmpuqBBx6QJD344IOGqb2qxJLtpb3bXW9WSznhq5QTZx7OWVhw5t6QCc8d128/BuvwPn8l7PdXfp5ZwaFWte1iUbebMzRobKqCQkrPFsUsqaGiQrOatMlV2y6uU+f+7K6HT+r3rcE6Euev7d+GKM9iVmCIVVd0zdF1AzI0+O5UBQa79peV7vj62e0mfbuq9OxC3cgCgqUK6Mj+AH30SqQ6XJOlRi1y1aKdRV7edmVneOvXLaHavK6mNiyJKPluSlJwqFXFz8Br3DJPjVvmlfr+fw6WJOm7L2rqxNH2Gv1Aojpck6UW7Sw6neyjlR/X0fxZDZWeQsnx6iQk/Mz9IG2uzFWbK3MN9zuZ4OMULPn42nXF1a6/beERRQqPOPNcN6PfLeBsh37317LZEWrdOVeRLfLVrmuOTCbpdIq3vl0VpvULHc85RNV2dsEFz/XpuZk47733nh588MGSQKksD3UdNmyYXnnlFc2fP1/33nuv07bs7GytWrVKkjR8+HCXdh988IFWrlypnJwcl4IN8+fPN2w3aNAg+fr66ujRo9q8ebPL9MDidt27d1eDBg10IUx2u73Cznt65JFH9Oabb8rHx0d9+/ZVUFCQNm7cqPT0dPXo0UMbNmxQQMD5n91TLDMzU2FhYTod11yhIRVyBiKqiFuadSvvIaAasHuo6hEAXE5F9kJ9oxXKyMi44PtJylPxeeV/t1+twBDPPrbCkmXV37r8clHHLCoqSvPmzdOLL76oZ555xnCf//73v7r//vvl4+NT5kBJkhITE9WqVStZLBbNnj1bf/vb3yRJVqtV9957rz755BNdc801+umnn2Q66zkxdrtdV199tXbs2KFx48bpww8/lJeX49jOnj1b999/v4KDg7V//37Vq+ecsX300Uf1xhtvqFOnTvrqq69KCk9s375dvXv3LgnSyvoZilXoYEmSFi1apLffflu//vqrCgsL1aJFC40dO1aPPfaYfH19z/8GZyFYgqcQLMETCJYAVAWVPVh6f/vVCgj27GSt3Owi3X+BwdL27dtLZmdJ0sGDB5WSkqLIyEg1bNiwZP3y5ctVv359/frrr+rSpYvsdrvatm2rbt1KP7eZO3euy7rFixfrrrvuktVqVbdu3dS0aVNt3bpVhw4dUt26dfX999+rZcuWLu327dunXr16KTk5Wc2bN9c111yj+Ph4/fzzz/L29taiRYs0bNgwl3YWi0X9+vXTDz/8oBo1aqhPnz7KycnRxo0bVVhYqEmTJmnmzJllOlZnq/DB0qVEsARPIViCJxAsAagKCJYunDvB0jfffKObbrrpvPvFx8eradOmZd5fcmSEjPzyyy+aPn26vvvuO2VkZKh+/fq69dZbNXXqVNWtW7fU9zt58qSmTZum1atX68SJEwoLC1OvXr309NNPn7O4W0FBgV599VV9+umnOnjwoHx9fdW5c2c9+OCDuvPOO8v0Wf6MYAm4DAiW4AkESwCqgsoeLL27/ZpyCZb+r8vWSnfMKiMiBgAAAAAwQLAEAAAAAAYqbOlwAAAAoKKz2s2y2j1cOtzD/VVnHGkAAAAAMEBmCQAAAHCTTSbZZDr/jpe4T3gGmSUAAAAAMECwBAAAAAAGmIYHAAAAuIkCD1UbRxoAAAAADJBZAgAAANxklVlWD+cfPN1fdcaRBgAAAAADZJYAAAAAN9nsJtnsHi4d7uH+qjMySwAAAABggGAJAAAAAAwwDQ8AAABwk60cCjzYyHd4DEcaAAAAAAyQWQIAAADcZLObZfPwQ2I93V91xpEGAAAAAAMESwAAAABggGl4AAAAgJusMskqzz73yNP9VWdklgAAAADAAJklAAAAwE0UeKjaONIAAAAAYIDMEgAAAOAmqzx/D5HVo71Vb2SWAAAAAMAAwRIAAAAAGGAaHgAAAOAmCjxUbRxpAAAAADBAZgkAAABwk9VultXDmR5P91edcaQBAAAAwADBEgAAAAAYYBoeAAAA4Ca7TLJ5+DlLdg/3V52RWQIAAAAAA2SWAAAAADdR4KFq40gDAAAAgAEySwAAAICbbHaTbHbP3kPk6f6qMzJLAAAAAGCAYAkAAAAADDANDwAAAHCTVWZZPZx/8HR/1RlHGgAAAAAMkFkCAAAA3ESBh6qNzBIAAAAAGCBYAgAAAAADTMMDAAAA3GSTWTYP5x883V91xpEGAAAAAANklgAAAAA3We0mWT1ccMHT/VVnZJYAAAAAwACZJQAAAMBNlA6v2sgsAQAAAIABgiUAAAAAMMA0PAAAAMBNdrtZNrtn8w92D/dXnXGkAQAAAMAAmSUAAADATVaZZJWHS4d7uL/qjMwSAAAAABggWAIAAAAAA0zDAwAAANxks3v+uUc2u0e7q9bILAEAAACAATJLAAAAgJts5VA63NP9VWccaQAAAAAwQLAEAAAAAAaYhgcAAAC4ySaTbB5+7pGn+6vOyCwBAAAAgAEySwAAAICbrHaTrB4uHe7p/qozMksAAAAAYIDMEgAAAOAmSodXbRxpAAAAADBAsAQAAAAABqrlNLxhba+Ut8mnvIeBKuzksublPQRUA/Xv2F/eQ0A1YLday3sIqPJMkr28x+A+m0yyebjgAqXDPYfMEgAAAAAYqJaZJQAAAOBSsJfDQ2ntZJY8hswSAAAAABggWAIAAAAAA0zDAwAAANxks5dDgQcP91edkVkCAAAAqrh9+/Zp1qxZioqKUseOHeXt7S2TyaRp06adt21MTIwGDRqkiIgIBQQEqG3btnr66aeVnZ19znYHDhxQVFSUIiMj5efnp8jISEVFRenQoUPnbJeVlaWnnnpKbdq0UUBAgCIiIjR48GB99dVX52xns9n0/vvvq1u3bgoJCVFISIi6deum2bNny253r+QimSUAAADATTa7WTa7Z/MP7vT37rvv6o033rjgdq+99pomTZokk8mkXr16qW7duvruu+80ffp0LV26VN9//70iIiJc2m3evFn9+/eXxWJR+/bt1bNnT8XGxmrevHlasmSJYmJi1L17d5d2SUlJ6tWrl+Li4lS/fn0NGTJEp06d0tq1a7V27Vq98cYbeuihh1zaWa1WjRw5UsuWLVNgYKD69u0ryRHo3X///YqJidGCBQtkNl/YsSOzBAAAAFRxHTp00OOPP65PP/1Ue/bs0d13333eNjt27NDkyZPl5eWlNWvWaNOmTVq0aJEOHjyovn37at++fZo4caJLO4vFopEjR8pisSg6OlqxsbFasGCBYmNjFR0drZycHI0cOVK5ubkubSdMmKC4uDj17dtXBw4c0KJFi7Rp0yatXr1aZrNZjz76qHbt2uXSbtasWVq2bJkaNmyo2NhYrVy5UitXrtTvv/+uBg0aaPHixXrnnXcu+LgRLAEAAABuKr5nydPLhbrvvvv0yiuvaMyYMWrbtm2ZMiwzZsyQ3W7Xvffeq4EDB5asDwwM1Jw5c2Q2m7V06VLt3bvXqd3cuXOVmJio1q1bu0zzmzZtmlq3bq2EhAR9/PHHTtt2796tFStWyMvLS3PmzFFgYGDJtkGDBikqKko2m00zZsxwamez2fTyyy9Lkl5++WU1a9asZFuzZs1Kts2YMUM2m+28n/tsBEsAAAAAnBQUFGjNmjWSpDFjxrhsb9KkiXr06CFJWr58udO24tejR492CcrMZrNGjRolSVq2bJlhux49eqhJkyYufRaPY9WqVSosLCxZ/8MPP+jkyZPy8/PTiBEjXNqNGDFCvr6+SkxM1E8//XSOT+2KYAkAAACAk7i4OFksFklS165dDfcpXr9jxw6n9cWvL1e7nJwc7d+/36Vd+/bt5e/v79IuICBA7du3N+zzfAiWAAAAADfZZCqX5XKLj4+XJIWHhyskJMRwn0aNGjntKzkq2aWmpkqSGjdufM52ycnJysnJcemztHahoaEKDQ116fN87Uoba1lQDQ8AAACohDIzM51e+/n5yc/P75K8d1ZWliQpKCio1H2Cg4NdxlHc7lxti9sVty3er6x9ZmZmGvZ5oWMtCzJLAAAAgJvKs8BDo0aNFBYWVrL8ufABLh6ZJQAAAKASSkhIKJmWJumSZZUklUy9O3ua3J8VP5T27DGcPWWvtLZnP8zWqK27fV5ou7IgswQAAABUQsX38BQvlzJYatq0qSQpPT3daWrd2RISEpz2lRyBS82aNSVJR48ePWe7iIgIp6lzxe9TWruzp9+d3ef52pU21rIgWAIAAADcVFmes3Sh2rRpU/Kco23bthnuU7y+S5cuTuuLX1+udkFBQWrdurVLu99//115eXku7XJzc/X7778b9nk+BEsAAAAAnPj6+mrw4MGSpPnz57tsP3LkiLZs2SJJGjZsmNO24tcLFixweQiszWbTwoULJUnDhw932nb77bdLkjZv3myYJSoex5AhQ+Tj41Oy/rrrrlO9evWUn5+vpUuXurRbunSpCgoK1KBBA3Xr1q30D22AYAkAAABwU1XNLEnSlClTZDKZ9NFHH2ndunUl6y0Wi8aPHy+r1aoRI0aobdu2Tu2ioqLUoEEDxcXFaerUqU7bpk6dqri4OEVGRmrcuHFO29q3b6+hQ4fKarVq/Pjxys3NLdm2du1azZ07V2azWdHR0U7tzGaznnzySUnSk08+6VJWfMqUKZKk6Ohol4fkno/JbrfbL6hFJZaZmamwsDDdaB4ub5PP+RsAbjq5rPX5dwIuUv079p9/J+Ai2a3W8h4Cqrgie6G+sX+ujIyMC775vjwVn1cOWDtBPkG+Hu27MKdAXw6cfUHHbPv27XrggQdKXh88eFApKSmKjIxUw4YNS9YvX75c9evXL3n92muvadKkSTKZTOrdu7fq1Kmj7777TidOnFCbNm30/fffKyIiwqW/zZs3q3///rJYLOrQoYM6dOig2NhYxcbGKigoSDExMerevbtLu6SkJPXs2VP79+9X/fr11atXLyUlJWnTpk2y2+1644039PDDD7u0s1qtuvPOO7V8+XIFBgaqX79+kqSYmBhZLBbdcccdWrhw4QUHS1TDAwAAANzkyUzP2X1eqMzMTP30008u648dO6Zjx46VvM7Pz3fa/thjj6ljx46aOXOmfv75Z+Xk5Khx48aKjo5WdHR0qQ+s7dGjh3bu3KkXX3xRMTExWrp0qWrXrq1x48bp2WefVYsWLQzb1alTR9u2bdOMGTO0dOlSrVixQkFBQRowYIAef/xx9e3b17Cdl5eXlixZov/+97/64IMPtHHjRkmObNX48eM1YcIEmUwXftzILAGXAZkleAKZJXgCmSVcbpU9s3TzF/eXS2Zpw6D3K90xq4y4ZwkAAAAADDANDwAAAHCTXZJNnp2GV22mhVUAZJYAAAAAwACZJQAAAMBNlaXAA9xDZgkAAAAADBAsAQAAAIABpuEBAAAAbmIaXtVGZgkAAAAADJBZAgAAANxEZqlqI7MEAAAAAAbILAEAAABuIrNUtZFZAgAAAAADBEsAAAAAYIBpeAAAAICb7HaT7B6eFufp/qozMksAAAAAYIDMEgAAAOAmm0yyycMFHjzcX3VGZgkAAAAADBAsAQAAAIABpuEBAAAAbuI5S1UbmSUAAAAAMEBmCQAAAHATpcOrNjJLAAAAAGCAzBIAAADgJu5ZqtrILAEAAACAAYIlAAAAADDANDwAAADATRR4qNrILAEAAACAATJLAAAAgJvs5VDggcyS55BZAgAAAAADBEsAAAAAYIBpeAAAAICb7JLsds/3Cc8gswQAAAAABsgsAQAAAG6yySSTPFtwwebh/qozMksAAAAAYIDMEgAAAOAmHkpbtZFZAgAAAAADBEsAAAAAYIBpeAAAAICbbHaTTB6eFmdjGp7HkFkCAAAAAANklgAAAAA32e3l8FBankrrMWSWAAAAAMAAwRIAAAAAGGAaHgAAAOAmnrNUtZFZAgAAAAADZJaqqMjmeerSO1OtOlrUqpNFjVvmyctbmvvv+vrszfqGbcZOStTdk06e833v691OCQf9ndY1bpWrQX9JUcsOuarbKF+hNYtkMkmpJ32168dgLf+gjg7vDbhknw2eZ04uVNDyVPn9miOvlELJLllreKuwfaBybqupomb+Lm1MmUUKWpEmv63Z8jpVIJPVLluYtwrbBChncE0Vtg90aeOz16KAbzLkHZ8vr+RCmbOsspslW20f5XcKkmVoTVnr+rq08/0tRzWnHj3nZ8iYWE+5t9Rw/yCg3EQ2z1OXGxy/Zy075pT8ns17pYE+m2X8e7bu6C9leu9XHmuqjUtruaxv2TFHox44qQ7XZisoxKq0JB/9tDFM89+sr4xUn4v6PKiYIlvkqcsNWWrVyaJWHXPVuFXx3816+uyNeudse1WvLA3/W5LaXGWRf4BNScd99f0X4Vowq47yLF5l6r95u1y9uSZOPr52Jcb76t6e7S7Fx4IHkFmq2giWqqhbxyVr2H3JbrU9+HuADv5uHNzkZLn+6LfrmqNh9yUrLclbxw75a88vQfILsKlp2zzdMjpV/Uak6pVHm+qbFTXdGg/Kl09crmo8d1TmXJustbyVf2WQZDbJOz5PAV9nyP/bDKVPaqj8HqElbbxOFKjm00fklVYkW4iXCjoESn5meR/Nl/+WLPlvyVLmvXVkGep8kur3S7YC16XLWttbRQ19ZQv3ljnHKu9DeQr64rQCvkpX+tONVNAxyHCs1nAvFVwVbLytoWuQhcph8N3JGjY+6YLabFjsGgAVq92gQFf2yJLNJv32o+v3peeg05oy65C8faR9vwbqZIKfWnXK0dB7k9Vr8GlNHtFGJ464XiBA5XbruBQNuy/lgtsN+1uSJj6fKJtNiv0pSOkpPmp/bbbueviUeg5K16TbWynz9LlPt7x9bPrHG0fk5U2JM6CiqbDB0r59+7R+/Xr98ssv+uWXX7Rnzx5ZrVa9+OKLeuaZZ8p7eBXe4X0BWvxeHR2MDdSB3wI1+qGT6ndHWpnabvkyTP97tUGZ+9rxXYhhxslksmvEhCT9bepxPfbKUW37JlTZGRX2K4dShL5zQuZcmyz9w5U5oZ7k/cfVLJtdwZ8lK3hxqsLeOaGka4IlX8fM3pCPTskrrUh5XYOV8XhD2f3PzPgN+PK0wt49qZCPk5TXI1S2iDNX6XNvCFNuv3DX7FGhXSHzTilo9WmFvZ6o5NktJS/Xq2rWhn7KeKTs311UDkf2BWjJe3V14PdAHYgN1OgHT6jfiHP/ns2c3LTUbX+fdlRX9sjSju9DlHTcz2lbzboFevzVw/L2kd6Y0lhr59eWJJnNdk1+9bD6Dk/TlFnxeuS2tpK4sluVHN4boMXv1v7j72aARj98Sv3uOH3ONi3aWzTh2URZi6Rno5pr29eOi0Z+/ja9MPeQruqVrYdfTtC0Cc3O+T5/eeyUmrfL04oPIzT0rxcesAG4fCrsmeu7776rN954o7yHUWmt+yzC6bXNdvn6OnXMz3C93W7SkvfravDdyWrQtEDtr8nWTzHhl28guORMmUXyOZwvScr6S+0zgZIkmU3KHl1bQSvSZM6xyftYgYqaOwJm310WSVL2qAinQEmScgfUUNCKNHknFsjnQK7yzwqWrI2Mv0vyMSkrqq4C16fLK7VI3gn5KmrKlf3qYt0C598zu839IMXHz6YbhzoCrS8XRrhsHzY+Sf6BNm3/LqQkUJIkm82kWU81Vre+GWpzpUVX35CpX74Nc3scqHjWfeacjSzL383RDyXJbJbWflazJFCSpPw8s16d3Fhzf9itXoMz1KhFnssFxWKtO1s06u+n9O2qcH3/RRjBUiVks5tk8vC0OBvT8DymwhZ46NChgx5//HF9+umn2rNnj+6+++7yHhLcZLU6/g9dWFBhv24ojU/Z/81sIWdN0fQt24+4LeQCrteYpOK/DXYf/kjAPT0HnlZImFWZp730w5fhLtuvH5AuSfr6c9dpw3kWL/0Y4wiQegxMv4yjRGXg7WPTtX0zJUlff+56P2TScV/t3uqYMnz9wAzD9/Dxs+nx148oK8NLbz3d8PINFoDbKmxm6b777nN6bTZzou0pLTvk6q/RxxUSXqScLC8djA3UjxvClJtTtptUzzbwLylq1CJfp5O9tXe78X0mqLjsAWYVtAuQ7+5chXya7DoNb0GyTAV25XcJkq32mQxRfpdgBXyToeCFKUr/R0PJ76xpeOtPyzuxQIVN/FTYtoyFP6yOvsz5dhU28pW1vvH9R+aMIgUtTJZXapHsviYVNfRTftdgp7Gheus/KlWS9NXyWi4XcAKCrGrYzJFJ3b/L+Pdq/64g9RuRphbtLZd3oKjwGjbPl3+gI/0Ut9O1YI0kxe0KVMfuOWrZIddw+z3/OKEmrfM144EmfxQOybtcw8VlZLc7Fk/3Cc+osMESys91/TN0XX/nq2DZGV5699lIxRhUjSrm52/Tg9MdFcmCQq1q2iZPDZvlKy3JWy9NbC5L9oUHWyh/GX+vrxr/TFDg+nT5/ZKtwhb+JQUevFKLlHtjmDIn1HVqkxVVR94J+fLflq069x1QQZsA2X1N8k7Il/fxAuV1DVbmA/UM7zuSHNX3QuY7CpSYsq3yOeToq6i+jzL+ESmZjdt5HytQyGfOU1jsH0iWwTWVdU+dUvtD9VA3Ml+dr8uSJH25wPW3rG5kQcl/JyUaB+TJiY7Au26jAsPtqD7qNXZ8B7LSvUq9mHjm+5Lvsq1d1xwNn5CsLetC9c0KKnUCFRXBEkqcOOKnD2c00NavQ5V03HGi0LhVnkb9/aS635ypf7xxRFabSV8vN65q5+1rU/+RzjddJx721Wv/aKLYn40rlKHiszb0U9rLTRX2eqKjdHhqdsm2wka+KugQKHug84mCLdxbadOaKPS9EwrYlCn/bWfaWCO8VdAxULaw0n9+zFlWBXztHLAXNvdXxkP1VdTY9b4mW5CXcobUVF73EFkb+MoWaJb3iQIFfJWhwC/SFLQyTaZcmzL/blxmGtXDzSNTZTY7sgDxe10zAQHB1pL/zrMYz2bI/aMMdOBZ+6J6CgxyZJXyckuf+ZL3RxAVGOJ8A5Sfv02TXzsqS6aXZkU3unyDhEc4MkueLh3u0e6qNYIllDB61sjubcF67t6W+r9/Juj2vyZr4nPH9N3qcBUVuv5xyMn01oDILpKk8IhCtexo0djHTuiVxfu1+L06+mBa5GX/DLj0fPZYFP6vY5KXSemTGqigU5Ds3ib57rEo5KMkhb11Qj57LMp86EwVOq9j+arxUoLMGVZl3F9P+dcEyx5olvehPIXOTVLoR0ny256j0882Msz2FDX318nPr5DsdpnTiuSzN1chnyWr1uR4Zf21riy31nTZP6u5883TRU39lfVXfxVcEaAaLx9X4IZ0WQbWKClCgerFZLLr5jscU/CMCjsAnvTXpxIV2TxfrzzSWGlJTBMGKrIqfSNQfn6+MjMznRa453+v1pe1SAqPKFLbq84/Vz89xUfbvg7T4yNa60BsgO6cmKRufY1vcEXFZcq2Kvxfx2TOtCp9SqTybgiTLdxb9mAv5V8TotPPNpLNz6TAjRny/S3H0chqV/jLx+R9olAZf6+v3IE1ZIvwkT3QS4UdgpT2fGNZa3jJb2eOAr45z3fCZJKtlo/ye4Qq9V9NZQv3VsiHp+QdX/Z5/fnXhaqwmSMb5bc1y91DgUruqp5ZqhtZoLxck74u5ZlvuWdNFS6+F+XPAgIdGSWmFcOS4ziF8g8ovWyef9Af35esM6dbna7L0m33puinjaGKWcLzB4GKrkoHSzNmzFBYWFjJ0qgRqW53ZaV7Kz3FcfUron7Z5+oXFZpLpu1dT/WoSsfvl2x5ZVhlreujwtauxRis9XxL1vvudARLPnG58kkokN3HpPzuIS5t7MFeyu8S7NSmLOzBXsrrHiKTTfL7+cKCnqJIR7DklVp0Qe1QdfQf5biXbfPaGrIYPFxbUsn0Y0mq08D4d652g0LHvsd4yHF1dyrB8R0ICbcqIMh4Wmbx9+XUWd+X62/JkNns+I79e/F+p2XiC8clSbXqFZasa04xkQrPbjeVywLPqNLT8KKjozVp0qSS15mZmQRMbjKb7QoKdfwxyL3AK6rFc//DaxVe8nHh8vJKdvyb2QNLv65SvM2cbXVu42cqtaBC8T1OxW3Kyv5HVT1zxoW1M2c59rcHVOnrQyhFcFiRru+fLkn6ckHpU/As2V46Hu+nhs3y1apTjg7vc71A0KqTI8A/EGtc/QzVx7GDfsqzmOUfaFPrzhbt3OJ6cah1J0egc+A31+9LsytKz5D7BdjV+XrHdy04lPvjgPJUpYMlPz8/+fmV8pBLXJDu/TPkH2iTzeYohXohruzpyAIcP8S9IpWNtZbjJ8L7WIFMOVbZg/4UKBfZ5XPI8QffWsf3jzaODKQ52yavxAJZG7hegfeJy/2jzYXN1ff7Y6qf0XuWxpxaKN/djhOWwlZlLFWOKqXPsDT5+tuVeNhPu348d7GZLV+G686Jp3TT7WnasNg5sPIPtKpbP8fU0c1rwy/XcFFJFBWa9fPGUN0wJF033X7aJViq07BA7bo6frO2rD3zAOP3novUe88Z38Pb6bosvbLkoBLjfXVvz3aXb/C4pOx/LJ7uE57BZVZIkmo3KFCf4any8XOde33dgHQ99soRSdJXy2vqdLLzCe6w8UmqbTA1z8/fprsePqFeg9NVVCh9uaj0suOomPK7BMvmb5KpwK7Qt0/IlHvW96PQrpAPT8kruUh2bynveseJQmGbgJIgK+ztEzJlnDX1zWZX0NIU+e5zBEu5vc6cQEhS0JIU5/3/YMq2KmT2SfkcyJMt0Ky8nqFO2wNXpcmU6drO+3CearyUIFOBXUX1fJTXjaqM1VH/kY4peF8urCXp3FNXls+pozyLWV16ZemWu5JL1pvNdj340lGFhFm179dA/fJt6DneBdXFwrfqyGaT+o9KU9cbz9wX7edv06SZR+XlLX23JkwJB7lYCFRWVTqzVJ217GApeeaRJNVv4ghmBo9NKbkyKkn/vK+F0pJ8FBJepCffPKKHpifoYGygUk76yM/fpsat8xTZ3PF8iF83BxuWOL39viRNeO6YEvb7K+GQnwrzzapZp1DN2+UqJNyqgjyTXn+ysY4YTGlBxWYP81bmxPoKm5WogC1Z8v39gApb+kteJvkcdDz7yG6WMu+rJ2u9P7I93iZlPNJA4S8lyPd3i2r/30EVtgqQPcAs78N58j7pmKaXfUctFbZ3zlKG/C9ZwfOTVdTET9Z6vrJ7meSVWijv+DyZ8+yyBZqV/kRD2cKdf7qCP0tWyEenVNTMX9a6PrKbJO+TjnYmm2St7a3TTzeSfLg+VBm17GDR36ed/Xvm+E0a9JdkXXtW4ZgXJ7RwqSzWor1FLTvkylokbVhy/gs2aad8NXNyU02ZdUiPvnxUA0al6tQxX7XunKMGTQqUluStfz3UTOcLulD5tOxg0YMzjpW8Lv6eDR6bqm79zgRC/xzfrOR7diA2ULP/2UATn0/Ui58c0m8/Bis9xVsdrs1WrXpFSjjgpzefZPo/UJkRLFVRgSFWXdHF9abQ2g0KS244lSQfX0emIDnRVwvfrqvWnS1q0DRfLTta5O1jV2aat37cEKqvP6+pTStrGN5QOPflBupyQ6Zad7KoY7dsBYdalWcxK/GIn9YtCNHqj2vr5FGmQ1ZWeTeGqaiJnwJXpcn3d4v8dlkku2Sr4a3c3qGyDK7pUvyhoFOQUt9orsCVafLblSPfPRbJZpct1Ft53UNkuSVcBVe6ZnkyJ9SVz+5c+RzKk++uHJnybLIHmFXUxF8FVwbJMrCGS6AkSdl3Rsh3j0XeCfny/bVApnybo/pemwDldQtR7oBw2QOoXlZZBQZbdUUX12Igpf2ena04q/TLplClnSrb9M3v1tTQiaNtNfrvJ9Xh2my1aG/R6SQfrZxbW/PfrF9S7AZVy4X+3Sy2/L91dHhvgEZMSFKbqyzyD7ApKdFXC2bV1IJZdUt9YC2qjvIouECBB88x2e0V87FW27dv1wMPPFDy+uDBg0pJSVFkZKQaNmxYsn758uWqX79sD5rMzMxUWFiYbjQPl7eJP3a4fE4ua13eQ0A1UP+O/eU9BFQDdisFBnB5FdkL9Y39c2VkZCg0tPJMcS0+r2z+8VPyCvTsVEurJU+Hxk2vdMesMqqwmaXMzEz99NNPLuuPHTumY8fOpMnz8/M9OSwAAADgDCo8VGkVNli68cYbVUGTXgAAAACqgQobLAEAAAAVXnk8JJZ7ljyG0lAAAAAAYIBgCQAAAAAMMA0PAAAAcJPd7lg83Sc8g8wSAAAAABggswQAAAC4iYfSVm1klgAAAADAAMESAAAAABhgGh4AAADgLrvJ8889Yhqex5BZAgAAAAADBEsAAACAm4pLh3t6ccfRo0f14IMPqk2bNgoICJC/v7+aNWume+65Rzt37iy1XUxMjAYNGqSIiAgFBASobdu2evrpp5WdnX3O/g4cOKCoqChFRkbKz89PkZGRioqK0qFDh87ZLisrS0899VTJOCMiIjR48GB99dVXbn3ui0GwBAAAAFRxP/30kzp06KC3335bOTk56t+/vwYNGiSTyaSPP/5YXbt21eLFi13avfbaa7r55pu1bt06tW/fXkOGDFFGRoamT5+url27KiUlxbC/zZs3q3Pnzpo3b57Cw8M1bNgwhYeHa968eerUqZN+/PFHw3ZJSUnq2rWrZsyYoaysLA0ZMkTt27fX2rVr1a9fP82aNeuSHpfzIVgCAAAA3GUvp+UCTZgwQVlZWZowYYLi4+O1YsUKLVu2TAcOHNAzzzyjoqIiTZgwQXl5eSVtduzYocmTJ8vLy0tr1qzRpk2btGjRIh08eFB9+/bVvn37NHHiRJe+LBaLRo4cKYvFoujoaMXGxmrBggWKjY1VdHS0cnJyNHLkSOXm5hqOMy4uTn379tWBAwe0aNEibdq0SatXr5bZbNajjz6qXbt2XfgBcBPBEgAAAFCFpaamlgQY06ZNk4+PT8k2s9ms559/XgEBAUpPT9eePXtKts2YMUN2u1333nuvBg4cWLI+MDBQc+bMkdls1tKlS7V3716n/ubOnavExES1bt1a06ZNc9o2bdo0tW7dWgkJCfr444+dtu3evVsrVqyQl5eX5syZo8DAwJJtgwYNUlRUlGw2m2bMmHHxB6WMCJYAAACAKszPz6/M+0ZEREiSCgoKtGbNGknSmDFjXPZr0qSJevToIUlavny507bi16NHj5bZ7BxumM1mjRo1SpK0bNkyw3Y9evRQkyZNXPosHseqVatUWFhY5s90MQiWAAAAADfZ7aZyWS5EcHCwevXqJUl65plnnAINm82m559/Xrm5uRo4cKAaNWokSYqLi5PFYpEkde3a1fB9i9fv2LHDaX3x68vVLicnR/v37zfc51Ir03OW/pwiu1Djxo27qPYAAAAAnGVmZjq99vPzKzWL9N///leDBg3S7NmztWbNGnXt2lVeXl7asWOHjh8/rrvvvltvvfVWyf7x8fGSpPDwcIWEhBi+Z3FgVbyv5Khkl5qaKklq3LjxOdslJycrJydHQUFBTu9TWrvQ0FCFhoYqMzNT8fHxateuneF+l1KZgqWoqCiZTO4//IpgCQAAAFWWm6W8L1Zx0FHsueee0/PPP2+4b5s2bfTDDz/o7rvv1vr163X8+PGSbe3atdONN96o0NDQknVZWVmSVBLIGAkODpbkHLQVtztX2+J2xW2L9ytrn5mZmS6B4uVSpmBp3LhxFxUsAQAAALi0EhISnAKcc92btHnzZg0fPlze3t6aP3+++vTpI19fX23evFmTJk3S+PHjtXnzZs2ZM8cTQ680yhQszZ079zIPAwAAAMCFKJ6Wdj7p6ekaNmyYUlJS9MMPP6hbt24l22699Va1a9dOHTt21IcffqixY8fqpptuKpl6l5OTU+r7Fj+U9uwxnD1lr7S2Zz/M1qjthfZ5OVHgAQAAAHBTZSjwsGbNGiUnJ6t58+ZOgVKxs9fHxMRIkpo2bSrJEWidPbXubAkJCU77So6Ap2bNmpKko0ePnrNdRESE05S74vcprd3Z0+/O7vNyuiTBUkFBgU6cOKG0tLRL8XYAAAAALpHi4ONc2ZiwsDBJKjmfb9OmTclzjrZt22bYpnh9ly5dnNYXv75c7YKCgtS6detSP8uldFHB0v/+9z9de+21CgoKUmRkpB5//PGSbcuXL9eYMWOcqmMAAAAAVYq9nJYL0LBhQ0nS3r17lZGR4bK9sLBQ27dvlyQ1a9ZMkuTr66vBgwdLkubPn+/S5siRI9qyZYskadiwYU7bil8vWLBANpvNaZvNZtPChQslScOHD3fadvvtt0ty3F9llF0qHseQIUOcHqx7ObkdLN1333265557tG3bNgUEBMhud/5Xa926tRYsWKClS5de9CABAAAAuGfgwIEKCgpSbm6u/va3vzndM1RQUKDHHntMR48elY+Pj+64446SbVOmTJHJZNJHH32kdevWlay3WCwaP368rFarRowYobZt2zr1FxUVpQYNGiguLk5Tp0512jZ16lTFxcUpMjLSpWJ2+/btNXToUFmtVo0fP165ubkl29auXau5c+fKbDYrOjr6khyXsnArWPr000/14YcfqkOHDtq6dathhNq+fXtFRkZq7dq1Fz1IAAAAoGIyldNSdrVr19Z7770nb29vLV68WM2bN9fgwYN1++23q3nz5nr77bdlNpv15ptvqnnz5iXtunTpopkzZ8pqtWrQoEG66aabNGrUKLVs2VIbN25UmzZt9N5777n0FxgYqEWLFikwMFDTp09Xx44dddddd6ljx46aPn26goKCtHjxYgUEBLi0nT17tlq1aqWYmBi1aNFCo0aN0k033aTBgwfLarXqtddeU6dOnS7o818Mt4Kl2bNnKzg4WKtXr9bVV19dalnxjh07Mg0PAAAAKGdjx47Vtm3bFBUVpZCQEG3cuFFr166Vt7e3/vKXv+iHH37QxIkTXdo99thj2rBhgwYMGKBdu3ZpxYoVCg4OVnR0tLZu3aqIiAjD/nr06KGdO3dq3LhxSktL09KlS5WWlqZx48Zp586d6t69u2G7OnXqaNu2bZoyZYqCg4O1YsUK7dq1SwMGDFBMTIwefvjhS3pczqdMpcP/bOfOnerWrZvLg7D+rGbNmjp16pRbAwMAAABw6XTu3FkfffTRBbfr16+f+vXrd8HtWrZsqXnz5l1wu9DQUM2YMUMzZsy44LaXmlvBUn5+fknFjHNJTk6Wl5eXO10AAAAAFZ8bBRcuSZ/wCLem4TVs2FB79uw55z52u127d+8uqagBAAAAAJWJW8FS3759tXfvXq1YsaLUfT755BMdO3ZMN998s9uDAwAAACq0SlA6HO5zK1h6/PHH5efnpzFjxuj1119XYmJiyba0tDS99957euCBBxQUFOTxm7AAAAAA4FJwK1hq1aqV5s2bJ5vNpsmTJ6tRo0YymUyaN2+eateurb///e8qKirS3Llz1bhx40s9ZgAAAAC47Nx+KO2dd96prVu36s4771RISIjsdrvsdrv8/f01ZMgQ/fDDDxoxYsSlHCsAAABQsdhN5bPAI9yqhlesQ4cOWrBggex2u1JTU2Wz2RQRESGz2e0YDAAAAAAqhIsKloqZTKZSH0gFAAAAVFV2u2PxdJ/wjIsOlgoKCrR9+3YdO3ZMdrtdkZGRuvrqq+Xr63spxgcAAAAA5cLtYKmgoEAvvPCC3nnnHWVmZjptCwkJ0f/93//p+eefl5+f30UPEgAAAKiQeChtleZWsJSXl6f+/ftr8+bNstvtqlWrlpo2bSpJOnz4sFJTU/Xvf/9b33//vTZs2CB/f/9LOWYAAAAAuOzcqsTw8ssv6/vvv1erVq20atUqJScna+vWrdq6dauSk5O1evVqtW7dWlu2bNG///3vSz1mAAAAALjs3AqW5s+fr+DgYH311VcaPHiwy/ZBgwZp48aNCgwM1KeffnrRgwQAAAAqJEqHV2luBUtHjx7VTTfdpAYNGpS6T4MGDdSnTx8dPXrU7cEBAAAAQHlx656lGjVqKCAg4Lz7+fv7q0aNGu50AQAAAFR4Jrtj8XSf8Ay3Mkv9+vXTt99+q/z8/FL3ycvL03fffac+ffq4PTgAAAAAKC9uBUvTpk1TYWGhxowZo6SkJJftKSkpGjt2rAoLC/XSSy9d9CABAAAAwNPKNA3vn//8p8u6wYMH6+OPP9aXX36p/v37q1mzZpKk+Ph4rV+/Xrm5uRo3bpw+/vhjTZ069dKOGgAAAKgIeM5SlVamYOn555+XyWSS3X7mX8ZkclThsFgs+vzzzw3bzZs3TyaTiWAJAAAAQKVTpmDpueeeu9zjAAAAACqf8ijlTelwjyFYAgAAAAADbpUOBwAAACDuWari3KqGBwAAAABV3UVllvLz8/X1119r3759yszMdCoAUYwCDwAAAAAqI7eDpeXLl+v+++9XampqqfvY7XaCJQAAAFRdTMOr0twKlrZt26ZRo0ZJkkaPHq3ff/9dv/32m6ZMmaL9+/drw4YNyszM1Pjx4xUZGXlJBwwAAAAAnuBWsPSf//xHVqtVy5cv12233aZ7771Xv/32m1566SVJUnJyssaNG6e1a9dqx44dl3TAAAAAQIVBZqlKc6vAw+bNm9WuXTvddttthttr166tBQsWKCcnRy+88MJFDRAAAAAAyoNbwVJycrLatm1b8trb25GgysvLK1kXFham3r1764svvrjIIQIAAACA57kVLIWEhKioqKjkdVhYmCQpMTHRaT8fHx+dPHnyIoYHAAAAVGB2U/ks8Ai3gqXIyEglJCSUvC7OMn399dcl6woLC/Xjjz+qbt26FzlEAAAAAPA8two89OzZUx988IEyMjIUFhamwYMHy9vbW5MmTVJeXp4aN26s2bNnKzExUX/5y18u9ZgBAACACsFkdyye7hOe4VZm6fbbb1dkZKQ2bdokSapfv76eeuopZWVl6eGHH9btt9+uNWvWKDw8XNOmTbukAwYAAAAAT3Ars9S3b1/t37/fad1zzz2njh07avHixUpLS9MVV1yhRx99VI0bN74kAwUAAAAAT3IrWCrN8OHDNXz48Ev5lgAAAEDFxXOWqjS3puEBAAAAQFVHsAQAAAAABso0Da9Pnz5ud2AymbRx40a32wMAAABAeShTsPTNN9+43YHJxEOzAAAAUDWZVA6lwz3bXbVWpmDp7IfNAgAAAEB1UKZgqXfv3pd7HAAAAABQoVzS0uGVhs0qmahtgcun3vC48h4CqgFzq2blPQRUA9a4g+U9BKBis5sci6f7hEcQMQAAAACAgeqZWQIAAAAuBR5KW6WRWQIAAAAAAwRLAAAAAGCAaXgAAACAu5iGV6WRWQIAAAAAA2SWAAAAADeZ7I7F033CMy46WMrIyNDWrVuVnJysJk2a6Prrr78U4wIAAACAcuX2NLysrCzdd999qlOnjgYMGKCxY8fqgw8+KNn+wQcfqEGDBvrpp58uyUABAACACsdeTgs8wq1gKTc3VzfeeKM+/PBD1ahRQwMHDpTd7vyvduutt+rUqVP6/PPPL8U4AQAAAMCj3AqWXn31Ve3YsUN33XWXDh48qNWrV7vsU69ePV1xxRX6+uuvL3qQAAAAAOBpbgVLCxcuVL169TRnzhwFBQWVul/r1q117NgxtwcHAAAAVGhMw6vS3AqWDh48qGuvvVb+/v7n3C8wMFApKSluDQwAAAAAypNb1fC8vLxUWFh43v2OHTt2zswTAAAAUJlROrxqcyuz1KJFC+3cuVNFRUWl7pOdna1du3bpiiuucHtwAAAAAFBe3AqWbrvtNp04cULTpk0rdZ9p06YpIyNDw4YNc3twAAAAAFBe3JqG99hjj+mjjz7Siy++qF9//VUjR46UJJ06dUrLli3TokWLtHjxYjVt2lQTJ068pAMGAAAAKgy7ybF4uk94hFvBUnh4uNatW6fbbrtNK1eu1KpVq2QymbRu3TqtW7dOdrtdTZo00apVq7hnCQAAAECl5FawJEnt2rVTbGys5s6dqy+++EKHDh2SzWZTo0aNNHDgQE2YMEGBgYGXcqwAAABAxVIepbwp8OAxbgdLkuTv76+JEycy1Q4AAABAlXNRwRIAAABQnVE6vGpzqxoeAAAAAFR1bmWWmjdvXuZ9TSaTDh486E43AAAAAFBu3AqWDh8+fN59TCaT7Ha7TCZKGwIAAKCKosBDleZWsBQfH2+43maz6ciRI1q9erVmzZql6Oho3XvvvRc1QAAAAAAoD24FS02aNCl1W7NmzXTjjTeqW7duuuuuu9S7d+9z7g8AAABUWuVQ4IHMkudctgIPd955p6644grNmDHjcnUBAAAAAJfNZa2Gd8UVV2jr1q2XswsAAAAAuCwu63OWjh8/roKCgsvZBQAAAFB+KPBQpV22zNL//vc//fDDD2rXrt3l6gIAAAAALhu3Mkt//etfS92WlZWlvXv3avfu3TKZTHrkkUfcHhwAAABQoVWyzFJBQYHee+89LVq0SLt375bFYlFERIQ6duyoqKgojRo1yqVNTEyMXn31Vf3888/KyclRkyZNNGLECEVHRys4OLjUvg4cOKBp06YpJiZGycnJql27tvr166dnn332nM9tzcrK0owZM7R06VIdPXpUQUFB6tatmyZPnqw+ffq4/+Hd4FawNHfu3PPuExoaqhdeeEFjx451pwsAAAAAl9CxY8c0YMAA7d69WxEREerRo4eCgoKUkJCgb7/9VkFBQS7B0muvvaZJkybJZDKpV69eqlu3rr777jtNnz5dS5cu1ffff6+IiAiXvjZv3qz+/fvLYrGoffv26tmzp2JjYzVv3jwtWbJEMTEx6t69u0u7pKQk9erVS3Fxcapfv76GDBmiU6dOae3atVq7dq3eeOMNPfTQQ5ftGP2ZW8HSRx99VOo2X19fNWzYUNdee638/f3dHhgAAABQ0ZnKoXS4O/3l5ubq5ptv1t69e/X888/rqaeeko+PT8l2i8WiuLg4pzY7duzQ5MmT5eXlpVWrVmngwIEl+952223auHGjJk6cqCVLlji1s1gsGjlypCwWi6KjozV9+vSSbU899ZRmzJihkSNHat++fQoICHBqO2HCBMXFxalv375auXKlAgMDJUlffPGFbrvtNj366KPq3bu3OnXqdOEHwQ0mu91ebW4Ry8zMVFhYmG7UUHmbfM7fAHCX2au8R4BqwKtVs/IeAqoBa9zB8h4Cqrgie6G+sX+ujIwMhYaGlvdwyqz4vLLFU9Pl5eEEgTUvTwenP3VBx+zZZ5/Viy++qAkTJuj9998vU5uRI0dq8eLFuu+++/Tf//7XaduRI0fUvHlz2Ww27dmzR23bti3Z9s477+jvf/+7WrdurT179shsPlMmwWaz6YorrlBcXJzee+893X///SXbdu/erfbt28vLy0sHDx50eVbrfffdpzlz5mj06NH67LPPyvQZLpZbBR7++te/6oknnrjUYwEAAABwiRUWFurdd9+VJP3jH/8oU5uCggKtWbNGkjRmzBiX7U2aNFGPHj0kScuXL3faVvx69OjRToGSJJnN5pKpfsuWLTNs16NHD5dA6exxrFq1SoWFhWX6HBfLrWl4//vf/zR06NBLPRYAAAAAl9j27duVkpKiBg0aqGXLlvrtt9+0bNkyJSYmqkaNGurVq5cGDhzoFNjExcXJYrFIkrp27Wr4vl27dtV3332nHTt2OK0vfn2udmfvd6HtcnJytH//fo9U3XYrWKpXr55MJtOlHgsAAACAS2zXrl2SpMjISE2ZMkX//ve/dfadOC+//LKuuuoqff7552rcuLEkKT4+XpIUHh6ukJAQw/dt1KiR076So5JdamqqJJW8V2ntkpOTlZOTo6CgIKf3Ka1daGioQkNDlZmZqfj4eI8ES25Nw7v55pu1efNmj6W/AAAAgArJXk6LHPdNnb3k5+cbDrE4eNmxY4defvllPfDAA9q3b58yMjK0YcMGtW7dWjt27NDgwYNLzu+zsrIkqSSQMVJcNjwzM7NkXXG7c7U9u9y4UdsL7fNycitYev7555Wfn6+//e1vTgcEAAAAgGc0atRIYWFhJcuMGTMM9yvOIhUWFuquu+7SW2+9pdatWys0NFT9+vXThg0b5O/vr9jYWC1YsMCTH6HCc7t0+C233KKPP/5Ya9asUb9+/dS0aVOX0n+SZDKZNHXq1IseKAAAAIAzEhISnKrh+fn5Ge539jS6s6vPFWvcuLEGDx6spUuXKiYmRnfffXdJm5ycnFL7z87OliSnMZzdV2lti9uV1vZC+7ycyhQs9enTR7fccktJBbznn3++5J6l1NRULVy40KWNyWSS3W4nWAIAAECVVZ7PWSq+h+d8mjdvbvjfRvucOHFCktS0aVNJUnp6urKysgzvW0pISHDaV3IEPDVr1lRaWpqOHj2qzp07l9ouIiLCacpd06ZNtX37dh09etRwjMXTDf/c5+VUpmDpm2++cRrQs88+S4EHAAAAoBLo0qVLSSIjJSWlpMDC2VJSUiSduSeoTZs2CgwMlMVi0bZt23TTTTe5tNm2bVvJ+/+5v5iYGG3btk1Dhgy5oHbLli0r2V5au6CgILVu3fqcn/lScWsa3vPPP3+JhwEAAABUUh7OLF2oevXqqWfPnvruu+8UExOjq666yml7YWGhNm3aJEm69tprJUm+vr4aPHiwFi9erPnz57sES0eOHNGWLVskScOGDXPaNmzYMMXExGjBggV67rnnXB5KWzwrbfjw4U7tbr/9dj3zzDPavHmzjh496lIVb/78+ZKkIUOGyMfHx61jcaHcKvAAAAAAoPJ47rnnJEkzZszQjz/+WLK+qKhIkydP1qFDhxQSEqJ77723ZNuUKVNkMpn00Ucfad26dSXrLRaLxo8fL6vVqhEjRqht27ZOfUVFRalBgwaKi4tzuR1n6tSpiouLU2RkpMaNG+e0rX379ho6dKisVqvGjx+v3Nzckm1r167V3LlzZTabFR0dffEHpIzcyiwBAAAAkFMpb4/2eYH69u2rF198UVOnTlWvXr107bXXql69etq+fbsOHz6sgIAAffbZZ6pbt25Jmy5dumjmzJmaNGmSBg0apN69e6tOnTr67rvvdOLECbVp00bvvfeeS1+BgYFatGiR+vfvr+nTp2vlypXq0KGDYmNjFRsbq6CgIC1evNiwONzs2bO1e/duxcTEqEWLFurVq5eSkpK0adMm2e12vfHGG+rUqdOFHwA3kVkCAAAAqoFnnnlGX375pW6++Wbt3btXq1atktVqVVRUlLZv367Bgwe7tHnssce0YcMGDRgwQLt27dKKFSsUHBys6Ohobd26VREREYZ99ejRQzt37tS4ceOUlpampUuXKi0tTePGjdPOnTvVvXt3w3Z16tTRtm3bNGXKFAUHB2vFihXatWuXBgwYoJiYGD388MOX9Jicj8l+9uN7S2E2m90u6GAymVRUVORW20stMzNTYWFhulFD5W3yzDxHVFNmr/IeAaoBr1bNynsIqAascQfLewio4orshfrG/rkyMjI8Vg76Uig+r2z55HR5+fl7tG9rfp4OvPxUpTtmlVGZp+GVIaYCAAAAqpXyLB2Oy6/MwdItt9yiJ5988nKOBQAAAAAqjDIHS/Xq1VPv3r0v51gAAACAyqWSFHiAeyjwAAAAAAAGCJYAAAAAwADPWQIAAADcRIGHqo3MEgAAAAAYKFNmyWazXe5xAAAAAJUPBR6qNDJLAAAAAGCAe5YAAAAAd5FZqtLILAEAAACAAYIlAAAAADDANDwAAADATZQOr9rILAEAAACAATJLAAAAgLso8FClkVkCAAAAAAMESwAAAABggGl4AAAAgLuYhlelkVkCAAAAAANkllDCy9uujt2z1fWmLHW6LlsNm+XLP9CmzNPe2vdroL74pJZ+3hha3sNEBRLZPE9demeqVUeLWnWyqHHLPHl5S3P/XV+fvVnfsE3t+gW6pk+GWnWyqFVHi5q0yZOvn11rP6ul1//RpNS+2l2drT7D09SiQ67qNCxQaHiRbDaTTh3z1a+bQ7T0/To6dczvcn1UXGYNG2WpS9dTatk6Xa1an1ajJlny8rLr4znttOB/Vxi2+cs9u/WXqD3nfN8J427WsQTX3y2z2aaBQ+J1Y98ENW6aqYCAIuXk+Cj+YJg2rm+ir9Y3lt1ucmpTp26O5i5Yd87+Fs1vrbn/7XieT4vKqHaDAo18IEldb8pURP1C5eaYtX9XoFZ8GKGfN4a57P/l8V/L9L6vPNJYMUtqXuLRwpMoHV61ESyhRKfrsvWvhYckSamnvPX7z0HKyzWrcat8Xdc/U9f1z9SaT2rqzScjJZnO/WaoFm4dl6xh9yVfUJueg9I18YVjF9zXNX0yNeSeFJ065qtjB/10OjlYQaFWtexg0dB7k9V/ZKqejWqhXT+EXPB7o/wNvu2Qbr/jgFttDx4I06ED4YbbcnJ8XNZ5+1j10ivfq2PnFBUWmPV7bC1lpPupdp1cdboyWVd2SdZ1PRI17dnuMvqty8310vebIg37OxBXw63PgIqtdWeLXvr0oEJrWJV60lvbvg5RaA2rOl+fra43Zul/r9bVJzOdLxCtX1T6d6FOw0Jd2SNbNpu064fgyz18ABehQgZLhYWF+vbbb7Vu3Tp988032r9/v3JyclSrVi1de+21uv/++zV48ODyHmaVY7NJ360O0+cfRCj2Z+cf7963ndaTbx3V4LvTtHtrEFfBIEk6vC9Ai9+ro4OxgTrwW6BGP3RS/e5IO2ebkwm++vzD2jrwW6AOxAbohlvTNeaRk+ft66vlNbTus1ou2SNvH5vue/q4ht2XrCfeOKxx3TvIZiOYr2yOxIdqyYJWOnQgXAfiwjVq7D717X+0TG1//L6BPp3Xrsx93Tr0kDp2TtGpk4F64pHeSk4KLNnWqvVpzXj1W13fK1E33HRM337dyKV9ZoafXnu5a5n7Q+Xm42fT1P/GK7SGVd+sCNfMSY1VkOe4i6F1Z4um/e+gxk46pd9/Dtb2785crJn5WOmZ8genJ+jKHtna8V2Iko77XvbPAMB9FTJY2rRpk26++WZJUr169dSzZ08FBQVp9+7dWrVqlVatWqUJEybovffek8nESdGlsnNziHZuNr4qv2llDV11Q7YGjklTvztPEyxBkrTuswin1zbb+dv8sD5cP6wPL3ndY2B6mfpKOBBguL6o0Kz/TovUoLEpqt2gUI1b5+nwXuN9UXF9+UUzp9eXM+DtfFWSJGn1582dAiVJ2h9XQ5u+jtTAWw/rivaphsESqpcet2SoTsNCZaV76c0pkSWBkiTF7QzUp6/V0wMvHtdfHjvpFCyVxsfPphuHpkuSvlzA39IqgQIPVVqFLPBgNps1YsQIffvttzpx4oRWr16thQsX6rffftOCBQvk5eWl2bNn65NPPinvoVYrB2MdJ6ARDQrLeSSAM7tdsv9xcl2YzwUUnFtBgVeZ9svM4B44SK2vtEiSDvwWoJxM12vMO/4IkNpdk6Matc//97HnoHSFhFuVedpLW9a53usEoGKpkJmlPn36qE+fPobbRo0apQ0bNmjOnDn6+OOPNW7cOA+Prvpq2CxfkpR2qkJ+bVBNmc12jX3shPwDbTqyz1+JhznBrW5atEpX1N9+U0hogXKyfXToQLh+2lJfubmu9ytJ0raf66nXjcd16+2H9O3XjZyySy1bn1bvm44pL89LG9c3Nmzv71+kO+/ap7r1clRUZNaJxCBt+7mejidwv1xVFBDoSJlnnjb+25eR5gi+zWapZcdcbf3K+HtXbMBox1TljUtrqLCgQl6zxgWiwEPVVinPeq+66ipJUkJCQjmPpPqoUbtQN490/MB//wVXwlB+ajco0LjHEyVJIeGOAg+1GxTqeLyfXvq/Zi4VzFD1de9xQt17nHBal53to/dmddZX613vG4lZ10QdOyer34Cj+uCTL/V7bC2ln3YUeLiifaoOx4fprVevUtKpIMP+wsILdO+EWKd1f7Pt0jcxjfXWa1cpL69S/mlFKdJTHf+e9RrnG26v36Sg5L/rNTLep1jdyHx1vj5bkvTlglqXaIQALqdK+Yu+f/9+SVL9+saliXFpmb3sevKtowoOs+nQbn998Qk/8Cg/IeFF6j/SuYjE/l0BenVyEx2J416l6uREYpDm/re9tv1UT0mnHNmhxk0zdedd+9Tt+pN6PHqbbDaTvolxzhDZ7Sa9+q+uij8Ypqi/xerKLmcqOubleunXX+roRKJroFRYaNba1U31/aZIHT0coqxMP9Wtn6MevY7rzjH71Kf/UYXVyNPUJ3qKiqFVx6+bgzXmkVNq1SlXLdpbdPB35/vcBt+dUvLfgSHnvnGz/6g0mc3Svl8DFL+H36sqg3uWqrRKFyydPHlSc+fOlSSNGDGifAdTTTz88jFd1StbGWlemjahqYoKmTaA8nNod6AGRHaRZFeteoVqd3WOxj2eqLfW7tX7/4zUig/rlPcQ4SFfbXDNGu2OjdALT0fo/od+1dDhBzXhgV36/ptIFRWd+d0KCCzUk1N/VtdrT2rF0pZas6K5UlMCVK9+jkb+ZZ+Gj9yvHjcc1z8e7q2U5DMnxqfTAjRr5tVO/SUcCdWCI6Havq2uZr71ja6+JknX9UjUD5sbXr4PDo/auTlEu34IUqfrcvTC3HjNeipSv/0YrNAaRRpyT4r63XFahQUm+fjaZT9HrGQy2UtmaKxfyEVHoLKoVGe9RUVFGjt2rDIyMtSxY0fdf//959w/Pz9fmZmZTgsuzMR/HtfAMWnKOu2l6NHNdfwQ94OgojAp9aSvvltTQ48ObaPTKT66/7ljan6FpbwHhgrg07ntZLWaFF4jX22ucM5E/u3/duna7if1xcrm+u87nZV4PET5+d46cjhMr7x0rbb9XFd161k0bvzvZe4vbm9N/bTFMdvh2uvPXwoflcu0+5sq9ucg1W5QqH/Ojdfyvb9p3g97dMfEZH0+p7YO7faXJGWll34N+qpeWaobWai8XJO+Ws7zuIDKolJlliZOnKiNGzeqVq1aWrJkiXx9z/1sghkzZuiFF17w0OiqngnPJmrYfSnKSvdS9F3NdTA28PyNgHKQk+mtLevCdVtUsrr3z9ChPXxXq7vsLF+ln/ZTrYg8RdTOLVlvNtvV54/nN32z0bgs+DcbG6nrtad0ZZekC+oz4aijwENEbQL2qiYj1UeTh7VUl17Z6twjS6E1rDqd4q0fvgzT/l2Bmv+L4x62+L3+pb5HcWGH778IlyWrbBUZUUkwDa9KqzTB0iOPPKI5c+aoRo0a2rBhg1q3bn3eNtHR0Zo0aVLJ68zMTDVqxDMzymL8M4kaMTFZ2RlmPXVXc+3fxcknKrY8iyNRHh5RVM4jQUVgNtsVFOQo45xrOfOnLiw8T76+jrlSFotx1TJLjmN9SGiB4fbSFO+fW8r7orIzaft3IS7PUqrfJF+16hUpI81LB34zvg8pJLxI1w/IkCR9+RnPVgIqk0oRLE2ePFlvvvmmwsPDtX79+pJqeOfj5+cnPz+mjV2ovz6VqJEPOAKl6NEtFLeTQAkVX+frsyRJx5gqCkndrk+Uf4BVNpu0f9+ZKU9ZmX4qKDDL19emNlek6fAh1+qebf+YtnfqhHE1PCN+/kXqdp2jIl/cXqZYVSd3THRkINd+WqvUe3pvGnZavv52Jcb7atcPwZ4cHjzAJM+XdKGEjOdU+HuWnnjiCb366qsKCwvT+vXr1bVr1/IeUpV2zxMnNOrBZMfUOwIlVCCj/n5SYTVdH/gYHFakB15MUJsrLcrO8NK3qzhRrQ5q17Hopn5H5eNjddl2XY/jeuTx7ZKkb2Ia6/TpM1OjiorMJfcW3f3X39W0eYZT285dkjT0DkfF1T9P07vl1kOGU+zq1svRs9O2qFZEnrKyfLT+i6YX9dlQ8TRulafAYOfvmtnLrtEPndKgsak6Hu+r+W/ULbV98RS8LxfWEqe5QOVSoTNLU6ZM0SuvvKKwsDBt2LBB11xzTXkPqUrr3j9DYx51XCFLPOyrIVEphvtlnvbWf//ZwJNDQwXVsoNFD04/WvK6+Hkjg8emqFu/Myeh/7yvhdKSHFOTatYp1LMfHCzZFlHfEQBdd3OGmq7cW7L+raca68BZ98n9NTpR9/wjUfF7A3TiiJ+sRSZF1CtQiw65CgiyKTvDSy9NbKb0FKZAVUYtWp3W3x/9teR1/QaOZ9EMHBKva687UzDhxanddTotQCEhBfrH01v198d26ND+cKWk+MvPz6rGTbLUsJGj7c7ttfXWa64zEWa/3VktW6WrfsMczZq9UXt31yyphte67WlJ0q/ba2vpQufp3oOHHtKDj+3Q0cOhOn4sWIWFZtWrb1Hzluny9bUpI8NXLz3bXZmZZDermkFjUzToL6na/1ugUk/6yMfXprZdLKpZp0jH430VPbqF8nON70Nq0d6ilh1yZS2S1i9iCh5Q2VTYYOmZZ57Ryy+/XDL1jkDp8gsJP3PVrM2VuWpzZa7hficTfAiWIEkKDLHqii6uV9prNyhU7QZnskA+vjan/zZqEx5R5HS/UWCI81Xct55upA7XZqtFB4uu7JGlgCCrcnO8FL83QL9sCtHqj2sTKFVigYFFatsuzWV97Tq5ql3nzG+Rj4/ju5ScHKBF81urddvTatAgRy1an5a3t02ZGX76aUs9fbOxsb79OtLwIcWpKQF68G99ddvwg+reI1FNm2Wobbs05WT7aNevEfpmYyOt/6KZbDbntiuXtdTV15xS0+YZ6tg5RYFBhcrN9Vb8wTBt+6meVq9oroz00m/wR+X188ZQ1Y0sUMuOuWrdyaLCApOOHfTX0vfraOXcCBXklT5RpzirtG1TqNJO8RtVJVHgoUoz2e32Cne4V65cqaFDh0qSunbtqvbt2xvuFxERof/85z9lft/MzEyFhYXpRg2Vt4kfLFxGZiod4fLzatWsvIeAasAad/D8OwEXocheqG/snysjI0OhoaHlPZwyKz6vbPd/0+Xl59kLJdb8PO1+96lKd8wqowqZWUpLO3N1cdu2bdq2bZvhfk2aNLmgYAkAAAC4lEx2x+LpPuEZFbLAQ1RUlOx2+3mXw4cPl/dQAQAAAFRRFTKzBAAAAFQK3LNUpVXIzBIAAAAAlDeCJQAAAAAwwDQ8AAAA4GIwLa7KIrMEAAAAAAbILAEAAABuonR41UZmCQAAAAAMECwBAAAAgAGm4QEAAADu4jlLVRqZJQAAAAAwQGYJAAAAcBMFHqo2MksAAAAAYIDMEgAAAOAu7lmq0sgsAQAAAIABgiUAAAAAMMA0PAAAAMBNFHio2sgsAQAAAIABMksAAACAuyjwUKWRWQIAAAAAAwRLAAAAAGCAaXgAAACAu5iGV6WRWQIAAAAAA2SWAAAAADdROrxqI7MEAAAAAAbILAEAAADu4p6lKo3MEgAAAAAYIFgCAAAAAANMwwMAAADcZLLbZbJ7dl6cp/urzsgsAQAAAIABMksAAACAuyjwUKWRWQIAAAAAAwRLAAAAAGCAaXgAAACAm0x2x+LpPuEZZJYAAAAAwACZJQAAAMBdFHio0sgsAQAAANXQE088IZPJJJPJpGnTppW6X0xMjAYNGqSIiAgFBASobdu2evrpp5WdnX3O9z9w4ICioqIUGRkpPz8/RUZGKioqSocOHTpnu6ysLD311FNq06aNAgICFBERocGDB+urr75y63NeDIIlAAAAwE3F9yx5erlYW7Zs0cyZM2Uymc6532uvvaabb75Z69atU/v27TVkyBBlZGRo+vTp6tq1q1JSUgzbbd68WZ07d9a8efMUHh6uYcOGKTw8XPPmzVOnTp30448/GrZLSkpS165dNWPGDGVlZWnIkCFq37691q5dq379+mnWrFkX/dkvBMESAAAAUI1YLBZFRUWpfv36Gjp0aKn77dixQ5MnT5aXl5fWrFmjTZs2adGiRTp48KD69u2rffv2aeLEiYbvP3LkSFksFkVHRys2NlYLFixQbGysoqOjlZOTo5EjRyo3N9el7YQJExQXF6e+ffvqwIEDWrRokTZt2qTVq1fLbDbr0Ucf1a5duy7p8TgXgiUAAACgGomOjtb+/fs1e/ZshYWFlbrfjBkzZLfbde+992rgwIEl6wMDAzVnzhyZzWYtXbpUe/fudWo3d+5cJSYmqnXr1i7T+6ZNm6bWrVsrISFBH3/8sdO23bt3a8WKFfLy8tKcOXMUGBhYsm3QoEGKioqSzWbTjBkzLubjXxCCJQAAAMBd9nJa3PTNN99o1qxZGjdunAYNGlTqfgUFBVqzZo0kacyYMS7bmzRpoh49ekiSli9f7rSt+PXo0aNlNjuHG2azWaNGjZIkLVu2zLBdjx491KRJE5c+i8exatUqFRYWlv4hLyGCJQAAAKAayM7O1l//+lfVrVtXr7/++jn3jYuLk8VikSR17drVcJ/i9Tt27HBaX/z6crXLycnR/v37zzn+S4VgCQAAAHBTZSrw8Pjjjys+Pl7vvvuuatSocc594+PjJUnh4eEKCQkx3KdRo0ZO+0qOSnapqamSpMaNG5+zXXJysnJyclz6LK1daGioQkNDXfq8nHjOEgAAAFAJZWZmOr328/OTn5+f4b7r16/X+++/r9GjR+v2228/73tnZWVJkoKCgkrdJzg42GUcxe3O1ba4XXHb4v3K2mdmZqbLZ79cyCwBAAAAlVCjRo0UFhZWspRW+CAjI0Pjx49X7dq1PV56u7IjswQAAAC46yILLrjdp6SEhISSaWmSSs0qPfroozp27JgWLlyoiIiIMnVRPPXu7Glyf1b8UNqzx3D2lL3S2p79MFujthfa5+VEsAQAAABUQmffw3Muy5cvl7e3t9555x298847TtuKy37PmTNHMTExqlevnhYsWKCmTZtKktLT05WVlWV431JCQoIklewrOQKemjVrKi0tTUePHlXnzp1LbRcREeE05a5p06bavn27jh49avg5zp5+d3aflxPBEgAAAHAR3C244ElFRUXatGlTqdsPHz6sw4cPl5TsbtOmjQIDA2WxWLRt2zbddNNNLm22bdsmSerSpYvT+i5duigmJkbbtm3TkCFDLqjdsmXLSraX1i4oKEitW7cu9bNcStyzBAAAAFRh6enpstvthss999wjSXrxxRdlt9t1+PBhSZKvr68GDx4sSZo/f77Lex45ckRbtmyRJA0bNsxpW/HrBQsWyGazOW2z2WxauHChJGn48OFO24oLT2zevNkwu1Q8jiFDhsjHx6fMn/9iECwBAAAA7rLby2fxgClTpshkMumjjz7SunXrStZbLBaNHz9eVqtVI0aMUNu2bZ3aRUVFqUGDBoqLi9PUqVOdtk2dOlVxcXGKjIzUuHHjnLa1b99eQ4cOldVq1fjx45Wbm1uybe3atZo7d67MZrOio6Mvw6c1xjQ8AAAAAC66dOmimTNnatKkSRo0aJB69+6tOnXq6LvvvtOJEyfUpk0bvffeey7tAgMDtWjRIvXv31/Tp0/XypUr1aFDB8XGxio2NlZBQUFavHixAgICXNrOnj1bu3fvVkxMjFq0aKFevXopKSlJmzZtkt1u1xtvvKFOnTp54uNLIrMEAAAAoBSPPfaYNmzYoAEDBmjXrl1asWKFgoODFR0dra1bt5ZaXa9Hjx7auXOnxo0bp7S0NC1dulRpaWkaN26cdu7cqe7duxu2q1OnjrZt26YpU6YoODhYK1as0K5duzRgwADFxMTo4Ycfvpwf14XJbvdQHq8CyMzMVFhYmG7UUHmbPDPPEdWU2au8R4BqwKtVs/IeAqoBa9zB8h4Cqrgie6G+sX+ujIwMj5WDvhSKzyu73jFN3j7+Hu27qDBP25Y8U+mOWWVEZgkAAAAADHDPEgAAAOCucnwoLS4/MksAAAAAYIBgCQAAAAAMMA0PAAAAcJPJ5lg83Sc8g8wSAAAAABggswQAAAC4iwIPVRqZJQAAAAAwQLAEAAAAAAaYhgcAAAC4yWR3LJ7uE55BZgkAAAAADJBZAgAAANxltzsWT/cJjyCzBAAAAAAGyCwBAAAAbuKepaqNzBIAAAAAGCBYAgAAAAADTMMDgErKGnewvIeAasC7Xt3yHgKqOluBdLK8B3ER7H8snu4THkFmCQAAAAAMkFkCAAAA3ESBh6qNzBIAAAAAGCBYAgAAAAADTMMDAAAA3GW3OxZP9wmPILMEAAAAAAbILAEAAABuosBD1UZmCQAAAAAMkFkCAAAA3MVDaas0MksAAAAAYIBgCQAAAAAMMA0PAAAAcBMFHqo2MksAAAAAYIDMEgAAAOAum92xeLpPeASZJQAAAAAwQLAEAAAAAAaYhgcAAAC4i+csVWlklgAAAADAAJklAAAAwE0mlUPpcM92V62RWQIAAAAAA2SWAAAAAHfZ7Y7F033CI8gsAQAAAIABgiUAAAAAMMA0PAAAAMBNJns5FHhgFp7HkFkCAAAAAANklgAAAAB38VDaKo3MEgAAAAAYIFgCAAAAAANMwwMAAADcZLLbZfLwc4883V91RmYJAAAAAAyQWQIAAADcZftj8XSf8AgySwAAAABggMwSAAAA4CbuWarayCwBAAAAgAGCJQAAAAAwwDQ8AAAAwF32PxZP9wmPILMEAAAAAAbILAEAAADustsdi6f7hEeQWQIAAAAAAwRLAAAAAGCAaXgAAACAm0x2x+LpPuEZZJYAAAAAwACZJQAAAMBdFHio0sgsAQAAAIABMksAAACAm0w2x+LpPuEZZJYAAAAAwADBEgAAAAAYYBoeAAAA4C4KPFRpZJYAAAAAwACZJQAAAMBd9j8WT/cJjyCzBAAAAAAGCJYAAAAAwADT8AAAAAA3mex2mTxccMHT/VVnZJYAAAAAwACZJQAAAMBdlA6v0sgsAQAAAFVYYWGhNm7cqH/84x+65pprFB4eLh8fH9WrV0+33Xab1qxZc872MTExGjRokCIiIhQQEKC2bdvq6aefVnZ29jnbHThwQFFRUYqMjJSfn58iIyMVFRWlQ4cOnbNdVlaWnnrqKbVp00YBAQGKiIjQ4MGD9dVXX13wZ79YBEsAAACAu+ySbB5eLjCxtGnTJvXr10//+c9/dOzYMfXs2VPDhw9X7dq1tWrVKt166626//77ZTfIWL322mu6+eabtW7dOrVv315DhgxRRkaGpk+frq5duyolJcWwz82bN6tz586aN2+ewsPDNWzYMIWHh2vevHnq1KmTfvzxR8N2SUlJ6tq1q2bMmKGsrCwNGTJE7du319q1a9WvXz/NmjXrwj78RSJYAgAAAKows9msESNG6Ntvv9WJEye0evVqLVy4UL/99psWLFggLy8vzZ49W5988olTux07dmjy5Mny8vLSmjVrtGnTJi1atEgHDx5U3759tW/fPk2cONGlP4vFopEjR8pisSg6OlqxsbFasGCBYmNjFR0drZycHI0cOVK5ubkubSdMmKC4uDj17dtXBw4c0KJFi7Rp0yatXr1aZrNZjz76qHbt2nXZjtWfESwBAAAAVVifPn20ZMkS9erVy2XbqFGjFBUVJUn6+OOPnbbNmDFDdrtd9957rwYOHFiyPjAwUHPmzJHZbNbSpUu1d+9ep3Zz585VYmKiWrdurWnTpjltmzZtmlq3bq2EhASX/nbv3q0VK1bIy8tLc+bMUWBgYMm2QYMGKSoqSjabTTNmzHDrOLiDYAkAAABwU3HpcE8vl9JVV10lSUpISChZV1BQUHIv05gxY1zaNGnSRD169JAkLV++3Glb8evRo0fLbHYON8xms0aNGiVJWrZsmWG7Hj16qEmTJi59Fo9j1apVKiwsLOOnuzgESwAAAEA1tn//fklS/fr1S9bFxcXJYrFIkrp27WrYrnj9jh07nNYXv75c7XJyckrGfLkRLAEAAADusutM+XCPLZdu+CdPntTcuXMlSSNGjChZHx8fL0kKDw9XSEiIYdtGjRo57Ss5KtmlpqZKkho3bnzOdsnJycrJyXHps7R2oaGhCg0NdenzcuI5SwAAAEAllJmZ6fTaz89Pfn5+ZW5fVFSksWPHKiMjQx07dtT9999fsi0rK0uSFBQUVGr74OBgl3EUtztX2+J2xW2L9ytrn5mZmS6f/XIhswQAAABUQo0aNVJYWFjJcqGFDyZOnKiNGzeqVq1aWrJkiXx9fS/TSCsvMksAAACAu4qnxnm6TzkKMhRPS5N0QVmlRx55RHPmzFGNGjW0YcMGtW7d2ml78dS7s6fJ/VnxQ2nPHsPZU/ZKa3v2w2yN2l5on5cTmSUAAACgEiq+h6d4KWuwNHnyZL355psKDw/X+vXrS6rhna1p06aSpPT0dKepdWcrrp5XvK/kCHhq1qwpSTp69Og520VERDhNuSt+n9LanT397uw+LycyS9XQTcNOq+uNWWreLlc16xYpOKxI+blmHTvop81rw7TiwwjlWbxK9jeZ7Lriaou63pSlzj2y1LhlvgJDrMrJ8tLB2ACtX1hTXy8Pl2Qqt8+E8hHZPE9demeqVUeLWnWyqHHLPHl5S3P/XV+fvVnfsM3YSYm6e9LJc77vfb3bKeGgv+G2Bk3zdNfDJ3VVryyF1SxSRpq3dnwXok9fr6+TR8t+RQ1Vw+TXjqj/yNPn3OfW5p1UmO98bdDP36bb70tW79tOq2GzAtlsUsIBP8UsqanV8yJks/F7Vh1F1M3VHffEq2uPFEXUyZPF4q0De0K1akETbf2+tsv+a375skzvO/PZDvpqTcOS1/2GHNdjz8ees82zD3bRLz+49okKyCbPnwLZ3G/6xBNP6NVXX1VYWJjWr19fauW5Nm3aKDAwUBaLRdu2bdNNN93kss+2bdskSV26dHFa36VLF8XExGjbtm0aMmTIBbVbtmxZyfbS2gUFBblkwi4XgqVq6NZ7UtSuq0UJ+/104LcAZaV7KTyiSFd0zVGbq3I1YHSaHh/RUmmnfCRJ9ZsU6LWVByRJmWleitsVoOwMb9VvnK8uN2Sryw3ZuvH2dL14XxMVFZKsrE5uHZesYfclu9X24O8BOvh7gOG2nCwvw/XtumZrxvwD8g+06fBef/3+c7CatslV/5Fp6jU4XVPuaqW920u/KRRVV+zPQUo8bDzX3mZ1PosJCS/Sy4sOqEX7POVkmfX71iDZrFLbLhb9fdpxdb85U8/e04zfs2qmVbsM/XPWLwoNL1Rqsp+2bamtkLACdeqapquvS9X82S306fstndrErGpQ6vvVrpenztekyWaTfvulpuE+iQkB2v1rDcNtKcnGF4yAizFlyhS98sorCgsL04YNG3TNNdeUuq+vr68GDx6sxYsXa/78+S7B0pEjR7RlyxZJ0rBhw5y2DRs2TDExMVqwYIGee+45p2ct2Ww2LVy4UJI0fPhwp3a33367nnnmGW3evFlHjx51qYo3f/58SdKQIUPk4+NzgZ/ePRU2WPr000/15ZdfaufOnTpx4oROnz6twMBAtWnTRsOGDdNDDz3kVEkDZTf7hQZKjPdTVrrzP39IjSI9/+FhdeiWownPJepfDzgeBma3Szu+C9aSd2tr+7chTldcO3bP1oufxKv7zZka9WCSPn2tnkc/C8rX4X0BWvxeHR2MDdSB3wI1+qGT6ndHWpnabvkyTP97tfQTjT/z87fp6ffi5R9o04JZdfXRy2eu0t775HGNfuiUnn73kMb3bq+CPE5yq5t1n9XUhkW1yrTvw/86phbt8xS/x19TxzVXcqIjyAqPKNQLH8Xr6t5ZGjvplOa+bJwdRdXj42vVU6/8qtDwQm36sp5ef6GDCvIdF21atcvQC7N+0ZgJB/X7r+H69aeIknavPd+x1Pd8YMpudb4mTb/+XEvJJ40vDO3+tcY53wO4lJ555hm9/PLLJVPvzhUoFZsyZYqWLFmijz76SCNGjNAtt9wiSbJYLBo/frysVqtGjBihtm3bOrWLiorSSy+9pLi4OE2dOlUvvfRSybapU6cqLi5OkZGRGjdunFO79u3ba+jQoVqxYoXGjx+vlStXKiDA8f+ftWvXau7cuTKbzYqOjr7Yw1FmFTZYevfdd7VlyxZdccUV6tKli2rWrKlTp07phx9+0NatW/Xhhx9q06ZNatCg7CdbcNi3w/jKe9Zpb300o55mfn5QV99wZm7qiSN+mjKqhWGb334M1sK36ijqyZPqd8dpgqVqZt1nEU6vbRcxLeB8bh6Zqoh6hUo46Ke5/3b+//3cfzdQj0HpatQiX/1GpOqLT5m6AmM16xaq5+B0SdI7UxuWBEqSlJ7io9efaKT3YvZp2N+StPCtOsrNMc5yomq5/qYk1amXp+xMb709vV1JoCRJ+3eH6bP/ttDEf+zVXX876BQslcbH16reA05IktZ/3vA8e6OyM9ntMnm4wMOF9rdy5cqSgKVly5Z6++23DfeLiIjQf/7zn5LXXbp00cyZMzVp0iQNGjRIvXv3Vp06dfTdd9/pxIkTatOmjd577z2X9wkMDNSiRYvUv39/TZ8+XStXrlSHDh0UGxur2NhYBQUFafHixSWB0Nlmz56t3bt3KyYmRi1atFCvXr2UlJSkTZs2yW6364033lCnTp0u6PNfjAobLM2cOVOtWrUquUGsWGpqqm6//XZ9//33mjx5sj777LNyGmHVZP1jukphQdkn3x6MdXzRIxoUXpYxAZLU45Z0SdKmlTVktzt/P+12k75dVUN/efSkegxMJ1hCqVp3sshslgryTfrtR9fZCfF7ApSe4q3wiCJd2zdTm1YaT5FC1dKqXYYk6cCeUOVku07t+fUnR9ayXed01aiVr9Op574/skffUwoOLVJmuo9++KbupR8wcIHS0s7M+ti2bVup9wQ1adLEKViSpMcee0wdO3bUzJkz9fPPPysnJ0eNGzdWdHS0oqOjS31gbY8ePbRz5069+OKLiomJ0dKlS1W7dm2NGzdOzz77rFq0ML4QX6dOHW3btk0zZszQ0qVLtWLFCgUFBWnAgAF6/PHH1bdvXzePgnsqbLDUrVs3w/W1atXS9OnTdcMNN2j9+vUeHlXVFhBk1djJjhvvf1wfVuZ2DZvlS5LSkirs1wkVUMsOufpr9HGFhBf9USwkUD9uCCv1Sn6LDhZJUtzOQMPtxetbdsi9PANGhdb5+mw1a5ungCCbMk97ad+vgdr6VagKC5ynZAYEOdKfOZleLkF3sYw0x32cLTvmEixVEwGBVklSZobxfW+Z6Y4AymyWWrTN1LbN574gc/NtxyVJX6+tf8573+o3suju/9uv8JoFyrV46cjBYP30bR1lpvOsm0qlHEuHl1VUVJSioqLc7q5fv37q16/fBbdr2bKl5s2bd8HtQkNDNWPGjAt+btTlUCnPbr29HcO+kFrycNWld5Zuuv20zGYpvHahrrjaoqAQm7Z+FaIPXirbXH2/AJuGjk+RJG1eE34ZR4uq5rr+Gbquf4bTuuwML737bKRiljrfexIQZFVYTcfJTPJx45OIM/edFMkvwKr8XKZPVSc33+laES/1pLdendxY27458yyO9FTH348atYvkH2h1qvwpOap/1o10ZMnrNc6/jCNGRZKe5vj9qNfQYri9XsMzF2HqNjj3BZk69XPVqavjKv76zyPPuW/7K9PV/sp0p3X5eXs0f3YLLZnX/HzDBuABlS5YysrK0vPPPy9Juu2228p3MJVck1Z56j/K+QTjq2Xhev/5BrKUUo3szx6cfkz1mxQo5YS3PptV53IME1XMiSN++nBGA239OlRJfwQ+jVvladTfT6r7zZn6xxtHZLWZ9PXyM1NwA4KtJf+dl2t8lTbXcmZ9UIiNYKmaOLQ7QO9MDdSv3wcr6bivfP1tatEuT2Mnn1T7a3L0/EfxempMc+36wTFNZO/2QOVZzPIPtGngmFQt/8D5d6vfnWnyD3RknwKDL+NNeKhQdm6rqdH3HVLLKzL1/+3de3RU9bn/8c/kHnKFEAiQkJCEGMAcRVBAEAUKIihUUDnSQEJLBf3p4SZqrQpYKdIKv6NVVysiUJUqClblGoLSgBeEElCIXE1IuJNALkCuzD5/xAxOsxNgwJlk8n6tlbVg79mznz1rE/Yzz/f7fGOvK9YPe+0XuxxyX57tz80Cq+p9r4HDjsjDQ9q3O1g5B8yHJ53J99F7b8ZqS0a4jh9ppsoKD7WLPqd7/jtX/Ycc1bj/2S8PD2nZIhKmRqERVJbguAafLKWlpWnp0qWyWq22Bg8lJSUaPHiw5s6d6+rwGrWP3gzXR2+Gy9PLUKt2Fep1Z7FGTzqh7nfs1azfxGjXlvq7DY6efEKDRp1RealFsyfGqORMg7+d0ABsWF67Y1nWtkDNGBevh5/P0y9/fUoTZxzWppWhtG7GJX20wD7ZKT3nqe2bvLV9U6BmLMzWrYOLNXHWET0yKNG2f/nfwvWrKSc07qljslot2rQyVBcuSL0GFWvCzCOqrLDI28eQQa7UZHy7NUzf/bu5krqd0XPzM/X6i520K7O5gkMqNfT+PPUfelSVlRZ5e9d/X1gshn5xT/UQvPWf1N3Y4d9fhddaQ+nA9yH6/zOSlL0vSL+dulcP/vag0j5up8LTjKIBXKnBP4lkZWVpyZIlevvtt5WWlqaSkhKNHj1aixcvVkhI/fNqysvLbSv9/nTFX9i7UGXRsUO+WvFGuH6f3EGBoRf05Ku58vGr+3+EEQ+dUsoTx1VRZtHzv4lR1lbWtsHVe2d+G12oqh5Kl9j14nCY0rMXq0R+/ub3pX+zi9vPlTT4X2342Vn09rzq4cRxXcoU3rbCtued+RFa+fcw+fobeuQPR/SPzN1a9u1uTXkpTwe+q15oW1Kt5RXg3uY8eYN27whVeESZZvxvpj7412da+MkmjRiTo0/+Ea3sfdVVopLiutd2ubFHgVq1KVNZmYc2rnWs9fwn/4hW0Rlv+fha1bVngUPvAeDaafBPFJMnT5ZhGKqoqNCBAwc0b948rVmzRp07d1ZGRka9x86ZM0chISG2n6ioKCdF3XjtzQxQ7j4/tWpXqYT/Mh+7PezXpzRh5lFVlFv0h9/G2M0HAK5GSaGXCvOrH0Ratrn4cFt6zlPFZ6oTpvB2FabH1jwMFxZ4MQQPkqTc/Re/kW/Z5mK3TqvVor/8LkoTBlynxXMjtOqdMC3/W7ieS+2gJ+6PV7Mfh31m72FR0Kak6IyvnvjNLfr9I920bFEHrV0RqX+8GatJyT21YH6iwlpVz2HL2W8+tE662Njhyw2tdd6kq97lsFotOppb/QVky9ZlDr0HnKxmGJ6zf+AUjeZrM29vb8XFxWnq1Knq3bu3evXqpeTkZO3du9e0R7sk/e53v9PUqVNtfy8uLiZhugxlP879CG1Ze1z2Pan5+n8vVCdKL/w2Rt9sIFHCtePhYSgguPpB9afVJEk68F0z3dS3RAk3nNeW9NBaxybccP7H15n/PkDTE9z84ly382drfzeYs8dfOXv+834x1Pnmc5Kk7Rl1PxTDXVm0Y0vLWmspRUSeV1h4uYoKvXVwj/n/e4HBFep1x0lJUtrH9Td2uJSgkOovf1jnC3C9Bl9ZMtOjRw917txZeXl5dfaJl6q75QUHB9v9oH7BLaoU27m608/hH+zHSQ8dk69H/3jElihtSefzxLXVc1CR/JpZZbVK+761bxH+xdpQSdLtw87IYrH/Rs1iMdT3nupmJV+sCXVGqGgEbh9efU+cK/bQ4YOXVyW6fVihWkdWKmtbMx34zrxNPZqeEWNyJElrV0Sqqsr80anfXcfk42vV0Tx/ffdvx1vOxyUWKzLmx6USdl/+Mh5wIauLfuAUjTJZkqSAgOoS9cmTJ10cSePSvmOZ+t17Rt6+tf+VtYst1zN/y5GPn6Gsbc3svnG9a3SBHp1DooSrE962Qv1HFJjef73uLNSUPx+SJH32UQudOWU/hGX9sjDlH/dWVFy5UqYftduXMv2oouLKdeqod62243BfsV3Oq+fAInl41k6e7/zvAo178pgk6Z9vhetC1cU1lVq0rrSbw1Sjxy+KNGlunirKLHrlKUYhNDVRHc7KP8B+RIWHp1UPjPtBd43I05HcZnp/Yd3d6WqG4K3/OFJS3Qu7+/pd0ND7c+XfrPbojS5dT+vpP+2QJO3KDNW+3aFXfB0Arq1GMwzvp/Lz87Vz505JUkJCgoujaVxCW1bpqddyVXrOQwd3+evUMW95exsKb1eh+KRSeXpKh/b56o8PR9uOie1Sqv/502F5eEjHc33UZ2ih+gwtNH3/eVPaO+lK0BDEX39ej/4x1/b3NtHVD6BDk/PV4xcX11B6fnycTp/0VlBolZ585ZAe+2OeDu5qpvzj3vL1s6p9QpkiY6vnA+z4IlB/+V3tB9XyMg/NnthBc5Ye0IP/c0I9BxUpZ4+/YhJL1SGxTKXnPPTCxFhVlDXa74BwhVpHVmjmWzkqOeOpA7v8deaUtwJCLijmulLbWkmffxSqd+ZH2B133Y3n9NybOfohy1/Hc310ocqiDp1K1b5juc6f9dCs33RQ9vcM52xqBo/I010jDuvAnmAVnPSTt49VidcXqnnLCh3JbaZnHumu8jLzx6bY64oVl1iiC1UWpX/att7zeHlZ9chT32v8lL06uDdIp477y9PTqnbR5xUTf1aSlL0/UC8+deO1vkQADmiQyVJWVpYyMzM1cuRI+fnZD53Yt2+fJkyYoPLycvXs2VNJSUkuirJxOrTXT4vmROj6HucUFV+uuOtL5ellqKTQUzs2B+qL1SFKe7+F3ar3gcEX5PHjX9t3LFf7jnUv1Eiy1LQ0C7qgTjfVbgQS3rZS4W0vTqj39qmuJJ066qP3X2uthBvOq21MueKTzsvL21DxaS99vT5Yn/+zhf71SXMZhvm3slnbAvXwoESNnnRcXW8rUZ8hhSo67aX1H7TQu//bRscO0WK3Kfkhy18rFoQr4b/OKzK+XJ27n5PFIp3J91LGyhClvR+mrZ/VroIf2uuv9A+bq1O387qpb4k8PA2dOuKj5W+Ea/nfwlVw3HzhY7i3bV+Eq3XbMsUlFqtjp2JVVnrocE4zrXgnRiuXtVdFed3zhwb9WFXa/nWYTufXP+SzvMxT/1gQq/hOxYqKOafo2LPy8bPqbLG3Mr8O0+b01kr/tF2dw/3Q8FgMQxYnN1xw9vmaMothNLxPe+PGjerXr58CAgLUtWtXRUZGqqKiQrm5udq+fbusVqs6deqktWvXqn37y384Ly4uVkhIiO7QcHlZHOtSA1wWDyblwglYCAhO4BXR2tUhwM1VWSuUfvwNFRUVNar55TXPlb9ImCovT+d+WVd1oVzp++Y3us+sMWqQlaUuXbpo9uzZ2rRpk/bs2aPMzExVVlaqRYsWGjBggEaMGKFx48bJ15dvkQEAAOBCrmjl3fBqHW6rQSZL4eHhevrpp10dBgAAAIAmrEEmSwAAAECjYDUki5MrPVYqS87C7EEAAAAAMEGyBAAAAAAmGIYHAAAAOIoGD26NyhIAAAAAmKCyBAAAADjMBZUlUVlyFipLAAAAAGCCZAkAAAAATDAMDwAAAHAUDR7cGpUlAAAAADBBZQkAAABwlNWQ0xsuWKksOQuVJQAAAAAwQWUJAAAAcJRhrf5x9jnhFFSWAAAAAMAEyRIAAAAAmGAYHgAAAOAoWoe7NSpLAAAAAGCCyhIAAADgKFqHuzUqSwAAAABggmQJAAAAAEwwDA8AAABwFA0e3BqVJQAAAAAwQWUJAAAAcJQhF1SWnHu6pozKEgAAAACYoLIEAAAAOIo5S26NyhIAAAAAmCBZAgAAAAATDMMDAAAAHGW1SrK64JxwBipLAAAAAGCCyhIAAADgKBo8uDUqSwAAAABggmQJAAAAAEwwDA8AAABwFMPw3BqVJQAAAAAwQWUJAAAAcJTVkOTkSo+VypKzUFkCAAAAABNUlgAAAAAHGYZVhuHcRWKdfb6mjMoSAAAAAJggWQIAAAAAEwzDAwAAABxlGM5vuEDrcKehsgQAAAAAJqgsAQAAAI4yXNA6nMqS01BZAgAAAAATJEsAAAAAYIJheAAAAICjrFbJ4uR1j1hnyWmoLAEAAACACSpLAAAAgKNo8ODWqCwBAAAAgAkqSwAAAICDDKtVhpPnLBnMWXIaKksAAAAAYIJkCQAAAABMMAwPAAAAcBQNHtwalSUAAAAAMEFlCQAAAHCU1ZAsVJbcFZUlAAAAADBBsgQAAAAAJhiGBwAAADjKMCQ5ed0jhuE5DZUlAAAAADBBZQkAAABwkGE1ZDi5wYNBZclpqCwBAAAAgAmSJQAAAAAwwTA8AAAAwFGGVc5v8ODk8zVhVJYAAAAAwASVJQAAAMBBNHhwb1SWAAAAAMAElSUAAADAUcxZcmtUlgAAAADARJOqLNWM76xSpcRQT/yc+MYHzsB9BmewVrg6Ari5qh/vscY6D8cVz5VVqnTuCZuwJpUslZSUSJI2a7WLI4Hb4xkWgLs47uoA0FSUlJQoJCTE1WFcNh8fH0VERGjzcdc8V0ZERMjHx8cl525KLEZjTeMdYLVadfToUQUFBclisbg6nEahuLhYUVFRysvLU3BwsKvDgZviPoMzcJ/BGbjPrpxhGCopKVHbtm3l4dG4ZoiUlZWposI11VcfHx/5+fm55NxNSZOqLHl4eCgyMtLVYTRKwcHB/NLHz477DM7AfQZn4D67Mo2povRTfn5+JCxurnGl7wAAAADgJCRLAAAAAGCCZAn18vX11YwZM+Tr6+vqUODGuM/gDNxncAbuM8C9NKkGDwAAAABwuagsAQAAAIAJkiUAAAAAMEGyBAAAAAAmSJZg6oMPPtAdd9yh5s2bKyAgQDfccIP+9Kc/qbKy0tWhwQ3s3btXf/nLX5SamqqkpCR5eXnJYrHohRdecHVocBOVlZXasGGDpk+frptvvlmhoaHy9vZWRESEhg0bplWrVrk6RLiJd999V2PHjtUNN9ygVq1aydvbWyEhIbrllls0Z84cnT171tUhArgKNHhALZMnT9bLL78sLy8v9e/fX4GBgfrss89UWFioPn36KC0tTf7+/q4OE41YzT32n/7whz/omWeecUFEcDfp6ekaOHCgJCkiIkLdunVTQECAsrKytGvXLknSQw89pL/+9a+yWCyuDBWNXJ8+ffTll1+qU6dOioqKUosWLXTixAl99dVXKi0tVXx8vP71r3+pbdu2rg4VgAOoLMHOP//5T7388ssKDAzUli1btG7dOi1fvlz79+9XUlKSNm/erGeffdbVYaKRu/766/X444/r3Xff1ffff68xY8a4OiS4GQ8PD40cOVIZGRk6duyYVq5cqffff1/fffed3nvvPXl6euqNN97Q22+/7epQ0cjNmzdP+fn52r17t9auXaulS5dqw4YNysvLU58+fXTgwAFNmzbN1WECcBCVJdi55ZZbtHXrVr3wwgv6/e9/b7dv8+bNuu222+Tr66sTJ04oJCTERVHC3aSmpmrJkiVUluA048eP18KFCzVgwAClp6e7Ohy4qU2bNqlv375q0aKFCgoKXB0OAAdQWYLNkSNHtHXrVknS6NGja+3v06ePoqKiVF5ertWrVzs7PAC4Zrp27SpJysvLc3EkcGdeXl6SxAK1QCNGsgSbzMxMSVKLFi3UoUMH09d0797d7rUA0Bjt379fktSmTRsXRwJ3VVJSopkzZ0qShg0b5tpgADjMy9UBoOHIzs6WJLVv377O10RFRdm9FgAam+PHj2vx4sWSpJEjR7o2GLiNtLQ0LV26VFar1dbgoaSkRIMHD9bcuXNdHR4AB5EswaakpESSFBAQUOdrAgMDJUnFxcVOiQkArqWqqiolJyerqKhISUlJmjBhgqtDgpvIysrSkiVL7LaNHj1a8+fPZ44v0IgxDA8A0GRMnDhRGzZsUFhYmD788EP5+Pi4OiS4icmTJ8swDFVUVOjAgQOaN2+e1qxZo86dOysjI8PV4QFwEMkSbIKCgiRJ586dq/M1NYvrBQcHOyUmALhWJk2apIULF6p58+Zav369EhISXB0S3JC3t7fi4uI0depUrVmzRmfOnFFycrJKS0tdHRoAB5AswSYmJkZS/d2havbVvBYAGoNp06bplVdeUWhoqNLS0mzd8ICfU48ePdS5c2fl5eVp27Ztrg4HgANIlmBT8/BQUFBQZwOHml/2N910k9PiAoCr8cQTT9jmjaSlpdm6egLOUDMP+OTJky6OBIAjSJZgExkZqZtvvlmStHTp0lr7N2/erLy8PPn6+mrIkCHODg8ArthTTz2lP//5zwoJCdH69ettv+MAZ8jPz9fOnTsliWGfQCNFsgQ7Tz/9tCTpxRdf1Pbt223bCwoK9Mgjj0iSHn30UTr7AGjwnnnmGc2dO1ehoaEkSvhZZGVl6d1331VZWVmtffv27dP999+v8vJy9ezZU0lJSS6IEMDVshiGYbg6CDQskyZN0iuvvCJvb28NGDBAAQEB2rBhgwoLC9W7d2+tX79e/v7+rg4Tjdj27dttybckHTx4UPn5+YqMjFS7du1s2z/66CMWDYVDPvnkEw0fPlxS9WLaXbp0MX1dy5Yt9dJLLzkzNLiRjRs3ql+/fgoICFDXrl0VGRmpiooK5ebmavv27bJarerUqZPWrl1b7xqGABoukiWYWrZsmV577TXt2LFDlZWViouLU3JysqZMmUKrXVy1mgeMS8nOzqaZCByyePFijRs37pKvi46OVk5Ozs8fENzSqVOntGDBAm3atEl79uzRqVOnVFlZqRYtWigpKUkjRozQuHHj5Ovr6+pQATiIZAkAAAAATDBnCQAAAABMkCwBAAAAgAmSJQAAAAAwQbIEAAAAACZIlgAAAADABMkSAAAAAJggWQIAAAAAEyRLAAAAAGCCZAmAS8TExMhisdj9+Pr6qn379ho1apQ2bdrk6hBtZs6cKYvFopkzZ9ptX7x4sSwWi1JTU10S17VQ17XVZ+PGjbJYLLrjjjtcFsPVSE1NlcVi0eLFi51yPgBA40WyBMClevfurZSUFKWkpOiuu+6S1WrVsmXLdPvtt2v+/PmuDs9papLHnJwcV4cCAAB+5OXqAAA0bePHj7erzJSVlWnChAn6+9//rieeeEJ33323EhISXBdgPe6991717NlTISEhrg4FAAD8DKgsAWhQ/Pz89NprrykgIEAXLlzQihUrXB1SnUJCQpSYmKg2bdq4OhQAAPAzIFkC0OAEBgbquuuukyS7YWk1c5skadGiRerVq5dCQkJqDV87evSopk6dqk6dOqlZs2YKCgrSzTffrFdffVVVVVWm5ywtLdXMmTPVsWNH+fr6qk2bNkpJSVFubm6dcV5qztKRI0c0ffp0JSUlKSgoSAEBAUpISFBqaqq+/PJLu/c4dOiQJKlDhw5287g2btxo957OujZHpaen67HHHtONN96oli1bytfXV5GRkRo1apS2bt16yeMPHTqksWPHqk2bNvLz81NCQoJmzpyp0tLSOo/Zt2+fJkyYoLi4OPn5+SkkJER9+/bVO++8cy0vDQDQBDEMD0CDVFxcLEny9fWtte+xxx7T66+/rltvvVVDhw7VDz/8YEuiMjIy9Mtf/lJnzpxRTEyMBg4cqPLycn3zzTd67LHH9Omnn2rlypXy9va2vd/58+c1YMAAff311woICNCgQYPk7++vdevWadWqVRo6dOgVx79hwwbdd999KiwsVKtWrTRgwAD5+PgoJydHS5culSTdeuutio+PV0pKij788EOdO3dOI0eOVGBgoO19IiIibH9uKNdWn4kTJyovL09dunRR79695eXlpT179mjZsmVasWKF3nvvPY0cOdL02OzsbHXr1k1eXl7q27evSktL9fnnn2vWrFlKT09Xenq6/Pz87I754IMPNHbsWJWVlSkxMVFDhgxRUVGRtmzZojFjxuizzz7TW2+9dU2vEQDQhBgA4ALR0dGGJGPRokW19u3cudPw8PAwJBlvvfWWbbskQ5IRHBxsfPXVV7WOO3bsmBEWFmZYLBbj9ddfNy5cuGDbl5+fb/Tv39+QZMyaNcvuuMcff9yQZCQmJhpHjhyxbT937pwxfPhw23lnzJhhd9yiRYsMSUZKSord9tzcXCMkJMSQZDz11FNGeXm53f4TJ04YmzZtMv08srOzzT4up19bfT7//HNDknH77bfX2vfRRx8Zp0+fNt3u5eVlhIWFGefPn7fbN2PGDFscw4cPt9ufl5dnJCQk2D7Ln/r2228NX19fw8/Pz1i+fLndvpycHCMpKcmQZCxZssRuX0pKSp33HgAAP0WyBMAlzJKlwsJCY9WqVUZcXJwhyWjbtq1x9uxZ2/6aB+rnn3/e9D2ffPJJQ5Lx6KOPmu4/fPiw4e3tbYSHhxtWq9UwDMM4f/68ERQUZEgy1qxZU+uYY8eOGX5+fleULE2ePNmQZNxzzz2X8UlUu1Sy5Oxrq099yVJ9HnzwQUOSsWrVKrvtNcmSv7+/cezYsVrHffrpp7YkubS01LZ91KhRhiTjpZdeMj3fN998Y0gyunXrZredZAkAcLmYswTApcaNG2ebnxMaGqqhQ4fq4MGDiouL0+rVqxUQEFDrmPvuu8/0vVatWiVJGjVqlOn+du3aqWPHjjp16pT2798vSdq+fbtKSkrUsmVLDR48uNYxERERGjRo0BVd09q1ayVJDz300BUdV5+Gcm2X4+jRo1qwYIGmTZtm63aYmpqq3bt3S5L27t1retygQYPshh3WuPvuuxUWFqbi4mJt375dkmS1WrVmzRpJdX8m3bt3V2BgoDIzM1VWVnYtLg0A0MQwZwmAS/Xu3Vvx8fGSJB8fH7Vq1Uo9e/bU4MGD5eVl/isqJibGdPsPP/wgSbrtttsued5Tp04pISFBhw8frvc9peqmC1eipllDYmLiFR1Xn4ZybZcya9YszZ49W5WVlXW+pmY+2pXEEhMTo4KCAts1FRQU2N4nKirqknEVFBSoXbt2l3wdAAA/RbIEwKX+c52ly+Hv72+63Wq1SqquPJlVpH4qLCzsis7pao3h2lasWKGZM2cqMDBQr776qvr376+2bdvK399fFotFTz/9tObMmSPDMBw+R82xNZ+HJKWkpFzyOLNGIQAAXArJEgC3ERUVpf379+vJJ59U9+7dL+uYmmrDT1uP/6f69plp37699u7dqz179tiqZleroVxbfZYtWyZJmj17tukQxJrhgXXJzs6uc19NnJGRkZKkli1byt/fX6WlpXrppZfUsmVLB6MGAKBuzFkC4DbuuusuSRcf2i9Ht27dFBgYqPz8fKWlpdXaf+LECdPt9amZH7RgwYLLPsbHx0eS6lwrqaFcW31Onz4tSYqOjq617+TJk1q/fn29x6elpenkyZO1tq9evVoFBQUKCgpSt27dJEmenp4aOHCgpCv7TAAAuBIkSwDcxvTp0xUaGqr58+dr3rx5qqioqPWa7Oxsu8VK/f39bVWQKVOm6NixY7Z9paWlevjhh+tdENXM1KlTFRQUpE8++UTPPPNMrfk7J0+e1ObNm+221VRMapogNNRrq0+nTp0kSW+88YZdfEVFRUpJSVFRUVG9x5vFdPToUU2bNk1S9RpOP11nacaMGfLx8dH06dO1ZMkSu6F5NXbt2qUVK1Zc1XUBAJoukiUAbiMyMlIff/yxmjdvrscff1xRUVEaMGCAkpOTdc899yg+Pl6xsbF69dVX7Y57/vnndcsttygrK0sJCQkaNmyYHnjgAcXGxiojI0Njx469ojjat2+vDz/8UEFBQZo9e7aioqJ077336oEHHlCPHj0UGRmpN9980+6YmoVak5OTNXLkSI0fP17jx4+3dY5rKNdWn8mTJys0NFSrV69WbGys7rvvPg0fPlzR0dHauXOnfv3rX9d7/NixY7Vx40bFxsbqgQce0LBhw5SQkKA9e/aoV69emjVrlt3rb7rpJltymJqaqujoaN15551KTk7WkCFDFBUVpaSkJCpPAACHkSwBcCt9+/bV7t279eyzzyoyMlJbt27VBx98oB07dqh169aaMWNGreFxAQEB+vzzz/Xss8+qdevWWrdunTIyMjRgwABt27bNoY5xgwYN0q5duzRp0iSFhoZq7dq1WrNmjQoLCzVmzBhNnDjR7vUPP/yw5syZo+joaK1evVoLFy7UwoUL7apBDeXa6tKhQwdlZmbqV7/6lTw9PbVy5Urt3LlTDz74oDIzMy/Zta5Dhw7atm2b+vXrp4yMDK1bt05t2rTRc889p/T0dNPGHvfff792796tKVOmKDQ0VF988YWWL1+urKwsxcfH68UXX9Ts2bOv2TUCAJoWi3E1bYkAAAAAwE1RWQIAAAAAEyRLAAAAAGCCZAkAAAAATJAsAQAAAIAJkiUAAAAAMEGyBAAAAAAmSJYAAAAAwATJEgAAAACYIFkCAAAAABMkSwAAAABggmQJAAAAAEyQLAEAAACACZIlAAAAADDxf8yQoq+hqr9kAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1200x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "correctly_classified, average_class_accuracy, pr_auc, labels, predictions, misclassified_samples = evaluation_of_net(model, test_loader, class_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9665b246-6542-4b50-bb79-8748ca2f63de",
   "metadata": {},
   "source": [
    "## 2. Multiple people Localization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "06945cce-57e3-47db-b844-ff2df175611c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_test = \"D:\\IR_blobs\\IR_blobs_github\\data_IR_final\\localization\\localization_test_12_06_final.csv\"\n",
    "path_train = \"D:\\IR_blobs\\IR_blobs_github\\data_IR_final\\localization\\localization_train_12_06_final.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ae6697f6-a574-4cfc-9c68-2fdd5a49f9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(path_test)\n",
    "df_train = pd.read_csv(path_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6a7ffcf6-2f20-472d-b524-6fff220608d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "838bb1bf-d68f-4f04-9a3b-b1ff716670c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pixel 1</th>\n",
       "      <th>Pixel 2</th>\n",
       "      <th>Pixel 3</th>\n",
       "      <th>Pixel 4</th>\n",
       "      <th>Pixel 5</th>\n",
       "      <th>Pixel 6</th>\n",
       "      <th>Pixel 7</th>\n",
       "      <th>Pixel 8</th>\n",
       "      <th>Pixel 9</th>\n",
       "      <th>Pixel 10</th>\n",
       "      <th>Pixel 11</th>\n",
       "      <th>Pixel 12</th>\n",
       "      <th>Pixel 13</th>\n",
       "      <th>Pixel 14</th>\n",
       "      <th>Pixel 15</th>\n",
       "      <th>Pixel 16</th>\n",
       "      <th>Pixel 17</th>\n",
       "      <th>Pixel 18</th>\n",
       "      <th>Pixel 19</th>\n",
       "      <th>Pixel 20</th>\n",
       "      <th>Pixel 21</th>\n",
       "      <th>Pixel 22</th>\n",
       "      <th>Pixel 23</th>\n",
       "      <th>Pixel 24</th>\n",
       "      <th>Pixel 25</th>\n",
       "      <th>Pixel 26</th>\n",
       "      <th>Pixel 27</th>\n",
       "      <th>Pixel 28</th>\n",
       "      <th>Pixel 29</th>\n",
       "      <th>Pixel 30</th>\n",
       "      <th>Pixel 31</th>\n",
       "      <th>Pixel 32</th>\n",
       "      <th>Pixel 33</th>\n",
       "      <th>Pixel 34</th>\n",
       "      <th>Pixel 35</th>\n",
       "      <th>Pixel 36</th>\n",
       "      <th>Pixel 37</th>\n",
       "      <th>Pixel 38</th>\n",
       "      <th>Pixel 39</th>\n",
       "      <th>Pixel 40</th>\n",
       "      <th>Pixel 41</th>\n",
       "      <th>Pixel 42</th>\n",
       "      <th>Pixel 43</th>\n",
       "      <th>Pixel 44</th>\n",
       "      <th>Pixel 45</th>\n",
       "      <th>Pixel 46</th>\n",
       "      <th>Pixel 47</th>\n",
       "      <th>Pixel 48</th>\n",
       "      <th>Pixel 49</th>\n",
       "      <th>Pixel 50</th>\n",
       "      <th>Pixel 51</th>\n",
       "      <th>Pixel 52</th>\n",
       "      <th>Pixel 53</th>\n",
       "      <th>Pixel 54</th>\n",
       "      <th>Pixel 55</th>\n",
       "      <th>Pixel 56</th>\n",
       "      <th>Pixel 57</th>\n",
       "      <th>Pixel 58</th>\n",
       "      <th>Pixel 59</th>\n",
       "      <th>Pixel 60</th>\n",
       "      <th>Pixel 61</th>\n",
       "      <th>Pixel 62</th>\n",
       "      <th>Pixel 63</th>\n",
       "      <th>Pixel 64</th>\n",
       "      <th>session</th>\n",
       "      <th>target_coordinates</th>\n",
       "      <th>people_#</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42710</th>\n",
       "      <td>20.78</td>\n",
       "      <td>18.93</td>\n",
       "      <td>17.46</td>\n",
       "      <td>17.17</td>\n",
       "      <td>17.64</td>\n",
       "      <td>18.14</td>\n",
       "      <td>17.57</td>\n",
       "      <td>18.54</td>\n",
       "      <td>20.10</td>\n",
       "      <td>19.27</td>\n",
       "      <td>17.82</td>\n",
       "      <td>17.34</td>\n",
       "      <td>17.68</td>\n",
       "      <td>17.84</td>\n",
       "      <td>19.82</td>\n",
       "      <td>21.44</td>\n",
       "      <td>17.43</td>\n",
       "      <td>17.38</td>\n",
       "      <td>17.40</td>\n",
       "      <td>17.45</td>\n",
       "      <td>17.52</td>\n",
       "      <td>18.22</td>\n",
       "      <td>19.74</td>\n",
       "      <td>20.23</td>\n",
       "      <td>16.86</td>\n",
       "      <td>16.93</td>\n",
       "      <td>17.34</td>\n",
       "      <td>17.59</td>\n",
       "      <td>17.50</td>\n",
       "      <td>17.55</td>\n",
       "      <td>17.67</td>\n",
       "      <td>17.94</td>\n",
       "      <td>16.73</td>\n",
       "      <td>17.04</td>\n",
       "      <td>17.24</td>\n",
       "      <td>17.10</td>\n",
       "      <td>17.21</td>\n",
       "      <td>17.50</td>\n",
       "      <td>17.13</td>\n",
       "      <td>17.45</td>\n",
       "      <td>17.04</td>\n",
       "      <td>17.83</td>\n",
       "      <td>17.33</td>\n",
       "      <td>16.96</td>\n",
       "      <td>16.98</td>\n",
       "      <td>17.67</td>\n",
       "      <td>16.99</td>\n",
       "      <td>16.91</td>\n",
       "      <td>19.21</td>\n",
       "      <td>18.93</td>\n",
       "      <td>17.40</td>\n",
       "      <td>16.43</td>\n",
       "      <td>17.03</td>\n",
       "      <td>16.87</td>\n",
       "      <td>16.43</td>\n",
       "      <td>16.32</td>\n",
       "      <td>19.66</td>\n",
       "      <td>17.30</td>\n",
       "      <td>16.33</td>\n",
       "      <td>16.46</td>\n",
       "      <td>16.27</td>\n",
       "      <td>16.57</td>\n",
       "      <td>16.72</td>\n",
       "      <td>15.40</td>\n",
       "      <td>268</td>\n",
       "      <td>[(0, 0), (7, 1), (0, 7)]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42711</th>\n",
       "      <td>20.64</td>\n",
       "      <td>18.66</td>\n",
       "      <td>17.45</td>\n",
       "      <td>17.32</td>\n",
       "      <td>17.77</td>\n",
       "      <td>18.04</td>\n",
       "      <td>18.63</td>\n",
       "      <td>19.92</td>\n",
       "      <td>20.15</td>\n",
       "      <td>19.11</td>\n",
       "      <td>17.77</td>\n",
       "      <td>17.49</td>\n",
       "      <td>17.42</td>\n",
       "      <td>17.65</td>\n",
       "      <td>19.83</td>\n",
       "      <td>20.97</td>\n",
       "      <td>17.17</td>\n",
       "      <td>17.43</td>\n",
       "      <td>17.19</td>\n",
       "      <td>16.97</td>\n",
       "      <td>17.48</td>\n",
       "      <td>17.95</td>\n",
       "      <td>18.80</td>\n",
       "      <td>18.30</td>\n",
       "      <td>16.90</td>\n",
       "      <td>16.91</td>\n",
       "      <td>17.17</td>\n",
       "      <td>17.42</td>\n",
       "      <td>17.27</td>\n",
       "      <td>17.52</td>\n",
       "      <td>17.51</td>\n",
       "      <td>17.70</td>\n",
       "      <td>16.83</td>\n",
       "      <td>16.94</td>\n",
       "      <td>17.15</td>\n",
       "      <td>16.98</td>\n",
       "      <td>17.16</td>\n",
       "      <td>17.20</td>\n",
       "      <td>16.88</td>\n",
       "      <td>17.53</td>\n",
       "      <td>17.15</td>\n",
       "      <td>17.89</td>\n",
       "      <td>17.23</td>\n",
       "      <td>17.00</td>\n",
       "      <td>17.16</td>\n",
       "      <td>17.82</td>\n",
       "      <td>17.16</td>\n",
       "      <td>16.77</td>\n",
       "      <td>18.98</td>\n",
       "      <td>18.77</td>\n",
       "      <td>17.37</td>\n",
       "      <td>16.31</td>\n",
       "      <td>17.24</td>\n",
       "      <td>17.01</td>\n",
       "      <td>16.59</td>\n",
       "      <td>16.09</td>\n",
       "      <td>19.29</td>\n",
       "      <td>17.49</td>\n",
       "      <td>16.10</td>\n",
       "      <td>16.22</td>\n",
       "      <td>16.09</td>\n",
       "      <td>16.44</td>\n",
       "      <td>16.17</td>\n",
       "      <td>15.99</td>\n",
       "      <td>268</td>\n",
       "      <td>[(0, 0), (7, 1), (0, 7)]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42712</th>\n",
       "      <td>20.97</td>\n",
       "      <td>18.72</td>\n",
       "      <td>17.37</td>\n",
       "      <td>17.44</td>\n",
       "      <td>17.66</td>\n",
       "      <td>18.24</td>\n",
       "      <td>19.46</td>\n",
       "      <td>20.88</td>\n",
       "      <td>20.13</td>\n",
       "      <td>19.00</td>\n",
       "      <td>17.59</td>\n",
       "      <td>17.46</td>\n",
       "      <td>17.61</td>\n",
       "      <td>17.92</td>\n",
       "      <td>19.90</td>\n",
       "      <td>20.69</td>\n",
       "      <td>17.02</td>\n",
       "      <td>16.96</td>\n",
       "      <td>17.30</td>\n",
       "      <td>17.16</td>\n",
       "      <td>17.51</td>\n",
       "      <td>18.06</td>\n",
       "      <td>17.55</td>\n",
       "      <td>17.57</td>\n",
       "      <td>16.77</td>\n",
       "      <td>16.90</td>\n",
       "      <td>17.17</td>\n",
       "      <td>17.19</td>\n",
       "      <td>17.23</td>\n",
       "      <td>17.40</td>\n",
       "      <td>17.68</td>\n",
       "      <td>17.42</td>\n",
       "      <td>16.80</td>\n",
       "      <td>16.92</td>\n",
       "      <td>17.17</td>\n",
       "      <td>16.97</td>\n",
       "      <td>17.25</td>\n",
       "      <td>17.41</td>\n",
       "      <td>16.91</td>\n",
       "      <td>17.27</td>\n",
       "      <td>16.81</td>\n",
       "      <td>17.96</td>\n",
       "      <td>17.19</td>\n",
       "      <td>16.93</td>\n",
       "      <td>17.14</td>\n",
       "      <td>17.61</td>\n",
       "      <td>16.94</td>\n",
       "      <td>16.59</td>\n",
       "      <td>18.97</td>\n",
       "      <td>18.39</td>\n",
       "      <td>17.17</td>\n",
       "      <td>16.29</td>\n",
       "      <td>16.89</td>\n",
       "      <td>16.99</td>\n",
       "      <td>16.34</td>\n",
       "      <td>15.70</td>\n",
       "      <td>19.20</td>\n",
       "      <td>17.39</td>\n",
       "      <td>16.16</td>\n",
       "      <td>16.21</td>\n",
       "      <td>15.89</td>\n",
       "      <td>16.57</td>\n",
       "      <td>16.23</td>\n",
       "      <td>15.68</td>\n",
       "      <td>268</td>\n",
       "      <td>[(0, 0), (7, 0), (0, 7)]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42713</th>\n",
       "      <td>20.86</td>\n",
       "      <td>18.82</td>\n",
       "      <td>17.34</td>\n",
       "      <td>17.39</td>\n",
       "      <td>17.59</td>\n",
       "      <td>18.15</td>\n",
       "      <td>19.67</td>\n",
       "      <td>20.80</td>\n",
       "      <td>20.04</td>\n",
       "      <td>19.04</td>\n",
       "      <td>17.68</td>\n",
       "      <td>17.52</td>\n",
       "      <td>17.68</td>\n",
       "      <td>18.32</td>\n",
       "      <td>20.09</td>\n",
       "      <td>20.86</td>\n",
       "      <td>17.20</td>\n",
       "      <td>17.29</td>\n",
       "      <td>17.66</td>\n",
       "      <td>17.52</td>\n",
       "      <td>17.71</td>\n",
       "      <td>18.00</td>\n",
       "      <td>17.48</td>\n",
       "      <td>18.31</td>\n",
       "      <td>17.14</td>\n",
       "      <td>17.06</td>\n",
       "      <td>17.23</td>\n",
       "      <td>17.25</td>\n",
       "      <td>17.64</td>\n",
       "      <td>17.48</td>\n",
       "      <td>17.43</td>\n",
       "      <td>18.02</td>\n",
       "      <td>16.46</td>\n",
       "      <td>17.06</td>\n",
       "      <td>17.27</td>\n",
       "      <td>17.24</td>\n",
       "      <td>17.35</td>\n",
       "      <td>17.62</td>\n",
       "      <td>16.94</td>\n",
       "      <td>17.27</td>\n",
       "      <td>17.09</td>\n",
       "      <td>17.92</td>\n",
       "      <td>17.51</td>\n",
       "      <td>16.95</td>\n",
       "      <td>17.02</td>\n",
       "      <td>17.86</td>\n",
       "      <td>17.19</td>\n",
       "      <td>16.88</td>\n",
       "      <td>18.94</td>\n",
       "      <td>18.55</td>\n",
       "      <td>17.13</td>\n",
       "      <td>16.43</td>\n",
       "      <td>17.09</td>\n",
       "      <td>16.89</td>\n",
       "      <td>16.87</td>\n",
       "      <td>16.52</td>\n",
       "      <td>19.27</td>\n",
       "      <td>17.38</td>\n",
       "      <td>16.00</td>\n",
       "      <td>16.41</td>\n",
       "      <td>15.77</td>\n",
       "      <td>16.51</td>\n",
       "      <td>15.78</td>\n",
       "      <td>15.80</td>\n",
       "      <td>268</td>\n",
       "      <td>[(0, 0), (7, 1), (0, 7)]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42714</th>\n",
       "      <td>20.98</td>\n",
       "      <td>18.98</td>\n",
       "      <td>17.47</td>\n",
       "      <td>17.55</td>\n",
       "      <td>17.65</td>\n",
       "      <td>18.11</td>\n",
       "      <td>19.89</td>\n",
       "      <td>20.99</td>\n",
       "      <td>20.08</td>\n",
       "      <td>19.09</td>\n",
       "      <td>17.95</td>\n",
       "      <td>17.64</td>\n",
       "      <td>17.75</td>\n",
       "      <td>17.78</td>\n",
       "      <td>20.09</td>\n",
       "      <td>20.36</td>\n",
       "      <td>17.31</td>\n",
       "      <td>17.19</td>\n",
       "      <td>17.48</td>\n",
       "      <td>17.19</td>\n",
       "      <td>17.54</td>\n",
       "      <td>17.90</td>\n",
       "      <td>17.75</td>\n",
       "      <td>18.10</td>\n",
       "      <td>16.99</td>\n",
       "      <td>17.04</td>\n",
       "      <td>17.32</td>\n",
       "      <td>17.17</td>\n",
       "      <td>17.20</td>\n",
       "      <td>17.24</td>\n",
       "      <td>17.81</td>\n",
       "      <td>17.52</td>\n",
       "      <td>16.71</td>\n",
       "      <td>17.14</td>\n",
       "      <td>17.29</td>\n",
       "      <td>17.27</td>\n",
       "      <td>17.28</td>\n",
       "      <td>17.42</td>\n",
       "      <td>16.98</td>\n",
       "      <td>17.71</td>\n",
       "      <td>16.80</td>\n",
       "      <td>17.54</td>\n",
       "      <td>17.56</td>\n",
       "      <td>16.91</td>\n",
       "      <td>17.23</td>\n",
       "      <td>17.65</td>\n",
       "      <td>17.22</td>\n",
       "      <td>16.67</td>\n",
       "      <td>18.86</td>\n",
       "      <td>19.12</td>\n",
       "      <td>17.79</td>\n",
       "      <td>16.61</td>\n",
       "      <td>17.12</td>\n",
       "      <td>16.87</td>\n",
       "      <td>16.45</td>\n",
       "      <td>16.29</td>\n",
       "      <td>19.73</td>\n",
       "      <td>18.22</td>\n",
       "      <td>16.39</td>\n",
       "      <td>16.51</td>\n",
       "      <td>16.25</td>\n",
       "      <td>16.54</td>\n",
       "      <td>16.12</td>\n",
       "      <td>15.61</td>\n",
       "      <td>268</td>\n",
       "      <td>[(0, 0), (7, 0), (0, 7)]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Pixel 1  Pixel 2  Pixel 3  Pixel 4  Pixel 5  Pixel 6  Pixel 7  Pixel 8  \\\n",
       "42710    20.78    18.93    17.46    17.17    17.64    18.14    17.57    18.54   \n",
       "42711    20.64    18.66    17.45    17.32    17.77    18.04    18.63    19.92   \n",
       "42712    20.97    18.72    17.37    17.44    17.66    18.24    19.46    20.88   \n",
       "42713    20.86    18.82    17.34    17.39    17.59    18.15    19.67    20.80   \n",
       "42714    20.98    18.98    17.47    17.55    17.65    18.11    19.89    20.99   \n",
       "\n",
       "       Pixel 9  Pixel 10  Pixel 11  Pixel 12  Pixel 13  Pixel 14  Pixel 15  \\\n",
       "42710    20.10     19.27     17.82     17.34     17.68     17.84     19.82   \n",
       "42711    20.15     19.11     17.77     17.49     17.42     17.65     19.83   \n",
       "42712    20.13     19.00     17.59     17.46     17.61     17.92     19.90   \n",
       "42713    20.04     19.04     17.68     17.52     17.68     18.32     20.09   \n",
       "42714    20.08     19.09     17.95     17.64     17.75     17.78     20.09   \n",
       "\n",
       "       Pixel 16  Pixel 17  Pixel 18  Pixel 19  Pixel 20  Pixel 21  Pixel 22  \\\n",
       "42710     21.44     17.43     17.38     17.40     17.45     17.52     18.22   \n",
       "42711     20.97     17.17     17.43     17.19     16.97     17.48     17.95   \n",
       "42712     20.69     17.02     16.96     17.30     17.16     17.51     18.06   \n",
       "42713     20.86     17.20     17.29     17.66     17.52     17.71     18.00   \n",
       "42714     20.36     17.31     17.19     17.48     17.19     17.54     17.90   \n",
       "\n",
       "       Pixel 23  Pixel 24  Pixel 25  Pixel 26  Pixel 27  Pixel 28  Pixel 29  \\\n",
       "42710     19.74     20.23     16.86     16.93     17.34     17.59     17.50   \n",
       "42711     18.80     18.30     16.90     16.91     17.17     17.42     17.27   \n",
       "42712     17.55     17.57     16.77     16.90     17.17     17.19     17.23   \n",
       "42713     17.48     18.31     17.14     17.06     17.23     17.25     17.64   \n",
       "42714     17.75     18.10     16.99     17.04     17.32     17.17     17.20   \n",
       "\n",
       "       Pixel 30  Pixel 31  Pixel 32  Pixel 33  Pixel 34  Pixel 35  Pixel 36  \\\n",
       "42710     17.55     17.67     17.94     16.73     17.04     17.24     17.10   \n",
       "42711     17.52     17.51     17.70     16.83     16.94     17.15     16.98   \n",
       "42712     17.40     17.68     17.42     16.80     16.92     17.17     16.97   \n",
       "42713     17.48     17.43     18.02     16.46     17.06     17.27     17.24   \n",
       "42714     17.24     17.81     17.52     16.71     17.14     17.29     17.27   \n",
       "\n",
       "       Pixel 37  Pixel 38  Pixel 39  Pixel 40  Pixel 41  Pixel 42  Pixel 43  \\\n",
       "42710     17.21     17.50     17.13     17.45     17.04     17.83     17.33   \n",
       "42711     17.16     17.20     16.88     17.53     17.15     17.89     17.23   \n",
       "42712     17.25     17.41     16.91     17.27     16.81     17.96     17.19   \n",
       "42713     17.35     17.62     16.94     17.27     17.09     17.92     17.51   \n",
       "42714     17.28     17.42     16.98     17.71     16.80     17.54     17.56   \n",
       "\n",
       "       Pixel 44  Pixel 45  Pixel 46  Pixel 47  Pixel 48  Pixel 49  Pixel 50  \\\n",
       "42710     16.96     16.98     17.67     16.99     16.91     19.21     18.93   \n",
       "42711     17.00     17.16     17.82     17.16     16.77     18.98     18.77   \n",
       "42712     16.93     17.14     17.61     16.94     16.59     18.97     18.39   \n",
       "42713     16.95     17.02     17.86     17.19     16.88     18.94     18.55   \n",
       "42714     16.91     17.23     17.65     17.22     16.67     18.86     19.12   \n",
       "\n",
       "       Pixel 51  Pixel 52  Pixel 53  Pixel 54  Pixel 55  Pixel 56  Pixel 57  \\\n",
       "42710     17.40     16.43     17.03     16.87     16.43     16.32     19.66   \n",
       "42711     17.37     16.31     17.24     17.01     16.59     16.09     19.29   \n",
       "42712     17.17     16.29     16.89     16.99     16.34     15.70     19.20   \n",
       "42713     17.13     16.43     17.09     16.89     16.87     16.52     19.27   \n",
       "42714     17.79     16.61     17.12     16.87     16.45     16.29     19.73   \n",
       "\n",
       "       Pixel 58  Pixel 59  Pixel 60  Pixel 61  Pixel 62  Pixel 63  Pixel 64  \\\n",
       "42710     17.30     16.33     16.46     16.27     16.57     16.72     15.40   \n",
       "42711     17.49     16.10     16.22     16.09     16.44     16.17     15.99   \n",
       "42712     17.39     16.16     16.21     15.89     16.57     16.23     15.68   \n",
       "42713     17.38     16.00     16.41     15.77     16.51     15.78     15.80   \n",
       "42714     18.22     16.39     16.51     16.25     16.54     16.12     15.61   \n",
       "\n",
       "       session        target_coordinates  people_#  \n",
       "42710      268  [(0, 0), (7, 1), (0, 7)]         3  \n",
       "42711      268  [(0, 0), (7, 1), (0, 7)]         3  \n",
       "42712      268  [(0, 0), (7, 0), (0, 7)]         3  \n",
       "42713      268  [(0, 0), (7, 1), (0, 7)]         3  \n",
       "42714      268  [(0, 0), (7, 0), (0, 7)]         3  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.tail() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "625776d3-0d01-4c1d-903e-33f64853f5a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pixel 1</th>\n",
       "      <th>Pixel 2</th>\n",
       "      <th>Pixel 3</th>\n",
       "      <th>Pixel 4</th>\n",
       "      <th>Pixel 5</th>\n",
       "      <th>Pixel 6</th>\n",
       "      <th>Pixel 7</th>\n",
       "      <th>Pixel 8</th>\n",
       "      <th>Pixel 9</th>\n",
       "      <th>Pixel 10</th>\n",
       "      <th>Pixel 11</th>\n",
       "      <th>Pixel 12</th>\n",
       "      <th>Pixel 13</th>\n",
       "      <th>Pixel 14</th>\n",
       "      <th>Pixel 15</th>\n",
       "      <th>Pixel 16</th>\n",
       "      <th>Pixel 17</th>\n",
       "      <th>Pixel 18</th>\n",
       "      <th>Pixel 19</th>\n",
       "      <th>Pixel 20</th>\n",
       "      <th>Pixel 21</th>\n",
       "      <th>Pixel 22</th>\n",
       "      <th>Pixel 23</th>\n",
       "      <th>Pixel 24</th>\n",
       "      <th>Pixel 25</th>\n",
       "      <th>Pixel 26</th>\n",
       "      <th>Pixel 27</th>\n",
       "      <th>Pixel 28</th>\n",
       "      <th>Pixel 29</th>\n",
       "      <th>Pixel 30</th>\n",
       "      <th>Pixel 31</th>\n",
       "      <th>Pixel 32</th>\n",
       "      <th>Pixel 33</th>\n",
       "      <th>Pixel 34</th>\n",
       "      <th>Pixel 35</th>\n",
       "      <th>Pixel 36</th>\n",
       "      <th>Pixel 37</th>\n",
       "      <th>Pixel 38</th>\n",
       "      <th>Pixel 39</th>\n",
       "      <th>Pixel 40</th>\n",
       "      <th>Pixel 41</th>\n",
       "      <th>Pixel 42</th>\n",
       "      <th>Pixel 43</th>\n",
       "      <th>Pixel 44</th>\n",
       "      <th>Pixel 45</th>\n",
       "      <th>Pixel 46</th>\n",
       "      <th>Pixel 47</th>\n",
       "      <th>Pixel 48</th>\n",
       "      <th>Pixel 49</th>\n",
       "      <th>Pixel 50</th>\n",
       "      <th>Pixel 51</th>\n",
       "      <th>Pixel 52</th>\n",
       "      <th>Pixel 53</th>\n",
       "      <th>Pixel 54</th>\n",
       "      <th>Pixel 55</th>\n",
       "      <th>Pixel 56</th>\n",
       "      <th>Pixel 57</th>\n",
       "      <th>Pixel 58</th>\n",
       "      <th>Pixel 59</th>\n",
       "      <th>Pixel 60</th>\n",
       "      <th>Pixel 61</th>\n",
       "      <th>Pixel 62</th>\n",
       "      <th>Pixel 63</th>\n",
       "      <th>Pixel 64</th>\n",
       "      <th>session</th>\n",
       "      <th>people_#</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>42715.000000</td>\n",
       "      <td>42715.000000</td>\n",
       "      <td>42715.000000</td>\n",
       "      <td>42715.000000</td>\n",
       "      <td>42715.000000</td>\n",
       "      <td>42715.000000</td>\n",
       "      <td>42715.000000</td>\n",
       "      <td>42715.000000</td>\n",
       "      <td>42715.000000</td>\n",
       "      <td>42715.000000</td>\n",
       "      <td>42715.000000</td>\n",
       "      <td>42715.000000</td>\n",
       "      <td>42715.000000</td>\n",
       "      <td>42715.000000</td>\n",
       "      <td>42715.000000</td>\n",
       "      <td>42715.000000</td>\n",
       "      <td>42715.000000</td>\n",
       "      <td>42715.000000</td>\n",
       "      <td>42715.000000</td>\n",
       "      <td>42715.000000</td>\n",
       "      <td>42715.000000</td>\n",
       "      <td>42715.00000</td>\n",
       "      <td>42715.000000</td>\n",
       "      <td>42715.000000</td>\n",
       "      <td>42715.000000</td>\n",
       "      <td>42715.000000</td>\n",
       "      <td>42715.000000</td>\n",
       "      <td>42715.000000</td>\n",
       "      <td>42715.000000</td>\n",
       "      <td>42715.000000</td>\n",
       "      <td>42715.000000</td>\n",
       "      <td>42715.000000</td>\n",
       "      <td>42715.000000</td>\n",
       "      <td>42715.000000</td>\n",
       "      <td>42715.000000</td>\n",
       "      <td>42715.000000</td>\n",
       "      <td>42715.000000</td>\n",
       "      <td>42715.000000</td>\n",
       "      <td>42715.000000</td>\n",
       "      <td>42715.000000</td>\n",
       "      <td>42715.000000</td>\n",
       "      <td>42715.000000</td>\n",
       "      <td>42715.000000</td>\n",
       "      <td>42715.000000</td>\n",
       "      <td>42715.000000</td>\n",
       "      <td>42715.000000</td>\n",
       "      <td>42715.000000</td>\n",
       "      <td>42715.000000</td>\n",
       "      <td>42715.000000</td>\n",
       "      <td>42715.000000</td>\n",
       "      <td>42715.000000</td>\n",
       "      <td>42715.000000</td>\n",
       "      <td>42715.000000</td>\n",
       "      <td>42715.000000</td>\n",
       "      <td>42715.000000</td>\n",
       "      <td>42715.000000</td>\n",
       "      <td>42715.000000</td>\n",
       "      <td>42715.00000</td>\n",
       "      <td>42715.000000</td>\n",
       "      <td>42715.000000</td>\n",
       "      <td>42715.000000</td>\n",
       "      <td>42715.000000</td>\n",
       "      <td>42715.000000</td>\n",
       "      <td>42715.000000</td>\n",
       "      <td>42715.000000</td>\n",
       "      <td>42715.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>16.845126</td>\n",
       "      <td>16.898928</td>\n",
       "      <td>16.965858</td>\n",
       "      <td>16.986448</td>\n",
       "      <td>17.153517</td>\n",
       "      <td>17.446585</td>\n",
       "      <td>17.026555</td>\n",
       "      <td>16.893169</td>\n",
       "      <td>16.714208</td>\n",
       "      <td>16.719344</td>\n",
       "      <td>17.040512</td>\n",
       "      <td>17.030901</td>\n",
       "      <td>17.230637</td>\n",
       "      <td>17.227105</td>\n",
       "      <td>17.057059</td>\n",
       "      <td>17.012497</td>\n",
       "      <td>16.770933</td>\n",
       "      <td>16.746901</td>\n",
       "      <td>16.901666</td>\n",
       "      <td>16.899997</td>\n",
       "      <td>16.986785</td>\n",
       "      <td>17.45893</td>\n",
       "      <td>16.915009</td>\n",
       "      <td>17.152332</td>\n",
       "      <td>16.770055</td>\n",
       "      <td>16.651906</td>\n",
       "      <td>16.801741</td>\n",
       "      <td>16.943964</td>\n",
       "      <td>16.999932</td>\n",
       "      <td>17.019572</td>\n",
       "      <td>17.254725</td>\n",
       "      <td>17.013607</td>\n",
       "      <td>16.527193</td>\n",
       "      <td>16.655184</td>\n",
       "      <td>16.818231</td>\n",
       "      <td>16.850912</td>\n",
       "      <td>17.058185</td>\n",
       "      <td>17.061847</td>\n",
       "      <td>16.799116</td>\n",
       "      <td>16.939650</td>\n",
       "      <td>16.665056</td>\n",
       "      <td>16.739507</td>\n",
       "      <td>16.656212</td>\n",
       "      <td>16.727167</td>\n",
       "      <td>16.832454</td>\n",
       "      <td>17.355020</td>\n",
       "      <td>17.088716</td>\n",
       "      <td>16.357971</td>\n",
       "      <td>16.072622</td>\n",
       "      <td>16.279093</td>\n",
       "      <td>16.705724</td>\n",
       "      <td>16.337583</td>\n",
       "      <td>16.969345</td>\n",
       "      <td>16.809721</td>\n",
       "      <td>16.600954</td>\n",
       "      <td>15.995530</td>\n",
       "      <td>15.816963</td>\n",
       "      <td>15.79334</td>\n",
       "      <td>15.812303</td>\n",
       "      <td>16.074885</td>\n",
       "      <td>15.932682</td>\n",
       "      <td>16.405943</td>\n",
       "      <td>16.183402</td>\n",
       "      <td>15.298757</td>\n",
       "      <td>130.024980</td>\n",
       "      <td>2.216130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.215600</td>\n",
       "      <td>1.149243</td>\n",
       "      <td>1.130387</td>\n",
       "      <td>1.221679</td>\n",
       "      <td>1.195400</td>\n",
       "      <td>1.364527</td>\n",
       "      <td>1.464996</td>\n",
       "      <td>1.380490</td>\n",
       "      <td>1.309463</td>\n",
       "      <td>1.221094</td>\n",
       "      <td>1.115030</td>\n",
       "      <td>1.045392</td>\n",
       "      <td>1.017713</td>\n",
       "      <td>1.121288</td>\n",
       "      <td>1.227332</td>\n",
       "      <td>1.257311</td>\n",
       "      <td>1.202482</td>\n",
       "      <td>0.994535</td>\n",
       "      <td>1.017881</td>\n",
       "      <td>0.889819</td>\n",
       "      <td>0.946620</td>\n",
       "      <td>0.97162</td>\n",
       "      <td>1.064509</td>\n",
       "      <td>1.145809</td>\n",
       "      <td>1.133319</td>\n",
       "      <td>0.965033</td>\n",
       "      <td>0.942735</td>\n",
       "      <td>0.976712</td>\n",
       "      <td>0.944356</td>\n",
       "      <td>0.926091</td>\n",
       "      <td>1.189607</td>\n",
       "      <td>1.240218</td>\n",
       "      <td>1.142780</td>\n",
       "      <td>0.927693</td>\n",
       "      <td>0.982977</td>\n",
       "      <td>0.934599</td>\n",
       "      <td>0.905964</td>\n",
       "      <td>0.897566</td>\n",
       "      <td>1.051827</td>\n",
       "      <td>1.159494</td>\n",
       "      <td>1.120777</td>\n",
       "      <td>1.057776</td>\n",
       "      <td>0.929318</td>\n",
       "      <td>0.924799</td>\n",
       "      <td>0.900570</td>\n",
       "      <td>1.038722</td>\n",
       "      <td>1.102789</td>\n",
       "      <td>1.056512</td>\n",
       "      <td>1.295540</td>\n",
       "      <td>1.112387</td>\n",
       "      <td>1.029952</td>\n",
       "      <td>1.040687</td>\n",
       "      <td>1.011910</td>\n",
       "      <td>1.070460</td>\n",
       "      <td>1.264246</td>\n",
       "      <td>1.031206</td>\n",
       "      <td>1.277267</td>\n",
       "      <td>1.02758</td>\n",
       "      <td>1.002278</td>\n",
       "      <td>1.171418</td>\n",
       "      <td>1.035883</td>\n",
       "      <td>0.975930</td>\n",
       "      <td>1.127857</td>\n",
       "      <td>0.912303</td>\n",
       "      <td>78.536814</td>\n",
       "      <td>0.848178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>12.910000</td>\n",
       "      <td>12.490000</td>\n",
       "      <td>12.790000</td>\n",
       "      <td>13.230000</td>\n",
       "      <td>12.960000</td>\n",
       "      <td>12.510000</td>\n",
       "      <td>12.600000</td>\n",
       "      <td>12.020000</td>\n",
       "      <td>13.050000</td>\n",
       "      <td>13.080000</td>\n",
       "      <td>12.410000</td>\n",
       "      <td>13.500000</td>\n",
       "      <td>13.820000</td>\n",
       "      <td>13.700000</td>\n",
       "      <td>13.330000</td>\n",
       "      <td>12.110000</td>\n",
       "      <td>12.920000</td>\n",
       "      <td>13.050000</td>\n",
       "      <td>12.480000</td>\n",
       "      <td>13.430000</td>\n",
       "      <td>13.430000</td>\n",
       "      <td>13.98000</td>\n",
       "      <td>13.190000</td>\n",
       "      <td>13.090000</td>\n",
       "      <td>13.590000</td>\n",
       "      <td>13.230000</td>\n",
       "      <td>13.300000</td>\n",
       "      <td>13.400000</td>\n",
       "      <td>13.270000</td>\n",
       "      <td>13.350000</td>\n",
       "      <td>13.440000</td>\n",
       "      <td>13.020000</td>\n",
       "      <td>12.090000</td>\n",
       "      <td>13.180000</td>\n",
       "      <td>13.520000</td>\n",
       "      <td>13.030000</td>\n",
       "      <td>13.300000</td>\n",
       "      <td>13.290000</td>\n",
       "      <td>13.280000</td>\n",
       "      <td>12.860000</td>\n",
       "      <td>12.990000</td>\n",
       "      <td>13.270000</td>\n",
       "      <td>13.220000</td>\n",
       "      <td>13.190000</td>\n",
       "      <td>13.360000</td>\n",
       "      <td>13.830000</td>\n",
       "      <td>13.390000</td>\n",
       "      <td>12.520000</td>\n",
       "      <td>12.360000</td>\n",
       "      <td>12.520000</td>\n",
       "      <td>13.020000</td>\n",
       "      <td>12.670000</td>\n",
       "      <td>13.390000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>12.860000</td>\n",
       "      <td>12.040000</td>\n",
       "      <td>12.120000</td>\n",
       "      <td>12.24000</td>\n",
       "      <td>12.180000</td>\n",
       "      <td>12.560000</td>\n",
       "      <td>12.210000</td>\n",
       "      <td>12.980000</td>\n",
       "      <td>12.730000</td>\n",
       "      <td>12.010000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>16.200000</td>\n",
       "      <td>16.280000</td>\n",
       "      <td>16.340000</td>\n",
       "      <td>16.340000</td>\n",
       "      <td>16.610000</td>\n",
       "      <td>16.900000</td>\n",
       "      <td>16.290000</td>\n",
       "      <td>16.320000</td>\n",
       "      <td>15.970000</td>\n",
       "      <td>16.010000</td>\n",
       "      <td>16.410000</td>\n",
       "      <td>16.440000</td>\n",
       "      <td>16.650000</td>\n",
       "      <td>16.570000</td>\n",
       "      <td>16.340000</td>\n",
       "      <td>16.440000</td>\n",
       "      <td>16.150000</td>\n",
       "      <td>16.180000</td>\n",
       "      <td>16.320000</td>\n",
       "      <td>16.370000</td>\n",
       "      <td>16.450000</td>\n",
       "      <td>16.92000</td>\n",
       "      <td>16.320000</td>\n",
       "      <td>16.540000</td>\n",
       "      <td>16.140000</td>\n",
       "      <td>16.100000</td>\n",
       "      <td>16.260000</td>\n",
       "      <td>16.365000</td>\n",
       "      <td>16.450000</td>\n",
       "      <td>16.510000</td>\n",
       "      <td>16.610000</td>\n",
       "      <td>16.340000</td>\n",
       "      <td>15.860000</td>\n",
       "      <td>16.110000</td>\n",
       "      <td>16.240000</td>\n",
       "      <td>16.290000</td>\n",
       "      <td>16.530000</td>\n",
       "      <td>16.560000</td>\n",
       "      <td>16.200000</td>\n",
       "      <td>16.290000</td>\n",
       "      <td>15.980000</td>\n",
       "      <td>16.080000</td>\n",
       "      <td>16.120000</td>\n",
       "      <td>16.180000</td>\n",
       "      <td>16.320000</td>\n",
       "      <td>16.750000</td>\n",
       "      <td>16.440000</td>\n",
       "      <td>15.770000</td>\n",
       "      <td>15.220000</td>\n",
       "      <td>15.580000</td>\n",
       "      <td>16.100000</td>\n",
       "      <td>15.760000</td>\n",
       "      <td>16.390000</td>\n",
       "      <td>16.160000</td>\n",
       "      <td>15.820000</td>\n",
       "      <td>15.400000</td>\n",
       "      <td>14.950000</td>\n",
       "      <td>15.17000</td>\n",
       "      <td>15.250000</td>\n",
       "      <td>15.440000</td>\n",
       "      <td>15.380000</td>\n",
       "      <td>15.860000</td>\n",
       "      <td>15.540000</td>\n",
       "      <td>14.750000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>16.830000</td>\n",
       "      <td>16.900000</td>\n",
       "      <td>16.950000</td>\n",
       "      <td>16.980000</td>\n",
       "      <td>17.190000</td>\n",
       "      <td>17.480000</td>\n",
       "      <td>16.910000</td>\n",
       "      <td>17.010000</td>\n",
       "      <td>16.490000</td>\n",
       "      <td>16.600000</td>\n",
       "      <td>17.080000</td>\n",
       "      <td>17.020000</td>\n",
       "      <td>17.260000</td>\n",
       "      <td>17.150000</td>\n",
       "      <td>16.890000</td>\n",
       "      <td>17.160000</td>\n",
       "      <td>16.760000</td>\n",
       "      <td>16.730000</td>\n",
       "      <td>17.010000</td>\n",
       "      <td>16.940000</td>\n",
       "      <td>17.090000</td>\n",
       "      <td>17.51000</td>\n",
       "      <td>16.860000</td>\n",
       "      <td>17.240000</td>\n",
       "      <td>16.700000</td>\n",
       "      <td>16.680000</td>\n",
       "      <td>16.880000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>17.050000</td>\n",
       "      <td>17.040000</td>\n",
       "      <td>17.270000</td>\n",
       "      <td>17.070000</td>\n",
       "      <td>16.500000</td>\n",
       "      <td>16.670000</td>\n",
       "      <td>16.930000</td>\n",
       "      <td>16.870000</td>\n",
       "      <td>17.090000</td>\n",
       "      <td>17.080000</td>\n",
       "      <td>16.720000</td>\n",
       "      <td>16.950000</td>\n",
       "      <td>16.650000</td>\n",
       "      <td>16.770000</td>\n",
       "      <td>16.700000</td>\n",
       "      <td>16.760000</td>\n",
       "      <td>16.870000</td>\n",
       "      <td>17.400000</td>\n",
       "      <td>16.980000</td>\n",
       "      <td>16.330000</td>\n",
       "      <td>15.890000</td>\n",
       "      <td>16.160000</td>\n",
       "      <td>16.660000</td>\n",
       "      <td>16.210000</td>\n",
       "      <td>16.890000</td>\n",
       "      <td>16.650000</td>\n",
       "      <td>16.340000</td>\n",
       "      <td>15.960000</td>\n",
       "      <td>15.680000</td>\n",
       "      <td>15.76000</td>\n",
       "      <td>15.750000</td>\n",
       "      <td>15.990000</td>\n",
       "      <td>15.860000</td>\n",
       "      <td>16.320000</td>\n",
       "      <td>16.030000</td>\n",
       "      <td>15.320000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>17.290000</td>\n",
       "      <td>17.370000</td>\n",
       "      <td>17.420000</td>\n",
       "      <td>17.450000</td>\n",
       "      <td>17.640000</td>\n",
       "      <td>18.030000</td>\n",
       "      <td>17.500000</td>\n",
       "      <td>17.500000</td>\n",
       "      <td>17.110000</td>\n",
       "      <td>17.260000</td>\n",
       "      <td>17.590000</td>\n",
       "      <td>17.540000</td>\n",
       "      <td>17.740000</td>\n",
       "      <td>17.820000</td>\n",
       "      <td>17.560000</td>\n",
       "      <td>17.660000</td>\n",
       "      <td>17.280000</td>\n",
       "      <td>17.260000</td>\n",
       "      <td>17.430000</td>\n",
       "      <td>17.320000</td>\n",
       "      <td>17.490000</td>\n",
       "      <td>17.96000</td>\n",
       "      <td>17.400000</td>\n",
       "      <td>17.710000</td>\n",
       "      <td>17.190000</td>\n",
       "      <td>17.150000</td>\n",
       "      <td>17.260000</td>\n",
       "      <td>17.390000</td>\n",
       "      <td>17.450000</td>\n",
       "      <td>17.480000</td>\n",
       "      <td>17.790000</td>\n",
       "      <td>17.520000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>17.120000</td>\n",
       "      <td>17.320000</td>\n",
       "      <td>17.250000</td>\n",
       "      <td>17.490000</td>\n",
       "      <td>17.520000</td>\n",
       "      <td>17.240000</td>\n",
       "      <td>17.450000</td>\n",
       "      <td>17.160000</td>\n",
       "      <td>17.330000</td>\n",
       "      <td>17.130000</td>\n",
       "      <td>17.200000</td>\n",
       "      <td>17.310000</td>\n",
       "      <td>17.960000</td>\n",
       "      <td>17.600000</td>\n",
       "      <td>16.870000</td>\n",
       "      <td>16.540000</td>\n",
       "      <td>16.790000</td>\n",
       "      <td>17.160000</td>\n",
       "      <td>16.740000</td>\n",
       "      <td>17.390000</td>\n",
       "      <td>17.250000</td>\n",
       "      <td>17.030000</td>\n",
       "      <td>16.490000</td>\n",
       "      <td>16.630000</td>\n",
       "      <td>16.31000</td>\n",
       "      <td>16.200000</td>\n",
       "      <td>16.450000</td>\n",
       "      <td>16.290000</td>\n",
       "      <td>16.790000</td>\n",
       "      <td>16.560000</td>\n",
       "      <td>15.810000</td>\n",
       "      <td>197.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>21.840000</td>\n",
       "      <td>21.790000</td>\n",
       "      <td>21.770000</td>\n",
       "      <td>22.090000</td>\n",
       "      <td>22.120000</td>\n",
       "      <td>22.230000</td>\n",
       "      <td>22.230000</td>\n",
       "      <td>22.130000</td>\n",
       "      <td>21.730000</td>\n",
       "      <td>21.340000</td>\n",
       "      <td>21.890000</td>\n",
       "      <td>22.660000</td>\n",
       "      <td>21.950000</td>\n",
       "      <td>21.670000</td>\n",
       "      <td>22.230000</td>\n",
       "      <td>22.070000</td>\n",
       "      <td>21.780000</td>\n",
       "      <td>21.820000</td>\n",
       "      <td>22.280000</td>\n",
       "      <td>22.060000</td>\n",
       "      <td>21.750000</td>\n",
       "      <td>22.54000</td>\n",
       "      <td>21.910000</td>\n",
       "      <td>22.310000</td>\n",
       "      <td>22.560000</td>\n",
       "      <td>21.540000</td>\n",
       "      <td>21.760000</td>\n",
       "      <td>22.120000</td>\n",
       "      <td>22.290000</td>\n",
       "      <td>22.120000</td>\n",
       "      <td>22.230000</td>\n",
       "      <td>22.040000</td>\n",
       "      <td>21.450000</td>\n",
       "      <td>21.570000</td>\n",
       "      <td>22.690000</td>\n",
       "      <td>22.190000</td>\n",
       "      <td>22.100000</td>\n",
       "      <td>21.700000</td>\n",
       "      <td>22.430000</td>\n",
       "      <td>22.400000</td>\n",
       "      <td>21.480000</td>\n",
       "      <td>21.580000</td>\n",
       "      <td>21.720000</td>\n",
       "      <td>22.650000</td>\n",
       "      <td>21.640000</td>\n",
       "      <td>22.120000</td>\n",
       "      <td>21.720000</td>\n",
       "      <td>21.540000</td>\n",
       "      <td>20.730000</td>\n",
       "      <td>20.910000</td>\n",
       "      <td>21.420000</td>\n",
       "      <td>21.700000</td>\n",
       "      <td>21.930000</td>\n",
       "      <td>21.440000</td>\n",
       "      <td>21.320000</td>\n",
       "      <td>21.230000</td>\n",
       "      <td>20.270000</td>\n",
       "      <td>20.48000</td>\n",
       "      <td>20.900000</td>\n",
       "      <td>21.890000</td>\n",
       "      <td>21.070000</td>\n",
       "      <td>21.470000</td>\n",
       "      <td>21.420000</td>\n",
       "      <td>19.720000</td>\n",
       "      <td>268.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Pixel 1       Pixel 2       Pixel 3       Pixel 4       Pixel 5  \\\n",
       "count  42715.000000  42715.000000  42715.000000  42715.000000  42715.000000   \n",
       "mean      16.845126     16.898928     16.965858     16.986448     17.153517   \n",
       "std        1.215600      1.149243      1.130387      1.221679      1.195400   \n",
       "min       12.910000     12.490000     12.790000     13.230000     12.960000   \n",
       "25%       16.200000     16.280000     16.340000     16.340000     16.610000   \n",
       "50%       16.830000     16.900000     16.950000     16.980000     17.190000   \n",
       "75%       17.290000     17.370000     17.420000     17.450000     17.640000   \n",
       "max       21.840000     21.790000     21.770000     22.090000     22.120000   \n",
       "\n",
       "            Pixel 6       Pixel 7       Pixel 8       Pixel 9      Pixel 10  \\\n",
       "count  42715.000000  42715.000000  42715.000000  42715.000000  42715.000000   \n",
       "mean      17.446585     17.026555     16.893169     16.714208     16.719344   \n",
       "std        1.364527      1.464996      1.380490      1.309463      1.221094   \n",
       "min       12.510000     12.600000     12.020000     13.050000     13.080000   \n",
       "25%       16.900000     16.290000     16.320000     15.970000     16.010000   \n",
       "50%       17.480000     16.910000     17.010000     16.490000     16.600000   \n",
       "75%       18.030000     17.500000     17.500000     17.110000     17.260000   \n",
       "max       22.230000     22.230000     22.130000     21.730000     21.340000   \n",
       "\n",
       "           Pixel 11      Pixel 12      Pixel 13      Pixel 14      Pixel 15  \\\n",
       "count  42715.000000  42715.000000  42715.000000  42715.000000  42715.000000   \n",
       "mean      17.040512     17.030901     17.230637     17.227105     17.057059   \n",
       "std        1.115030      1.045392      1.017713      1.121288      1.227332   \n",
       "min       12.410000     13.500000     13.820000     13.700000     13.330000   \n",
       "25%       16.410000     16.440000     16.650000     16.570000     16.340000   \n",
       "50%       17.080000     17.020000     17.260000     17.150000     16.890000   \n",
       "75%       17.590000     17.540000     17.740000     17.820000     17.560000   \n",
       "max       21.890000     22.660000     21.950000     21.670000     22.230000   \n",
       "\n",
       "           Pixel 16      Pixel 17      Pixel 18      Pixel 19      Pixel 20  \\\n",
       "count  42715.000000  42715.000000  42715.000000  42715.000000  42715.000000   \n",
       "mean      17.012497     16.770933     16.746901     16.901666     16.899997   \n",
       "std        1.257311      1.202482      0.994535      1.017881      0.889819   \n",
       "min       12.110000     12.920000     13.050000     12.480000     13.430000   \n",
       "25%       16.440000     16.150000     16.180000     16.320000     16.370000   \n",
       "50%       17.160000     16.760000     16.730000     17.010000     16.940000   \n",
       "75%       17.660000     17.280000     17.260000     17.430000     17.320000   \n",
       "max       22.070000     21.780000     21.820000     22.280000     22.060000   \n",
       "\n",
       "           Pixel 21     Pixel 22      Pixel 23      Pixel 24      Pixel 25  \\\n",
       "count  42715.000000  42715.00000  42715.000000  42715.000000  42715.000000   \n",
       "mean      16.986785     17.45893     16.915009     17.152332     16.770055   \n",
       "std        0.946620      0.97162      1.064509      1.145809      1.133319   \n",
       "min       13.430000     13.98000     13.190000     13.090000     13.590000   \n",
       "25%       16.450000     16.92000     16.320000     16.540000     16.140000   \n",
       "50%       17.090000     17.51000     16.860000     17.240000     16.700000   \n",
       "75%       17.490000     17.96000     17.400000     17.710000     17.190000   \n",
       "max       21.750000     22.54000     21.910000     22.310000     22.560000   \n",
       "\n",
       "           Pixel 26      Pixel 27      Pixel 28      Pixel 29      Pixel 30  \\\n",
       "count  42715.000000  42715.000000  42715.000000  42715.000000  42715.000000   \n",
       "mean      16.651906     16.801741     16.943964     16.999932     17.019572   \n",
       "std        0.965033      0.942735      0.976712      0.944356      0.926091   \n",
       "min       13.230000     13.300000     13.400000     13.270000     13.350000   \n",
       "25%       16.100000     16.260000     16.365000     16.450000     16.510000   \n",
       "50%       16.680000     16.880000     17.000000     17.050000     17.040000   \n",
       "75%       17.150000     17.260000     17.390000     17.450000     17.480000   \n",
       "max       21.540000     21.760000     22.120000     22.290000     22.120000   \n",
       "\n",
       "           Pixel 31      Pixel 32      Pixel 33      Pixel 34      Pixel 35  \\\n",
       "count  42715.000000  42715.000000  42715.000000  42715.000000  42715.000000   \n",
       "mean      17.254725     17.013607     16.527193     16.655184     16.818231   \n",
       "std        1.189607      1.240218      1.142780      0.927693      0.982977   \n",
       "min       13.440000     13.020000     12.090000     13.180000     13.520000   \n",
       "25%       16.610000     16.340000     15.860000     16.110000     16.240000   \n",
       "50%       17.270000     17.070000     16.500000     16.670000     16.930000   \n",
       "75%       17.790000     17.520000     17.000000     17.120000     17.320000   \n",
       "max       22.230000     22.040000     21.450000     21.570000     22.690000   \n",
       "\n",
       "           Pixel 36      Pixel 37      Pixel 38      Pixel 39      Pixel 40  \\\n",
       "count  42715.000000  42715.000000  42715.000000  42715.000000  42715.000000   \n",
       "mean      16.850912     17.058185     17.061847     16.799116     16.939650   \n",
       "std        0.934599      0.905964      0.897566      1.051827      1.159494   \n",
       "min       13.030000     13.300000     13.290000     13.280000     12.860000   \n",
       "25%       16.290000     16.530000     16.560000     16.200000     16.290000   \n",
       "50%       16.870000     17.090000     17.080000     16.720000     16.950000   \n",
       "75%       17.250000     17.490000     17.520000     17.240000     17.450000   \n",
       "max       22.190000     22.100000     21.700000     22.430000     22.400000   \n",
       "\n",
       "           Pixel 41      Pixel 42      Pixel 43      Pixel 44      Pixel 45  \\\n",
       "count  42715.000000  42715.000000  42715.000000  42715.000000  42715.000000   \n",
       "mean      16.665056     16.739507     16.656212     16.727167     16.832454   \n",
       "std        1.120777      1.057776      0.929318      0.924799      0.900570   \n",
       "min       12.990000     13.270000     13.220000     13.190000     13.360000   \n",
       "25%       15.980000     16.080000     16.120000     16.180000     16.320000   \n",
       "50%       16.650000     16.770000     16.700000     16.760000     16.870000   \n",
       "75%       17.160000     17.330000     17.130000     17.200000     17.310000   \n",
       "max       21.480000     21.580000     21.720000     22.650000     21.640000   \n",
       "\n",
       "           Pixel 46      Pixel 47      Pixel 48      Pixel 49      Pixel 50  \\\n",
       "count  42715.000000  42715.000000  42715.000000  42715.000000  42715.000000   \n",
       "mean      17.355020     17.088716     16.357971     16.072622     16.279093   \n",
       "std        1.038722      1.102789      1.056512      1.295540      1.112387   \n",
       "min       13.830000     13.390000     12.520000     12.360000     12.520000   \n",
       "25%       16.750000     16.440000     15.770000     15.220000     15.580000   \n",
       "50%       17.400000     16.980000     16.330000     15.890000     16.160000   \n",
       "75%       17.960000     17.600000     16.870000     16.540000     16.790000   \n",
       "max       22.120000     21.720000     21.540000     20.730000     20.910000   \n",
       "\n",
       "           Pixel 51      Pixel 52      Pixel 53      Pixel 54      Pixel 55  \\\n",
       "count  42715.000000  42715.000000  42715.000000  42715.000000  42715.000000   \n",
       "mean      16.705724     16.337583     16.969345     16.809721     16.600954   \n",
       "std        1.029952      1.040687      1.011910      1.070460      1.264246   \n",
       "min       13.020000     12.670000     13.390000     13.000000     12.860000   \n",
       "25%       16.100000     15.760000     16.390000     16.160000     15.820000   \n",
       "50%       16.660000     16.210000     16.890000     16.650000     16.340000   \n",
       "75%       17.160000     16.740000     17.390000     17.250000     17.030000   \n",
       "max       21.420000     21.700000     21.930000     21.440000     21.320000   \n",
       "\n",
       "           Pixel 56      Pixel 57     Pixel 58      Pixel 59      Pixel 60  \\\n",
       "count  42715.000000  42715.000000  42715.00000  42715.000000  42715.000000   \n",
       "mean      15.995530     15.816963     15.79334     15.812303     16.074885   \n",
       "std        1.031206      1.277267      1.02758      1.002278      1.171418   \n",
       "min       12.040000     12.120000     12.24000     12.180000     12.560000   \n",
       "25%       15.400000     14.950000     15.17000     15.250000     15.440000   \n",
       "50%       15.960000     15.680000     15.76000     15.750000     15.990000   \n",
       "75%       16.490000     16.630000     16.31000     16.200000     16.450000   \n",
       "max       21.230000     20.270000     20.48000     20.900000     21.890000   \n",
       "\n",
       "           Pixel 61      Pixel 62      Pixel 63      Pixel 64       session  \\\n",
       "count  42715.000000  42715.000000  42715.000000  42715.000000  42715.000000   \n",
       "mean      15.932682     16.405943     16.183402     15.298757    130.024980   \n",
       "std        1.035883      0.975930      1.127857      0.912303     78.536814   \n",
       "min       12.210000     12.980000     12.730000     12.010000      0.000000   \n",
       "25%       15.380000     15.860000     15.540000     14.750000     61.000000   \n",
       "50%       15.860000     16.320000     16.030000     15.320000    128.000000   \n",
       "75%       16.290000     16.790000     16.560000     15.810000    197.000000   \n",
       "max       21.070000     21.470000     21.420000     19.720000    268.000000   \n",
       "\n",
       "           people_#  \n",
       "count  42715.000000  \n",
       "mean       2.216130  \n",
       "std        0.848178  \n",
       "min        0.000000  \n",
       "25%        2.000000  \n",
       "50%        2.000000  \n",
       "75%        3.000000  \n",
       "max        3.000000  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.describe() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "eef33bce-060e-4351-90cd-55d09a99cff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pixel 1</th>\n",
       "      <th>Pixel 2</th>\n",
       "      <th>Pixel 3</th>\n",
       "      <th>Pixel 4</th>\n",
       "      <th>Pixel 5</th>\n",
       "      <th>Pixel 6</th>\n",
       "      <th>Pixel 7</th>\n",
       "      <th>Pixel 8</th>\n",
       "      <th>Pixel 9</th>\n",
       "      <th>Pixel 10</th>\n",
       "      <th>Pixel 11</th>\n",
       "      <th>Pixel 12</th>\n",
       "      <th>Pixel 13</th>\n",
       "      <th>Pixel 14</th>\n",
       "      <th>Pixel 15</th>\n",
       "      <th>Pixel 16</th>\n",
       "      <th>Pixel 17</th>\n",
       "      <th>Pixel 18</th>\n",
       "      <th>Pixel 19</th>\n",
       "      <th>Pixel 20</th>\n",
       "      <th>Pixel 21</th>\n",
       "      <th>Pixel 22</th>\n",
       "      <th>Pixel 23</th>\n",
       "      <th>Pixel 24</th>\n",
       "      <th>Pixel 25</th>\n",
       "      <th>Pixel 26</th>\n",
       "      <th>Pixel 27</th>\n",
       "      <th>Pixel 28</th>\n",
       "      <th>Pixel 29</th>\n",
       "      <th>Pixel 30</th>\n",
       "      <th>Pixel 31</th>\n",
       "      <th>Pixel 32</th>\n",
       "      <th>Pixel 33</th>\n",
       "      <th>Pixel 34</th>\n",
       "      <th>Pixel 35</th>\n",
       "      <th>Pixel 36</th>\n",
       "      <th>Pixel 37</th>\n",
       "      <th>Pixel 38</th>\n",
       "      <th>Pixel 39</th>\n",
       "      <th>Pixel 40</th>\n",
       "      <th>Pixel 41</th>\n",
       "      <th>Pixel 42</th>\n",
       "      <th>Pixel 43</th>\n",
       "      <th>Pixel 44</th>\n",
       "      <th>Pixel 45</th>\n",
       "      <th>Pixel 46</th>\n",
       "      <th>Pixel 47</th>\n",
       "      <th>Pixel 48</th>\n",
       "      <th>Pixel 49</th>\n",
       "      <th>Pixel 50</th>\n",
       "      <th>Pixel 51</th>\n",
       "      <th>Pixel 52</th>\n",
       "      <th>Pixel 53</th>\n",
       "      <th>Pixel 54</th>\n",
       "      <th>Pixel 55</th>\n",
       "      <th>Pixel 56</th>\n",
       "      <th>Pixel 57</th>\n",
       "      <th>Pixel 58</th>\n",
       "      <th>Pixel 59</th>\n",
       "      <th>Pixel 60</th>\n",
       "      <th>Pixel 61</th>\n",
       "      <th>Pixel 62</th>\n",
       "      <th>Pixel 63</th>\n",
       "      <th>Pixel 64</th>\n",
       "      <th>session</th>\n",
       "      <th>people_#</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>11050.000000</td>\n",
       "      <td>11050.000000</td>\n",
       "      <td>11050.000000</td>\n",
       "      <td>11050.000000</td>\n",
       "      <td>11050.000000</td>\n",
       "      <td>11050.000000</td>\n",
       "      <td>11050.000000</td>\n",
       "      <td>11050.000000</td>\n",
       "      <td>11050.000000</td>\n",
       "      <td>11050.000000</td>\n",
       "      <td>11050.000000</td>\n",
       "      <td>11050.000000</td>\n",
       "      <td>11050.000000</td>\n",
       "      <td>11050.000000</td>\n",
       "      <td>11050.000000</td>\n",
       "      <td>11050.000000</td>\n",
       "      <td>11050.000000</td>\n",
       "      <td>11050.000000</td>\n",
       "      <td>11050.000000</td>\n",
       "      <td>11050.000000</td>\n",
       "      <td>11050.000000</td>\n",
       "      <td>11050.000000</td>\n",
       "      <td>11050.000000</td>\n",
       "      <td>11050.000000</td>\n",
       "      <td>11050.000000</td>\n",
       "      <td>11050.000000</td>\n",
       "      <td>11050.000000</td>\n",
       "      <td>11050.000000</td>\n",
       "      <td>11050.000000</td>\n",
       "      <td>11050.000000</td>\n",
       "      <td>11050.000000</td>\n",
       "      <td>11050.000000</td>\n",
       "      <td>11050.000000</td>\n",
       "      <td>11050.000000</td>\n",
       "      <td>11050.000000</td>\n",
       "      <td>11050.000000</td>\n",
       "      <td>11050.000000</td>\n",
       "      <td>11050.000000</td>\n",
       "      <td>11050.000000</td>\n",
       "      <td>11050.000000</td>\n",
       "      <td>11050.000000</td>\n",
       "      <td>11050.000000</td>\n",
       "      <td>11050.000000</td>\n",
       "      <td>11050.000000</td>\n",
       "      <td>11050.000000</td>\n",
       "      <td>11050.000000</td>\n",
       "      <td>11050.000000</td>\n",
       "      <td>11050.000000</td>\n",
       "      <td>11050.000000</td>\n",
       "      <td>11050.000000</td>\n",
       "      <td>11050.000000</td>\n",
       "      <td>11050.000000</td>\n",
       "      <td>11050.000000</td>\n",
       "      <td>11050.000000</td>\n",
       "      <td>11050.000000</td>\n",
       "      <td>11050.000000</td>\n",
       "      <td>11050.000000</td>\n",
       "      <td>11050.000000</td>\n",
       "      <td>11050.000000</td>\n",
       "      <td>11050.000000</td>\n",
       "      <td>11050.000000</td>\n",
       "      <td>11050.000000</td>\n",
       "      <td>11050.00000</td>\n",
       "      <td>11050.000000</td>\n",
       "      <td>11050.000000</td>\n",
       "      <td>11050.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>16.874330</td>\n",
       "      <td>16.921444</td>\n",
       "      <td>16.976503</td>\n",
       "      <td>16.971698</td>\n",
       "      <td>17.174776</td>\n",
       "      <td>17.478671</td>\n",
       "      <td>17.050977</td>\n",
       "      <td>16.974066</td>\n",
       "      <td>16.724832</td>\n",
       "      <td>16.744799</td>\n",
       "      <td>17.087674</td>\n",
       "      <td>17.029565</td>\n",
       "      <td>17.267544</td>\n",
       "      <td>17.237629</td>\n",
       "      <td>17.058465</td>\n",
       "      <td>17.078401</td>\n",
       "      <td>16.856981</td>\n",
       "      <td>16.809812</td>\n",
       "      <td>16.988185</td>\n",
       "      <td>16.950543</td>\n",
       "      <td>17.074374</td>\n",
       "      <td>17.511714</td>\n",
       "      <td>16.918392</td>\n",
       "      <td>17.176538</td>\n",
       "      <td>16.783467</td>\n",
       "      <td>16.717047</td>\n",
       "      <td>16.871586</td>\n",
       "      <td>16.997138</td>\n",
       "      <td>17.056458</td>\n",
       "      <td>17.038457</td>\n",
       "      <td>17.265719</td>\n",
       "      <td>17.017191</td>\n",
       "      <td>16.503244</td>\n",
       "      <td>16.684624</td>\n",
       "      <td>16.907675</td>\n",
       "      <td>16.880801</td>\n",
       "      <td>17.106652</td>\n",
       "      <td>17.080072</td>\n",
       "      <td>16.799324</td>\n",
       "      <td>16.953208</td>\n",
       "      <td>16.664822</td>\n",
       "      <td>16.781742</td>\n",
       "      <td>16.718013</td>\n",
       "      <td>16.769567</td>\n",
       "      <td>16.891094</td>\n",
       "      <td>17.410037</td>\n",
       "      <td>17.104938</td>\n",
       "      <td>16.376304</td>\n",
       "      <td>15.974565</td>\n",
       "      <td>16.247051</td>\n",
       "      <td>16.725203</td>\n",
       "      <td>16.317422</td>\n",
       "      <td>16.991969</td>\n",
       "      <td>16.814239</td>\n",
       "      <td>16.521136</td>\n",
       "      <td>15.963922</td>\n",
       "      <td>15.769124</td>\n",
       "      <td>15.784101</td>\n",
       "      <td>15.815153</td>\n",
       "      <td>16.077814</td>\n",
       "      <td>15.937851</td>\n",
       "      <td>16.414398</td>\n",
       "      <td>16.10871</td>\n",
       "      <td>15.265738</td>\n",
       "      <td>37.976561</td>\n",
       "      <td>1.949231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.033223</td>\n",
       "      <td>1.023157</td>\n",
       "      <td>1.045179</td>\n",
       "      <td>1.050542</td>\n",
       "      <td>1.071565</td>\n",
       "      <td>1.231595</td>\n",
       "      <td>1.346729</td>\n",
       "      <td>1.261470</td>\n",
       "      <td>1.282029</td>\n",
       "      <td>1.163880</td>\n",
       "      <td>1.058479</td>\n",
       "      <td>0.949195</td>\n",
       "      <td>0.984129</td>\n",
       "      <td>1.082941</td>\n",
       "      <td>1.181676</td>\n",
       "      <td>1.155345</td>\n",
       "      <td>1.203697</td>\n",
       "      <td>0.998438</td>\n",
       "      <td>0.960331</td>\n",
       "      <td>0.875535</td>\n",
       "      <td>0.949656</td>\n",
       "      <td>0.961478</td>\n",
       "      <td>1.012309</td>\n",
       "      <td>1.034940</td>\n",
       "      <td>1.065849</td>\n",
       "      <td>0.949678</td>\n",
       "      <td>0.926547</td>\n",
       "      <td>0.931826</td>\n",
       "      <td>0.920005</td>\n",
       "      <td>0.893848</td>\n",
       "      <td>1.070146</td>\n",
       "      <td>1.059314</td>\n",
       "      <td>0.996061</td>\n",
       "      <td>0.905424</td>\n",
       "      <td>0.986412</td>\n",
       "      <td>0.913349</td>\n",
       "      <td>0.909681</td>\n",
       "      <td>0.894319</td>\n",
       "      <td>1.022788</td>\n",
       "      <td>1.036986</td>\n",
       "      <td>1.031338</td>\n",
       "      <td>0.998744</td>\n",
       "      <td>0.927739</td>\n",
       "      <td>0.891337</td>\n",
       "      <td>0.904230</td>\n",
       "      <td>1.012676</td>\n",
       "      <td>1.089616</td>\n",
       "      <td>1.015642</td>\n",
       "      <td>1.132799</td>\n",
       "      <td>1.035803</td>\n",
       "      <td>0.969334</td>\n",
       "      <td>0.967350</td>\n",
       "      <td>0.969329</td>\n",
       "      <td>1.090223</td>\n",
       "      <td>1.175530</td>\n",
       "      <td>0.923270</td>\n",
       "      <td>1.129771</td>\n",
       "      <td>0.953245</td>\n",
       "      <td>0.974218</td>\n",
       "      <td>1.019293</td>\n",
       "      <td>0.961399</td>\n",
       "      <td>0.965692</td>\n",
       "      <td>1.01426</td>\n",
       "      <td>0.816669</td>\n",
       "      <td>22.348382</td>\n",
       "      <td>0.975142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>13.400000</td>\n",
       "      <td>13.490000</td>\n",
       "      <td>13.380000</td>\n",
       "      <td>13.420000</td>\n",
       "      <td>13.120000</td>\n",
       "      <td>13.090000</td>\n",
       "      <td>12.810000</td>\n",
       "      <td>12.220000</td>\n",
       "      <td>13.260000</td>\n",
       "      <td>13.230000</td>\n",
       "      <td>13.140000</td>\n",
       "      <td>13.320000</td>\n",
       "      <td>13.760000</td>\n",
       "      <td>13.680000</td>\n",
       "      <td>13.210000</td>\n",
       "      <td>12.520000</td>\n",
       "      <td>12.730000</td>\n",
       "      <td>13.120000</td>\n",
       "      <td>13.230000</td>\n",
       "      <td>13.100000</td>\n",
       "      <td>13.350000</td>\n",
       "      <td>13.910000</td>\n",
       "      <td>13.160000</td>\n",
       "      <td>13.390000</td>\n",
       "      <td>13.320000</td>\n",
       "      <td>12.950000</td>\n",
       "      <td>13.130000</td>\n",
       "      <td>13.240000</td>\n",
       "      <td>13.330000</td>\n",
       "      <td>13.300000</td>\n",
       "      <td>13.730000</td>\n",
       "      <td>13.080000</td>\n",
       "      <td>13.130000</td>\n",
       "      <td>13.200000</td>\n",
       "      <td>13.020000</td>\n",
       "      <td>13.290000</td>\n",
       "      <td>13.500000</td>\n",
       "      <td>13.450000</td>\n",
       "      <td>13.110000</td>\n",
       "      <td>13.360000</td>\n",
       "      <td>13.010000</td>\n",
       "      <td>13.270000</td>\n",
       "      <td>13.080000</td>\n",
       "      <td>12.970000</td>\n",
       "      <td>13.160000</td>\n",
       "      <td>13.850000</td>\n",
       "      <td>13.610000</td>\n",
       "      <td>12.240000</td>\n",
       "      <td>12.460000</td>\n",
       "      <td>12.760000</td>\n",
       "      <td>13.130000</td>\n",
       "      <td>12.570000</td>\n",
       "      <td>13.350000</td>\n",
       "      <td>13.100000</td>\n",
       "      <td>12.760000</td>\n",
       "      <td>12.160000</td>\n",
       "      <td>12.100000</td>\n",
       "      <td>12.290000</td>\n",
       "      <td>12.260000</td>\n",
       "      <td>12.170000</td>\n",
       "      <td>12.170000</td>\n",
       "      <td>12.860000</td>\n",
       "      <td>12.54000</td>\n",
       "      <td>12.020000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>16.360000</td>\n",
       "      <td>16.400000</td>\n",
       "      <td>16.420000</td>\n",
       "      <td>16.460000</td>\n",
       "      <td>16.730000</td>\n",
       "      <td>17.020000</td>\n",
       "      <td>16.410000</td>\n",
       "      <td>16.510000</td>\n",
       "      <td>16.020000</td>\n",
       "      <td>16.100000</td>\n",
       "      <td>16.540000</td>\n",
       "      <td>16.520000</td>\n",
       "      <td>16.750000</td>\n",
       "      <td>16.650000</td>\n",
       "      <td>16.410000</td>\n",
       "      <td>16.640000</td>\n",
       "      <td>16.262500</td>\n",
       "      <td>16.230000</td>\n",
       "      <td>16.490000</td>\n",
       "      <td>16.450000</td>\n",
       "      <td>16.590000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>16.370000</td>\n",
       "      <td>16.690000</td>\n",
       "      <td>16.200000</td>\n",
       "      <td>16.180000</td>\n",
       "      <td>16.350000</td>\n",
       "      <td>16.480000</td>\n",
       "      <td>16.540000</td>\n",
       "      <td>16.550000</td>\n",
       "      <td>16.750000</td>\n",
       "      <td>16.510000</td>\n",
       "      <td>15.950000</td>\n",
       "      <td>16.170000</td>\n",
       "      <td>16.390000</td>\n",
       "      <td>16.370000</td>\n",
       "      <td>16.600000</td>\n",
       "      <td>16.580000</td>\n",
       "      <td>16.220000</td>\n",
       "      <td>16.400000</td>\n",
       "      <td>16.070000</td>\n",
       "      <td>16.230000</td>\n",
       "      <td>16.220000</td>\n",
       "      <td>16.270000</td>\n",
       "      <td>16.380000</td>\n",
       "      <td>16.850000</td>\n",
       "      <td>16.460000</td>\n",
       "      <td>15.810000</td>\n",
       "      <td>15.260000</td>\n",
       "      <td>15.640000</td>\n",
       "      <td>16.170000</td>\n",
       "      <td>15.800000</td>\n",
       "      <td>16.432500</td>\n",
       "      <td>16.160000</td>\n",
       "      <td>15.820000</td>\n",
       "      <td>15.430000</td>\n",
       "      <td>15.040000</td>\n",
       "      <td>15.240000</td>\n",
       "      <td>15.310000</td>\n",
       "      <td>15.540000</td>\n",
       "      <td>15.440000</td>\n",
       "      <td>15.880000</td>\n",
       "      <td>15.55000</td>\n",
       "      <td>14.790000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>16.860000</td>\n",
       "      <td>16.900000</td>\n",
       "      <td>16.950000</td>\n",
       "      <td>16.970000</td>\n",
       "      <td>17.180000</td>\n",
       "      <td>17.460000</td>\n",
       "      <td>16.920000</td>\n",
       "      <td>17.040000</td>\n",
       "      <td>16.480000</td>\n",
       "      <td>16.590000</td>\n",
       "      <td>17.060000</td>\n",
       "      <td>17.010000</td>\n",
       "      <td>17.250000</td>\n",
       "      <td>17.120000</td>\n",
       "      <td>16.870000</td>\n",
       "      <td>17.150000</td>\n",
       "      <td>16.770000</td>\n",
       "      <td>16.730000</td>\n",
       "      <td>17.070000</td>\n",
       "      <td>16.980000</td>\n",
       "      <td>17.110000</td>\n",
       "      <td>17.500000</td>\n",
       "      <td>16.820000</td>\n",
       "      <td>17.220000</td>\n",
       "      <td>16.720000</td>\n",
       "      <td>16.730000</td>\n",
       "      <td>16.920000</td>\n",
       "      <td>17.030000</td>\n",
       "      <td>17.070000</td>\n",
       "      <td>17.020000</td>\n",
       "      <td>17.240000</td>\n",
       "      <td>17.060000</td>\n",
       "      <td>16.510000</td>\n",
       "      <td>16.680000</td>\n",
       "      <td>16.960000</td>\n",
       "      <td>16.860000</td>\n",
       "      <td>17.100000</td>\n",
       "      <td>17.040000</td>\n",
       "      <td>16.680000</td>\n",
       "      <td>16.930000</td>\n",
       "      <td>16.660000</td>\n",
       "      <td>16.790000</td>\n",
       "      <td>16.730000</td>\n",
       "      <td>16.770000</td>\n",
       "      <td>16.870000</td>\n",
       "      <td>17.380000</td>\n",
       "      <td>16.960000</td>\n",
       "      <td>16.300000</td>\n",
       "      <td>15.890000</td>\n",
       "      <td>16.170000</td>\n",
       "      <td>16.680000</td>\n",
       "      <td>16.210000</td>\n",
       "      <td>16.890000</td>\n",
       "      <td>16.600000</td>\n",
       "      <td>16.290000</td>\n",
       "      <td>15.930000</td>\n",
       "      <td>15.690000</td>\n",
       "      <td>15.750000</td>\n",
       "      <td>15.750000</td>\n",
       "      <td>16.020000</td>\n",
       "      <td>15.870000</td>\n",
       "      <td>16.300000</td>\n",
       "      <td>15.98000</td>\n",
       "      <td>15.270000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>17.280000</td>\n",
       "      <td>17.340000</td>\n",
       "      <td>17.390000</td>\n",
       "      <td>17.420000</td>\n",
       "      <td>17.600000</td>\n",
       "      <td>17.920000</td>\n",
       "      <td>17.430000</td>\n",
       "      <td>17.450000</td>\n",
       "      <td>17.070000</td>\n",
       "      <td>17.230000</td>\n",
       "      <td>17.590000</td>\n",
       "      <td>17.500000</td>\n",
       "      <td>17.750000</td>\n",
       "      <td>17.750000</td>\n",
       "      <td>17.480000</td>\n",
       "      <td>17.610000</td>\n",
       "      <td>17.317500</td>\n",
       "      <td>17.340000</td>\n",
       "      <td>17.510000</td>\n",
       "      <td>17.370000</td>\n",
       "      <td>17.540000</td>\n",
       "      <td>18.020000</td>\n",
       "      <td>17.390000</td>\n",
       "      <td>17.680000</td>\n",
       "      <td>17.180000</td>\n",
       "      <td>17.180000</td>\n",
       "      <td>17.310000</td>\n",
       "      <td>17.420000</td>\n",
       "      <td>17.500000</td>\n",
       "      <td>17.510000</td>\n",
       "      <td>17.770000</td>\n",
       "      <td>17.510000</td>\n",
       "      <td>16.940000</td>\n",
       "      <td>17.110000</td>\n",
       "      <td>17.360000</td>\n",
       "      <td>17.270000</td>\n",
       "      <td>17.520000</td>\n",
       "      <td>17.560000</td>\n",
       "      <td>17.240000</td>\n",
       "      <td>17.430000</td>\n",
       "      <td>17.090000</td>\n",
       "      <td>17.280000</td>\n",
       "      <td>17.150000</td>\n",
       "      <td>17.210000</td>\n",
       "      <td>17.370000</td>\n",
       "      <td>17.960000</td>\n",
       "      <td>17.580000</td>\n",
       "      <td>16.840000</td>\n",
       "      <td>16.400000</td>\n",
       "      <td>16.707500</td>\n",
       "      <td>17.150000</td>\n",
       "      <td>16.700000</td>\n",
       "      <td>17.410000</td>\n",
       "      <td>17.240000</td>\n",
       "      <td>16.930000</td>\n",
       "      <td>16.450000</td>\n",
       "      <td>16.460000</td>\n",
       "      <td>16.270000</td>\n",
       "      <td>16.150000</td>\n",
       "      <td>16.450000</td>\n",
       "      <td>16.300000</td>\n",
       "      <td>16.790000</td>\n",
       "      <td>16.50000</td>\n",
       "      <td>15.770000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>21.140000</td>\n",
       "      <td>21.590000</td>\n",
       "      <td>21.750000</td>\n",
       "      <td>21.970000</td>\n",
       "      <td>22.510000</td>\n",
       "      <td>22.640000</td>\n",
       "      <td>22.390000</td>\n",
       "      <td>21.640000</td>\n",
       "      <td>21.670000</td>\n",
       "      <td>21.210000</td>\n",
       "      <td>21.340000</td>\n",
       "      <td>21.430000</td>\n",
       "      <td>21.520000</td>\n",
       "      <td>21.950000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>21.800000</td>\n",
       "      <td>21.350000</td>\n",
       "      <td>20.920000</td>\n",
       "      <td>22.210000</td>\n",
       "      <td>21.450000</td>\n",
       "      <td>21.700000</td>\n",
       "      <td>22.410000</td>\n",
       "      <td>21.400000</td>\n",
       "      <td>21.980000</td>\n",
       "      <td>21.250000</td>\n",
       "      <td>21.250000</td>\n",
       "      <td>22.180000</td>\n",
       "      <td>21.800000</td>\n",
       "      <td>21.590000</td>\n",
       "      <td>21.500000</td>\n",
       "      <td>22.490000</td>\n",
       "      <td>22.310000</td>\n",
       "      <td>21.110000</td>\n",
       "      <td>20.860000</td>\n",
       "      <td>22.720000</td>\n",
       "      <td>22.180000</td>\n",
       "      <td>21.820000</td>\n",
       "      <td>21.380000</td>\n",
       "      <td>21.120000</td>\n",
       "      <td>21.880000</td>\n",
       "      <td>21.380000</td>\n",
       "      <td>21.780000</td>\n",
       "      <td>21.210000</td>\n",
       "      <td>21.130000</td>\n",
       "      <td>21.730000</td>\n",
       "      <td>21.620000</td>\n",
       "      <td>21.050000</td>\n",
       "      <td>21.570000</td>\n",
       "      <td>21.080000</td>\n",
       "      <td>20.680000</td>\n",
       "      <td>20.630000</td>\n",
       "      <td>20.680000</td>\n",
       "      <td>21.350000</td>\n",
       "      <td>21.470000</td>\n",
       "      <td>21.040000</td>\n",
       "      <td>20.130000</td>\n",
       "      <td>19.650000</td>\n",
       "      <td>20.380000</td>\n",
       "      <td>20.440000</td>\n",
       "      <td>20.960000</td>\n",
       "      <td>20.800000</td>\n",
       "      <td>21.210000</td>\n",
       "      <td>20.96000</td>\n",
       "      <td>19.180000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Pixel 1       Pixel 2       Pixel 3       Pixel 4       Pixel 5  \\\n",
       "count  11050.000000  11050.000000  11050.000000  11050.000000  11050.000000   \n",
       "mean      16.874330     16.921444     16.976503     16.971698     17.174776   \n",
       "std        1.033223      1.023157      1.045179      1.050542      1.071565   \n",
       "min       13.400000     13.490000     13.380000     13.420000     13.120000   \n",
       "25%       16.360000     16.400000     16.420000     16.460000     16.730000   \n",
       "50%       16.860000     16.900000     16.950000     16.970000     17.180000   \n",
       "75%       17.280000     17.340000     17.390000     17.420000     17.600000   \n",
       "max       21.140000     21.590000     21.750000     21.970000     22.510000   \n",
       "\n",
       "            Pixel 6       Pixel 7       Pixel 8       Pixel 9      Pixel 10  \\\n",
       "count  11050.000000  11050.000000  11050.000000  11050.000000  11050.000000   \n",
       "mean      17.478671     17.050977     16.974066     16.724832     16.744799   \n",
       "std        1.231595      1.346729      1.261470      1.282029      1.163880   \n",
       "min       13.090000     12.810000     12.220000     13.260000     13.230000   \n",
       "25%       17.020000     16.410000     16.510000     16.020000     16.100000   \n",
       "50%       17.460000     16.920000     17.040000     16.480000     16.590000   \n",
       "75%       17.920000     17.430000     17.450000     17.070000     17.230000   \n",
       "max       22.640000     22.390000     21.640000     21.670000     21.210000   \n",
       "\n",
       "           Pixel 11      Pixel 12      Pixel 13      Pixel 14      Pixel 15  \\\n",
       "count  11050.000000  11050.000000  11050.000000  11050.000000  11050.000000   \n",
       "mean      17.087674     17.029565     17.267544     17.237629     17.058465   \n",
       "std        1.058479      0.949195      0.984129      1.082941      1.181676   \n",
       "min       13.140000     13.320000     13.760000     13.680000     13.210000   \n",
       "25%       16.540000     16.520000     16.750000     16.650000     16.410000   \n",
       "50%       17.060000     17.010000     17.250000     17.120000     16.870000   \n",
       "75%       17.590000     17.500000     17.750000     17.750000     17.480000   \n",
       "max       21.340000     21.430000     21.520000     21.950000     22.000000   \n",
       "\n",
       "           Pixel 16      Pixel 17      Pixel 18      Pixel 19      Pixel 20  \\\n",
       "count  11050.000000  11050.000000  11050.000000  11050.000000  11050.000000   \n",
       "mean      17.078401     16.856981     16.809812     16.988185     16.950543   \n",
       "std        1.155345      1.203697      0.998438      0.960331      0.875535   \n",
       "min       12.520000     12.730000     13.120000     13.230000     13.100000   \n",
       "25%       16.640000     16.262500     16.230000     16.490000     16.450000   \n",
       "50%       17.150000     16.770000     16.730000     17.070000     16.980000   \n",
       "75%       17.610000     17.317500     17.340000     17.510000     17.370000   \n",
       "max       21.800000     21.350000     20.920000     22.210000     21.450000   \n",
       "\n",
       "           Pixel 21      Pixel 22      Pixel 23      Pixel 24      Pixel 25  \\\n",
       "count  11050.000000  11050.000000  11050.000000  11050.000000  11050.000000   \n",
       "mean      17.074374     17.511714     16.918392     17.176538     16.783467   \n",
       "std        0.949656      0.961478      1.012309      1.034940      1.065849   \n",
       "min       13.350000     13.910000     13.160000     13.390000     13.320000   \n",
       "25%       16.590000     17.000000     16.370000     16.690000     16.200000   \n",
       "50%       17.110000     17.500000     16.820000     17.220000     16.720000   \n",
       "75%       17.540000     18.020000     17.390000     17.680000     17.180000   \n",
       "max       21.700000     22.410000     21.400000     21.980000     21.250000   \n",
       "\n",
       "           Pixel 26      Pixel 27      Pixel 28      Pixel 29      Pixel 30  \\\n",
       "count  11050.000000  11050.000000  11050.000000  11050.000000  11050.000000   \n",
       "mean      16.717047     16.871586     16.997138     17.056458     17.038457   \n",
       "std        0.949678      0.926547      0.931826      0.920005      0.893848   \n",
       "min       12.950000     13.130000     13.240000     13.330000     13.300000   \n",
       "25%       16.180000     16.350000     16.480000     16.540000     16.550000   \n",
       "50%       16.730000     16.920000     17.030000     17.070000     17.020000   \n",
       "75%       17.180000     17.310000     17.420000     17.500000     17.510000   \n",
       "max       21.250000     22.180000     21.800000     21.590000     21.500000   \n",
       "\n",
       "           Pixel 31      Pixel 32      Pixel 33      Pixel 34      Pixel 35  \\\n",
       "count  11050.000000  11050.000000  11050.000000  11050.000000  11050.000000   \n",
       "mean      17.265719     17.017191     16.503244     16.684624     16.907675   \n",
       "std        1.070146      1.059314      0.996061      0.905424      0.986412   \n",
       "min       13.730000     13.080000     13.130000     13.200000     13.020000   \n",
       "25%       16.750000     16.510000     15.950000     16.170000     16.390000   \n",
       "50%       17.240000     17.060000     16.510000     16.680000     16.960000   \n",
       "75%       17.770000     17.510000     16.940000     17.110000     17.360000   \n",
       "max       22.490000     22.310000     21.110000     20.860000     22.720000   \n",
       "\n",
       "           Pixel 36      Pixel 37      Pixel 38      Pixel 39      Pixel 40  \\\n",
       "count  11050.000000  11050.000000  11050.000000  11050.000000  11050.000000   \n",
       "mean      16.880801     17.106652     17.080072     16.799324     16.953208   \n",
       "std        0.913349      0.909681      0.894319      1.022788      1.036986   \n",
       "min       13.290000     13.500000     13.450000     13.110000     13.360000   \n",
       "25%       16.370000     16.600000     16.580000     16.220000     16.400000   \n",
       "50%       16.860000     17.100000     17.040000     16.680000     16.930000   \n",
       "75%       17.270000     17.520000     17.560000     17.240000     17.430000   \n",
       "max       22.180000     21.820000     21.380000     21.120000     21.880000   \n",
       "\n",
       "           Pixel 41      Pixel 42      Pixel 43      Pixel 44      Pixel 45  \\\n",
       "count  11050.000000  11050.000000  11050.000000  11050.000000  11050.000000   \n",
       "mean      16.664822     16.781742     16.718013     16.769567     16.891094   \n",
       "std        1.031338      0.998744      0.927739      0.891337      0.904230   \n",
       "min       13.010000     13.270000     13.080000     12.970000     13.160000   \n",
       "25%       16.070000     16.230000     16.220000     16.270000     16.380000   \n",
       "50%       16.660000     16.790000     16.730000     16.770000     16.870000   \n",
       "75%       17.090000     17.280000     17.150000     17.210000     17.370000   \n",
       "max       21.380000     21.780000     21.210000     21.130000     21.730000   \n",
       "\n",
       "           Pixel 46      Pixel 47      Pixel 48      Pixel 49      Pixel 50  \\\n",
       "count  11050.000000  11050.000000  11050.000000  11050.000000  11050.000000   \n",
       "mean      17.410037     17.104938     16.376304     15.974565     16.247051   \n",
       "std        1.012676      1.089616      1.015642      1.132799      1.035803   \n",
       "min       13.850000     13.610000     12.240000     12.460000     12.760000   \n",
       "25%       16.850000     16.460000     15.810000     15.260000     15.640000   \n",
       "50%       17.380000     16.960000     16.300000     15.890000     16.170000   \n",
       "75%       17.960000     17.580000     16.840000     16.400000     16.707500   \n",
       "max       21.620000     21.050000     21.570000     21.080000     20.680000   \n",
       "\n",
       "           Pixel 51      Pixel 52      Pixel 53      Pixel 54      Pixel 55  \\\n",
       "count  11050.000000  11050.000000  11050.000000  11050.000000  11050.000000   \n",
       "mean      16.725203     16.317422     16.991969     16.814239     16.521136   \n",
       "std        0.969334      0.967350      0.969329      1.090223      1.175530   \n",
       "min       13.130000     12.570000     13.350000     13.100000     12.760000   \n",
       "25%       16.170000     15.800000     16.432500     16.160000     15.820000   \n",
       "50%       16.680000     16.210000     16.890000     16.600000     16.290000   \n",
       "75%       17.150000     16.700000     17.410000     17.240000     16.930000   \n",
       "max       20.630000     20.680000     21.350000     21.470000     21.040000   \n",
       "\n",
       "           Pixel 56      Pixel 57      Pixel 58      Pixel 59      Pixel 60  \\\n",
       "count  11050.000000  11050.000000  11050.000000  11050.000000  11050.000000   \n",
       "mean      15.963922     15.769124     15.784101     15.815153     16.077814   \n",
       "std        0.923270      1.129771      0.953245      0.974218      1.019293   \n",
       "min       12.160000     12.100000     12.290000     12.260000     12.170000   \n",
       "25%       15.430000     15.040000     15.240000     15.310000     15.540000   \n",
       "50%       15.930000     15.690000     15.750000     15.750000     16.020000   \n",
       "75%       16.450000     16.460000     16.270000     16.150000     16.450000   \n",
       "max       20.130000     19.650000     20.380000     20.440000     20.960000   \n",
       "\n",
       "           Pixel 61      Pixel 62     Pixel 63      Pixel 64       session  \\\n",
       "count  11050.000000  11050.000000  11050.00000  11050.000000  11050.000000   \n",
       "mean      15.937851     16.414398     16.10871     15.265738     37.976561   \n",
       "std        0.961399      0.965692      1.01426      0.816669     22.348382   \n",
       "min       12.170000     12.860000     12.54000     12.020000      0.000000   \n",
       "25%       15.440000     15.880000     15.55000     14.790000     19.000000   \n",
       "50%       15.870000     16.300000     15.98000     15.270000     38.000000   \n",
       "75%       16.300000     16.790000     16.50000     15.770000     58.000000   \n",
       "max       20.800000     21.210000     20.96000     19.180000     76.000000   \n",
       "\n",
       "           people_#  \n",
       "count  11050.000000  \n",
       "mean       1.949231  \n",
       "std        0.975142  \n",
       "min        0.000000  \n",
       "25%        2.000000  \n",
       "50%        2.000000  \n",
       "75%        3.000000  \n",
       "max        3.000000  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.describe() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47f37eb-4ef6-4564-a096-f39ff199ba03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data_localization(df):\n",
    "\n",
    "    pixel = df.columns[0:64]\n",
    "    sessions = np.unique(df[\"session\"])\n",
    "    # Assuming 'df' is a pandas DataFrame\n",
    "    sessions_unique = df['session'].unique()\n",
    "    num_sessions = len(sessions_unique)\n",
    "\n",
    "    # Preallocate a list of NumPy arrays with known dimensions\n",
    "    session_arrays = [None] * num_sessions\n",
    "    \n",
    "    for i, session_label in enumerate(sessions_unique):\n",
    "        df_temp = df[df['session'] == session_label]\n",
    "        # Preallocate object array for tuples (image, empty array) for each image\n",
    "        pics_tuples = np.empty((len(df_temp),), dtype=object)\n",
    "\n",
    "        for idx, row in df_temp.iterrows():\n",
    "            # Extract pixel values and check if it can be reshaped to 8x8\n",
    "            pic = pd.to_numeric(row[pixel], errors='coerce').values\n",
    "            min_value = pic.min()\n",
    "            if pic.size == 64:\n",
    "                pic_reshaped = pic.reshape((8, 8))\n",
    "                empty_8x8 = np.zeros((8, 8))  # Create an empty 8x8 array\n",
    "\n",
    "                targets = ast.literal_eval(row[\"target_coordinates\"])\n",
    "\n",
    "                for p in targets:\n",
    "\n",
    "                    #a = p[0]\n",
    "                    #b = p[1]\n",
    "                    a = p[1]\n",
    "                    b = p[0]\n",
    "\n",
    "                    empty_8x8[a][b] = 1\n",
    "                pic_reshaped = pic_reshaped - min_value\n",
    "                pics_tuples[idx - df_temp.index[0]] = (pic_reshaped.reshape(1, 8, 8), empty_8x8)\n",
    "            else:\n",
    "                raise ValueError(\"The image does not have exactly 64 pixels to reshape into 8x8\")\n",
    "\n",
    "        session_arrays[i] = pics_tuples \n",
    "        \n",
    "    return session_arrays\n",
    "\n",
    "\n",
    "def prepare_data_localization(df, sequence=True, seq_len=8):\n",
    "    data = format_data_localization(df)\n",
    "    data_collection = []\n",
    "\n",
    "    if sequence:\n",
    "        for seq in data:\n",
    "            num_frames = len(seq)\n",
    "            for i in range(num_frames - seq_len + 1):\n",
    "                frames = np.concatenate([seq[j][0] for j in range(i, i + seq_len)])\n",
    "                detection_map = seq[i + seq_len - 1][1]\n",
    "                frames_expanded = np.expand_dims(frames, axis=1)\n",
    "                data_collection.append((frames_expanded, detection_map))\n",
    "    else:\n",
    "        for seq in data:\n",
    "            data_collection.extend(seq)\n",
    "    \n",
    "    return data_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "444e639b-6025-46f5-b9e3-cd9e6e9c0afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data_localization(df):\n",
    "\n",
    "    pixel = df.columns[0:64]\n",
    "    sessions = np.unique(df[\"session\"])\n",
    "    # Assuming 'df' is a pandas DataFrame\n",
    "    sessions_unique = df['session'].unique()\n",
    "    num_sessions = len(sessions_unique)\n",
    "\n",
    "    # Preallocate a list of NumPy arrays with known dimensions\n",
    "    session_arrays = [None] * num_sessions\n",
    "    \n",
    "    for i, session_label in enumerate(sessions_unique):\n",
    "        df_temp = df[df['session'] == session_label]\n",
    "        # Preallocate object array for tuples (image, empty array) for each image\n",
    "        pics_tuples = np.empty((len(df_temp),), dtype=object)\n",
    "\n",
    "        for idx, row in df_temp.iterrows():\n",
    "            # Extract pixel values and check if it can be reshaped to 8x8\n",
    "            pic = pd.to_numeric(row[pixel], errors='coerce').values\n",
    "            min_value = pic.min()\n",
    "            if pic.size == 64:\n",
    "                pic_reshaped = pic.reshape((8, 8))\n",
    "                empty_8x8 = np.zeros((8, 8))  # Create an empty 8x8 array\n",
    "\n",
    "                targets = ast.literal_eval(row[\"target_coordinates\"])\n",
    "\n",
    "                for p in targets:\n",
    "\n",
    "                    #a = p[0]\n",
    "                    #b = p[1]\n",
    "                    a = p[1]\n",
    "                    b = p[0]\n",
    "\n",
    "                    empty_8x8[a][b] = 1\n",
    "                pic_reshaped = pic_reshaped - min_value\n",
    "                pics_tuples[idx - df_temp.index[0]] = (pic_reshaped.reshape(1, 8, 8), empty_8x8)\n",
    "            else:\n",
    "                raise ValueError(\"The image does not have exactly 64 pixels to reshape into 8x8\")\n",
    "\n",
    "        session_arrays[i] = pics_tuples \n",
    "        \n",
    "    return session_arrays\n",
    "\n",
    "\n",
    "def prepare_data_localization(df, sequence=True, seq_len=8):\n",
    "    data = format_data_localization(df)\n",
    "    data_collection = []\n",
    "\n",
    "    if sequence:\n",
    "        for seq in data:\n",
    "            num_frames = len(seq)\n",
    "            for i in range(num_frames - seq_len + 1):\n",
    "                frames = np.concatenate([seq[j][0] for j in range(i, i + seq_len)])\n",
    "                detection_map = seq[i + seq_len - 1][1]\n",
    "                frames_expanded = np.expand_dims(frames, axis=1)\n",
    "                data_collection.append((frames_expanded, detection_map))\n",
    "    else:\n",
    "        for seq in data:\n",
    "            data_collection.extend(seq)\n",
    "    \n",
    "    return data_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b3b61692-da52-437d-8c4c-f6480b3e0144",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the sequence length to the optimal 8 subsequent frames in order to capture temporal changes leading to the current frame\n",
    "seq_len = 8\n",
    "collection_train_seq = prepare_data_localization(df_train, sequence=True, seq_len=seq_len)\n",
    "collection_test_seq = prepare_data_localization(df_test, sequence=True, seq_len=seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a34e8805-eb9c-41f4-896f-f8b081543043",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing Torch datasets\n",
    "class SimpleTorchDataset(Dataset):\n",
    "    def __init__(self, data_collection):\n",
    "        self.data_collection = data_collection\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_collection)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        frames, label = self.data_collection[idx]\n",
    "        \n",
    "        # Ensure frames and label are numpy arrays of appropriate types\n",
    "        frames = np.array(frames, dtype=np.float32)\n",
    "        label = np.array(label, dtype=np.float32)\n",
    "        \n",
    "        # Convert to PyTorch tensors\n",
    "        frames = torch.tensor(frames, dtype=torch.float32)\n",
    "        label = torch.tensor(label, dtype=torch.float32)\n",
    "        \n",
    "        return frames, label\n",
    "\n",
    "\n",
    "def create_datasets(data_collection_train, data_collection_test):\n",
    "    # Creating PyTorch datasets\n",
    "    train_set = SimpleTorchDataset(data_collection_train)\n",
    "    test_set = SimpleTorchDataset(data_collection_test)\n",
    "    \n",
    "    return train_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "68746163-2e1e-4f4d-871b-99d62aec63c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = create_datasets(collection_train_seq, collection_test_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f9e00c80-51f4-488b-b660-4d02ec5d0900",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_epochs = 500\n",
    "interpolate = False\n",
    "dim_dim = [8, 16]\n",
    "std = 0.008\n",
    "patience = 20\n",
    "learning_rate = 0.0007\n",
    "date = 120624\n",
    "code_word = f\"clstm_unet_8_16_len{seq_len}_shrink_std{std}_lr{learning_rate}_pt{patience}_{date}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "97932ef8-8718-4dcf-9095-f92286ef7ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = dataset_to_loader(train_set, batch_size, balancing = False, shuffle = True)\n",
    "test_loader = dataset_to_loader(test_set, batch_size, balancing = False, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a3506b1d-ac10-4cea-b049-4fa96f922943",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convolutional LSTM version of UNET with temporal and spatial awareness \n",
    "#The implementation of the ConvLSTM Cell is taken from from https://github.com/ndrplz/ConvLSTM_pytorch/blob/master/convlstm.py\n",
    "class ConvLSTMCell(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, bias):\n",
    "        \"\"\"\n",
    "        Initialize ConvLSTM cell.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_dim: int\n",
    "            Number of channels of input tensor.\n",
    "        hidden_dim: int\n",
    "            Number of channels of hidden state.\n",
    "        kernel_size: (int, int)\n",
    "            Size of the convolutional kernel.\n",
    "        bias: bool\n",
    "            Whether or not to add the bias.\n",
    "        \"\"\"\n",
    "\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = kernel_size[0] // 2, kernel_size[1] // 2\n",
    "        self.bias = bias\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels=self.input_dim + self.hidden_dim,\n",
    "                              out_channels=4 * self.hidden_dim,\n",
    "                              kernel_size=self.kernel_size,\n",
    "                              padding=self.padding,\n",
    "                              bias=self.bias)\n",
    "\n",
    "    def forward(self, input_tensor, cur_state):\n",
    "        h_cur, c_cur = cur_state\n",
    "\n",
    "        combined = torch.cat([input_tensor, h_cur], dim=1)  # concatenate along channel axis\n",
    "\n",
    "        combined_conv = self.conv(combined)\n",
    "        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=1)\n",
    "        i = torch.sigmoid(cc_i)\n",
    "        f = torch.sigmoid(cc_f)\n",
    "        o = torch.sigmoid(cc_o)\n",
    "        g = torch.tanh(cc_g)\n",
    "\n",
    "        c_next = f * c_cur + i * g\n",
    "        h_next = o * torch.tanh(c_next)\n",
    "\n",
    "        return h_next, c_next\n",
    "\n",
    "    def init_hidden(self, batch_size, image_size):\n",
    "        height, width = image_size\n",
    "        return (torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device),\n",
    "                torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device))\n",
    "\n",
    "class ConvLSTM(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Parameters:\n",
    "        input_dim: Number of channels in input\n",
    "        hidden_dim: Number of hidden channels\n",
    "        kernel_size: Size of kernel in convolutions\n",
    "        num_layers: Number of LSTM layers stacked on each other\n",
    "        batch_first: Whether or not dimension 0 is the batch or not\n",
    "        bias: Bias or no bias in Convolution\n",
    "        return_all_layers: Return the list of computations for all layers\n",
    "        Note: Will do same padding.\n",
    "\n",
    "    Input:\n",
    "        A tensor of size B, T, C, H, W or T, B, C, H, W\n",
    "    Output:\n",
    "        A tuple of two lists of length num_layers (or length 1 if return_all_layers is False).\n",
    "            0 - layer_output_list is the list of lists of length T of each output\n",
    "            1 - last_state_list is the list of last states\n",
    "                    each element of the list is a tuple (h, c) for hidden state and memory\n",
    "    Example:\n",
    "        >> x = torch.rand((32, 10, 64, 128, 128))\n",
    "        >> convlstm = ConvLSTM(64, 16, 3, 1, True, True, False)\n",
    "        >> _, last_states = convlstm(x)\n",
    "        >> h = last_states[0][0]  # 0 for layer index, 0 for h index\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, num_layers,\n",
    "                 batch_first=True, bias=True, return_all_layers=False):\n",
    "        super(ConvLSTM, self).__init__()\n",
    "\n",
    "        self._check_kernel_size_consistency(kernel_size)\n",
    "\n",
    "        # Make sure that both `kernel_size` and `hidden_dim` are lists having len == num_layers\n",
    "        kernel_size = self._extend_for_multilayer(kernel_size, num_layers)\n",
    "        hidden_dim = self._extend_for_multilayer(hidden_dim, num_layers)\n",
    "        if not len(kernel_size) == len(hidden_dim) == num_layers:\n",
    "            raise ValueError('Inconsistent list length.')\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_first = batch_first\n",
    "        self.bias = bias\n",
    "        self.return_all_layers = return_all_layers\n",
    "\n",
    "        cell_list = []\n",
    "        for i in range(0, self.num_layers):\n",
    "            cur_input_dim = self.input_dim if i == 0 else self.hidden_dim[i - 1]\n",
    "\n",
    "            cell_list.append(ConvLSTMCell(input_dim=cur_input_dim,\n",
    "                                          hidden_dim=self.hidden_dim[i],\n",
    "                                          kernel_size=self.kernel_size[i],\n",
    "                                          bias=self.bias))\n",
    "\n",
    "        self.cell_list = nn.ModuleList(cell_list)\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_tensor: \n",
    "            5-D Tensor either of shape (t, b, c, h, w) or (b, t, c, h, w)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        last_state_list, layer_output\n",
    "        \"\"\"\n",
    "        if not self.batch_first:\n",
    "            # (t, b, c, h, w) -> (b, t, c, h, w)\n",
    "            input_tensor = input_tensor.permute(1, 0, 2, 3, 4)\n",
    "\n",
    "        b, _, _, h, w = input_tensor.size()\n",
    "\n",
    "        \n",
    "        # Since the init is done in forward. Can send image size here\n",
    "        hidden_state = self._init_hidden(batch_size=b,\n",
    "                                         image_size=(h, w))\n",
    "\n",
    "        layer_output_list = []\n",
    "        last_state_list = []\n",
    "\n",
    "        seq_len = input_tensor.size(1)\n",
    "        cur_layer_input = input_tensor\n",
    "\n",
    "        for layer_idx in range(self.num_layers):\n",
    "\n",
    "            h, c = hidden_state[layer_idx]\n",
    "            output_inner = []\n",
    "            for t in range(seq_len):\n",
    "                h, c = self.cell_list[layer_idx](input_tensor=cur_layer_input[:, t, :, :, :],\n",
    "                                                 cur_state=[h, c])\n",
    "                output_inner.append(h)\n",
    "\n",
    "            layer_output = torch.stack(output_inner, dim=1)\n",
    "            cur_layer_input = layer_output\n",
    "\n",
    "            layer_output_list.append(layer_output)\n",
    "            last_state_list.append([h, c])\n",
    "\n",
    "        if not self.return_all_layers:\n",
    "            layer_output_list = layer_output_list[-1:]\n",
    "            last_state_list = last_state_list[-1:]\n",
    "\n",
    "        return layer_output_list, last_state_list\n",
    "\n",
    "    def _init_hidden(self, batch_size, image_size):\n",
    "        init_states = []\n",
    "        for i in range(self.num_layers):\n",
    "            init_states.append(self.cell_list[i].init_hidden(batch_size, image_size))\n",
    "        return init_states\n",
    "\n",
    "    @staticmethod\n",
    "    def _check_kernel_size_consistency(kernel_size):\n",
    "        if not (isinstance(kernel_size, tuple) or\n",
    "                (isinstance(kernel_size, list) and all([isinstance(elem, tuple) for elem in kernel_size]))):\n",
    "            raise ValueError('`kernel_size` must be tuple or list of tuples')\n",
    "\n",
    "    @staticmethod\n",
    "    def _extend_for_multilayer(param, num_layers):\n",
    "        if not isinstance(param, list):\n",
    "            param = [param] * num_layers\n",
    "        return param\n",
    "\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "#Adaptation of DoubleCNN class:\n",
    "class DoubleConvLSTM(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, hidden_dim, kernel_size=(3,3), bias=True):\n",
    "        super(DoubleConvLSTM, self).__init__()\n",
    "        self.conv_lstm = ConvLSTM(input_dim=in_channels,\n",
    "                                  hidden_dim=[hidden_dim, hidden_dim],\n",
    "                                  kernel_size=kernel_size,\n",
    "                                  num_layers=2,\n",
    "                                  batch_first=True,\n",
    "                                  bias=bias,\n",
    "                                  return_all_layers=False)\n",
    "    def forward(self, x):\n",
    "        # Assuming x is of shape (batch, seq_len, channels, height, width)\n",
    "        #print(f'in double conv: {x.shape}')\n",
    "        layer_output_list, _ = self.conv_lstm(x)\n",
    "        # Take the output of the last layer\n",
    "        x = layer_output_list[-1]  # Shape: (batch, seq_len, channels, height, width)\n",
    "        # We take the output of the last timestep\n",
    "        #x = x[:, -1, :, :, :]  # Shape: (batch, channels, height, width)\n",
    "        #print(f'in double conv after first LSTM: {x.shape}')\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "#The LSTM where pooling is performed oly before the bottleneck\n",
    "class T_UNET_IR(nn.Module):\n",
    "    def __init__(\n",
    "            self, in_channels=1, out_channels=1, features=[16, 32], hidden_dims=[16, 32], incl_botneck = False\n",
    "    ):\n",
    "        super(T_UNET_IR, self).__init__()\n",
    "        self.ups = nn.ModuleList()\n",
    "        self.downs = nn.ModuleList()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.incl_botneck = incl_botneck\n",
    "\n",
    "        # Down part of UNET\n",
    "        for i, feature in enumerate(features):\n",
    "            self.downs.append(DoubleConvLSTM(in_channels, feature, hidden_dim=hidden_dims[i]))\n",
    "            in_channels = feature\n",
    "        \n",
    "        # Up part of UNET\n",
    "        for i, feature in enumerate(reversed(features)):\n",
    "            if i == 0:\n",
    "                self.ups.append(\n",
    "                    nn.ConvTranspose2d(\n",
    "                        feature*2, feature, kernel_size=2, stride=2,\n",
    "                    )\n",
    "                )\n",
    "                self.ups.append(DoubleConv(feature*2, feature))\n",
    "            else:\n",
    "                self.ups.append(DoubleConv(feature*3, feature))      \n",
    "        if self.incl_botneck:\n",
    "            self.bottleneck = DoubleConvLSTM(features[-1], features[-1]*2, hidden_dim=hidden_dims[-1]*2)\n",
    "        else:\n",
    "            self.bottleneck = DoubleConv(features[-1], features[-1]*2)\n",
    "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip_connections = []\n",
    "\n",
    "        for down in self.downs:\n",
    "            x = down(x)\n",
    "            skip_connections.append(x[:, -1, :, :, :])\n",
    "        if self.incl_botneck == False:\n",
    "            x = x[:, -1, :, :, :]\n",
    "        x = self.pool(x)\n",
    "        #print(f'before bottle neck: {x.shape}')\n",
    "        x = self.bottleneck(x)\n",
    "        if self.incl_botneck:\n",
    "            x = x[:, -1, :, :, :]\n",
    "        skip_connections = skip_connections[::-1]\n",
    "        #print(f'after bottle neck: {x.shape}')\n",
    "        x = self.ups[0](x)\n",
    "        skip_connection = skip_connections[0]\n",
    "        if x.shape != skip_connection.shape:\n",
    "            x = TF.resize(x, size=skip_connection.shape[2:])\n",
    "        concat_skip = torch.cat((skip_connection, x), dim=1)\n",
    "        x = self.ups[1](concat_skip)\n",
    "        start = 2\n",
    "\n",
    "        for idx in range(start, len(self.ups)):\n",
    "            skip_connection = skip_connections[idx//2]\n",
    "            concat_skip = torch.cat((skip_connection, x), dim=1)\n",
    "            x = self.ups[idx](concat_skip)\n",
    "            #print(f'upbranch: {x.shape}, skip connection{skip_connection.shape}')\n",
    "\n",
    "\n",
    "        return self.final_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "cb517d67-e5a3-4a10-bd10-82598ab569cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T_UNET_IR(\n",
       "  (ups): ModuleList(\n",
       "    (0): ConvTranspose2d(32, 16, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (1): DoubleConv(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (2): DoubleConv(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(24, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (downs): ModuleList(\n",
       "    (0): DoubleConvLSTM(\n",
       "      (conv_lstm): ConvLSTM(\n",
       "        (cell_list): ModuleList(\n",
       "          (0): ConvLSTMCell(\n",
       "            (conv): Conv2d(9, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (1): ConvLSTMCell(\n",
       "            (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): DoubleConvLSTM(\n",
       "      (conv_lstm): ConvLSTM(\n",
       "        (cell_list): ModuleList(\n",
       "          (0): ConvLSTMCell(\n",
       "            (conv): Conv2d(24, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (1): ConvLSTMCell(\n",
       "            (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (bottleneck): DoubleConv(\n",
       "    (conv): Sequential(\n",
       "      (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (final_conv): Conv2d(8, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = T_UNET_IR(features=dim_dim, hidden_dims=dim_dim, incl_botneck = False)\n",
    "# Move the model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5d0cbae1-4f1d-4be5-ade9-231f530e970f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Focal loss to account for class imbalance (head location vs pixel without head) withing the frame\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-BCE_loss)  # prevents nans when probability 0\n",
    "        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
    "        if self.reduction == 'mean':\n",
    "            return torch.mean(F_loss)\n",
    "        elif self.reduction == 'sum':\n",
    "            return torch.sum(F_loss)\n",
    "        else:\n",
    "            return F_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8252877-2355-40d1-b03d-755db608ee99",
   "metadata": {},
   "source": [
    "Specifying the loss function and optizer to use in the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9e77dd87-069e-426f-b2a8-9e0193f675c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = FocalLoss(alpha = 0.8, gamma = 4.0, reduction = 'sum').to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "60c861af-df6b-40fa-b2d8-4c0db7e33874",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data augmentations:\n",
    "def RandomHorizontalFlip(sample, prob, dim = 2):\n",
    "    number = random.random()\n",
    "    state = False\n",
    "    if number <= prob:\n",
    "        state = True\n",
    "        return torch.flip(sample, (dim,)), state\n",
    "    else:\n",
    "        return sample, state\n",
    "\n",
    "def RandomVerticalFlip(sample, prob, dim = 3):\n",
    "    number = random.random()\n",
    "    state = False\n",
    "    if number <= prob:\n",
    "        state = True\n",
    "        return torch.flip(sample, (dim,)), state\n",
    "    else:\n",
    "        return sample, state\n",
    "\n",
    "def AddValue(frame):\n",
    "    tensor_ones = torch.ones(frame.shape)\n",
    "    integer_ = random.randint(0, 2)\n",
    "    float_ = random.random()\n",
    "    return frame + tensor_ones.to(device)*(integer_ + float_)\n",
    "\n",
    "class AddGaussianNoise:\n",
    "    def __init__(self, mean=0., std=0.5):\n",
    "        self.std = std\n",
    "        self.mean = mean\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        return tensor + torch.randn(tensor.size()).to(device) * self.std + self.mean\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)\n",
    "\n",
    "\n",
    "\n",
    "def apply_augmentation(inputs, add_noise = True, dim = (2, 3), add_val = False, std = 0.05):\n",
    "    noise = AddGaussianNoise(mean=0., std=std)\n",
    "    inputs, state_hor = RandomHorizontalFlip(inputs, 0.5, dim[0])\n",
    "    inputs, state_ver = RandomVerticalFlip(inputs, 0.5, dim[1])\n",
    "    if add_noise:\n",
    "        inputs = noise(inputs)\n",
    "    if add_val:\n",
    "        inputs = AddValue(inputs)\n",
    "    return inputs, (state_hor, state_ver)\n",
    "\n",
    "#we need to flip labels for localization task only\n",
    "def flip_labels(labels, states):\n",
    "    state_hor, state_ver = states\n",
    "    if state_hor:\n",
    "        labels = torch.flip(labels, (1,))\n",
    "    if state_ver:\n",
    "        labels = torch.flip(labels, (2,))\n",
    "    return labels\n",
    "\n",
    "\n",
    "\n",
    "def network_training_detect(net, train_loader, criterion, optimizer, num_epochs, code_word, path_to_save, interpolate = True, sequence = False, add_noise = False, patience = 20, std = 0.02, factor = 0.3):\n",
    "    patience_counter = 0  # Early stopping counter\n",
    "    epoch_recall_max = 0\n",
    "    start_time = time.time()\n",
    "    learning_rate_list = []\n",
    "    loss_batch = []\n",
    "    loss_epoch = []\n",
    "    if sequence:\n",
    "        dim_aug = (3, 4)\n",
    "    else:\n",
    "        dim_aug = (2, 3)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=patience//2, factor = factor)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f'current learning rate:{current_lr}')\n",
    "    for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        running_loss_epoch = 0.0\n",
    "        running_corrects = 0\n",
    "        running_true_positives = 0\n",
    "        running_false_negatives = 0\n",
    "        running_true_negatives = 0\n",
    "        running_false_positives = 0\n",
    "        total_pixels = 0  # total number of pixels processed\n",
    "        net.train()\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        learning_rate_list.append(current_lr)\n",
    "        if patience_counter >= patience//2:\n",
    "            print(f'current learning rate:{current_lr}')  \n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            inputs, labels = data\n",
    "            inputs, states = apply_augmentation(inputs.to(device), add_noise = add_noise, dim = dim_aug, std = std)\n",
    "            if interpolate == True:\n",
    "                inputs = F.interpolate(inputs, size=(16, 16), mode='bilinear')\n",
    "            inputs = inputs.to(device)\n",
    "            #to flip the target mask as well as input:\n",
    "            labels = flip_labels(labels, states)\n",
    "            #print(f'labels after flipping {labels[0]}')\n",
    "            labels = labels.float().to(device)\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.set_grad_enabled(True):\n",
    "                outputs = net(inputs)\n",
    "                loss = criterion(outputs.squeeze(1), labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # calculate running loss & accuracy\n",
    "            running_loss += loss.item()\n",
    "            running_loss_epoch += loss.item()\n",
    "\n",
    "            preds = (outputs > 0.5).squeeze(1)  # threshold predictions\n",
    "            running_corrects += torch.sum(preds == labels.bool())\n",
    "            running_true_positives += torch.sum((preds == 1) & (labels.bool() == 1))\n",
    "            running_false_negatives += torch.sum((preds == 0) & (labels.bool() == 1))\n",
    "            running_true_negatives += torch.sum((preds == 0) & (labels.bool() == 0))\n",
    "            running_false_positives += torch.sum((preds == 1) & (labels.bool() == 0))\n",
    "            total_pixels += labels.numel()\n",
    "\n",
    "            if i % 200 == 199:  # print every 200 mini-batches\n",
    "                loss_batch.append(running_loss / 200)\n",
    "                running_loss = 0.0\n",
    "        epoch_recall = running_true_positives / (running_true_positives + running_false_negatives)\n",
    "        epoch_acc = (running_true_positives + running_true_negatives) / (running_true_positives + running_true_negatives + running_false_positives + running_false_negatives)\n",
    "        scheduler.step(epoch_recall)\n",
    "        loss_epoch.append(running_loss_epoch / len(train_loader.dataset))\n",
    "        print(f'EPOCH {epoch}: Accuracy: {epoch_acc:.4f}')\n",
    "        print(f'Recall: {epoch_recall} ')\n",
    "        print(f'tp: {running_true_positives}, fn: {running_false_negatives}, fp: {running_false_positives}, tn: {running_true_negatives}')\n",
    "        print(f'Epoch loss: {running_loss_epoch / len(train_loader.dataset)}')\n",
    "        running_loss_epoch = 0.0\n",
    "        if epoch_recall > epoch_recall_max:\n",
    "            epoch_recall_max = epoch_recall\n",
    "            torch.save(net.state_dict(), f'{path_to_save}/model_{code_word}.pt')\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f'Early stopping triggered at epoch {epoch}.')\n",
    "            break  # Stop training if patience limit is reached\n",
    "\n",
    "    training_time = time.time() - start_time\n",
    "    print(f'Time of training: {training_time:.2f} seconds')\n",
    "    print(f'Max recall: {epoch_recall_max:.4f}')\n",
    "    return net, loss_batch, loss_epoch, learning_rate_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e530baa6-00c6-452a-89b3-46cf2a5934f4",
   "metadata": {},
   "source": [
    "Next we run training of our model. On each epoch the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "dc39a626-08cd-40d5-b163-195218a3b67e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current learning rate:0.0007\n",
      "EPOCH 0: Accuracy: 0.9723\n",
      "Recall: 0.2034350037574768 \n",
      "tp: 18395, fn: 72027, fp: 419, tn: 2522407\n",
      "Epoch loss: 0.36950541651431107\n",
      "EPOCH 1: Accuracy: 0.9918\n",
      "Recall: 0.7722235918045044 \n",
      "tp: 69826, fn: 20596, fp: 772, tn: 2522054\n",
      "Epoch loss: 0.04835031996408133\n",
      "EPOCH 2: Accuracy: 0.9936\n",
      "Recall: 0.820840060710907 \n",
      "tp: 74222, fn: 16200, fp: 640, tn: 2522186\n",
      "Epoch loss: 0.03839988737924719\n",
      "EPOCH 3: Accuracy: 0.9943\n",
      "Recall: 0.8425272703170776 \n",
      "tp: 76183, fn: 14239, fp: 582, tn: 2522244\n",
      "Epoch loss: 0.03445296826389936\n",
      "EPOCH 4: Accuracy: 0.9948\n",
      "Recall: 0.8554112911224365 \n",
      "tp: 77348, fn: 13074, fp: 571, tn: 2522255\n",
      "Epoch loss: 0.03198243164074925\n",
      "EPOCH 5: Accuracy: 0.9953\n",
      "Recall: 0.8689699172973633 \n",
      "tp: 78574, fn: 11848, fp: 468, tn: 2522358\n",
      "Epoch loss: 0.029196791298290406\n",
      "EPOCH 6: Accuracy: 0.9955\n",
      "Recall: 0.8759593963623047 \n",
      "tp: 79206, fn: 11216, fp: 477, tn: 2522349\n",
      "Epoch loss: 0.02792395760967954\n",
      "EPOCH 7: Accuracy: 0.9957\n",
      "Recall: 0.8798522353172302 \n",
      "tp: 79558, fn: 10864, fp: 475, tn: 2522351\n",
      "Epoch loss: 0.027005128847102294\n",
      "EPOCH 8: Accuracy: 0.9957\n",
      "Recall: 0.8815996050834656 \n",
      "tp: 79716, fn: 10706, fp: 482, tn: 2522344\n",
      "Epoch loss: 0.026705441216946957\n",
      "EPOCH 9: Accuracy: 0.9959\n",
      "Recall: 0.8856804966926575 \n",
      "tp: 80085, fn: 10337, fp: 461, tn: 2522365\n",
      "Epoch loss: 0.025483839780232376\n",
      "EPOCH 10: Accuracy: 0.9960\n",
      "Recall: 0.8894074559211731 \n",
      "tp: 80422, fn: 10000, fp: 500, tn: 2522326\n",
      "Epoch loss: 0.025241028332096002\n",
      "EPOCH 11: Accuracy: 0.9963\n",
      "Recall: 0.8963637351989746 \n",
      "tp: 81051, fn: 9371, fp: 428, tn: 2522398\n",
      "Epoch loss: 0.02344759399083014\n",
      "EPOCH 12: Accuracy: 0.9961\n",
      "Recall: 0.8929021954536438 \n",
      "tp: 80738, fn: 9684, fp: 457, tn: 2522369\n",
      "Epoch loss: 0.024095101657157128\n",
      "EPOCH 13: Accuracy: 0.9963\n",
      "Recall: 0.8975249528884888 \n",
      "tp: 81156, fn: 9266, fp: 433, tn: 2522393\n",
      "Epoch loss: 0.023443003094588702\n",
      "EPOCH 14: Accuracy: 0.9965\n",
      "Recall: 0.9023799300193787 \n",
      "tp: 81595, fn: 8827, fp: 413, tn: 2522413\n",
      "Epoch loss: 0.022480409820841443\n",
      "EPOCH 15: Accuracy: 0.9966\n",
      "Recall: 0.905553936958313 \n",
      "tp: 81882, fn: 8540, fp: 381, tn: 2522445\n",
      "Epoch loss: 0.021671752374601727\n",
      "EPOCH 16: Accuracy: 0.9966\n",
      "Recall: 0.9055871367454529 \n",
      "tp: 81885, fn: 8537, fp: 406, tn: 2522420\n",
      "Epoch loss: 0.021657778389553385\n",
      "EPOCH 17: Accuracy: 0.9966\n",
      "Recall: 0.906471848487854 \n",
      "tp: 81965, fn: 8457, fp: 391, tn: 2522435\n",
      "Epoch loss: 0.02164007652460431\n",
      "EPOCH 18: Accuracy: 0.9967\n",
      "Recall: 0.9094025492668152 \n",
      "tp: 82230, fn: 8192, fp: 403, tn: 2522423\n",
      "Epoch loss: 0.02088156559571717\n",
      "EPOCH 19: Accuracy: 0.9968\n",
      "Recall: 0.9110614657402039 \n",
      "tp: 82380, fn: 8042, fp: 401, tn: 2522425\n",
      "Epoch loss: 0.020483762332381313\n",
      "EPOCH 20: Accuracy: 0.9967\n",
      "Recall: 0.9101435542106628 \n",
      "tp: 82297, fn: 8125, fp: 424, tn: 2522402\n",
      "Epoch loss: 0.020481623776350464\n",
      "EPOCH 21: Accuracy: 0.9969\n",
      "Recall: 0.9139037132263184 \n",
      "tp: 82637, fn: 7785, fp: 383, tn: 2522443\n",
      "Epoch loss: 0.020004905326011645\n",
      "EPOCH 22: Accuracy: 0.9969\n",
      "Recall: 0.915695309638977 \n",
      "tp: 82799, fn: 7623, fp: 380, tn: 2522446\n",
      "Epoch loss: 0.01938409969242739\n",
      "EPOCH 23: Accuracy: 0.9970\n",
      "Recall: 0.9165136814117432 \n",
      "tp: 82873, fn: 7549, fp: 354, tn: 2522472\n",
      "Epoch loss: 0.01889658829366619\n",
      "EPOCH 24: Accuracy: 0.9971\n",
      "Recall: 0.9203290939331055 \n",
      "tp: 83218, fn: 7204, fp: 376, tn: 2522450\n",
      "Epoch loss: 0.0181810196859873\n",
      "EPOCH 25: Accuracy: 0.9971\n",
      "Recall: 0.9199309945106506 \n",
      "tp: 83182, fn: 7240, fp: 351, tn: 2522475\n",
      "Epoch loss: 0.018346761727010757\n",
      "EPOCH 26: Accuracy: 0.9972\n",
      "Recall: 0.9225851893424988 \n",
      "tp: 83422, fn: 7000, fp: 352, tn: 2522474\n",
      "Epoch loss: 0.017637329364376477\n",
      "EPOCH 27: Accuracy: 0.9971\n",
      "Recall: 0.9205945730209351 \n",
      "tp: 83242, fn: 7180, fp: 375, tn: 2522451\n",
      "Epoch loss: 0.01819537097637813\n",
      "EPOCH 28: Accuracy: 0.9973\n",
      "Recall: 0.9247196316719055 \n",
      "tp: 83615, fn: 6807, fp: 357, tn: 2522469\n",
      "Epoch loss: 0.017069224097406882\n",
      "EPOCH 29: Accuracy: 0.9972\n",
      "Recall: 0.922153890132904 \n",
      "tp: 83383, fn: 7039, fp: 363, tn: 2522463\n",
      "Epoch loss: 0.0178578598183933\n",
      "EPOCH 30: Accuracy: 0.9973\n",
      "Recall: 0.9245758652687073 \n",
      "tp: 83602, fn: 6820, fp: 346, tn: 2522480\n",
      "Epoch loss: 0.01719568891933941\n",
      "EPOCH 31: Accuracy: 0.9973\n",
      "Recall: 0.9255822896957397 \n",
      "tp: 83693, fn: 6729, fp: 351, tn: 2522475\n",
      "Epoch loss: 0.016713346689676743\n",
      "EPOCH 32: Accuracy: 0.9973\n",
      "Recall: 0.9268430471420288 \n",
      "tp: 83807, fn: 6615, fp: 338, tn: 2522488\n",
      "Epoch loss: 0.016553943940995956\n",
      "EPOCH 33: Accuracy: 0.9973\n",
      "Recall: 0.9255269765853882 \n",
      "tp: 83688, fn: 6734, fp: 369, tn: 2522457\n",
      "Epoch loss: 0.016805727995689784\n",
      "EPOCH 34: Accuracy: 0.9973\n",
      "Recall: 0.9264559745788574 \n",
      "tp: 83772, fn: 6650, fp: 377, tn: 2522449\n",
      "Epoch loss: 0.01689526604541043\n",
      "EPOCH 35: Accuracy: 0.9972\n",
      "Recall: 0.9242883324623108 \n",
      "tp: 83576, fn: 6846, fp: 387, tn: 2522439\n",
      "Epoch loss: 0.017275042118078395\n",
      "EPOCH 36: Accuracy: 0.9974\n",
      "Recall: 0.9289000630378723 \n",
      "tp: 83993, fn: 6429, fp: 311, tn: 2522515\n",
      "Epoch loss: 0.015824999182184336\n",
      "EPOCH 37: Accuracy: 0.9973\n",
      "Recall: 0.9273627996444702 \n",
      "tp: 83854, fn: 6568, fp: 368, tn: 2522458\n",
      "Epoch loss: 0.01648584623137915\n",
      "EPOCH 38: Accuracy: 0.9974\n",
      "Recall: 0.9294198155403137 \n",
      "tp: 84040, fn: 6382, fp: 335, tn: 2522491\n",
      "Epoch loss: 0.015769138602688793\n",
      "EPOCH 39: Accuracy: 0.9974\n",
      "Recall: 0.928136944770813 \n",
      "tp: 83924, fn: 6498, fp: 340, tn: 2522486\n",
      "Epoch loss: 0.016212547787678278\n",
      "EPOCH 40: Accuracy: 0.9975\n",
      "Recall: 0.9315211176872253 \n",
      "tp: 84230, fn: 6192, fp: 322, tn: 2522504\n",
      "Epoch loss: 0.015269531470980367\n",
      "EPOCH 41: Accuracy: 0.9975\n",
      "Recall: 0.9318860173225403 \n",
      "tp: 84263, fn: 6159, fp: 342, tn: 2522484\n",
      "Epoch loss: 0.0156515843904983\n",
      "EPOCH 42: Accuracy: 0.9975\n",
      "Recall: 0.931609570980072 \n",
      "tp: 84238, fn: 6184, fp: 318, tn: 2522508\n",
      "Epoch loss: 0.015172494498744233\n",
      "EPOCH 43: Accuracy: 0.9975\n",
      "Recall: 0.9326159358024597 \n",
      "tp: 84329, fn: 6093, fp: 319, tn: 2522507\n",
      "Epoch loss: 0.0151709405928675\n",
      "EPOCH 44: Accuracy: 0.9976\n",
      "Recall: 0.9327154755592346 \n",
      "tp: 84338, fn: 6084, fp: 315, tn: 2522511\n",
      "Epoch loss: 0.014795426397181486\n",
      "EPOCH 45: Accuracy: 0.9976\n",
      "Recall: 0.9328703284263611 \n",
      "tp: 84352, fn: 6070, fp: 326, tn: 2522500\n",
      "Epoch loss: 0.015118262184956066\n",
      "EPOCH 46: Accuracy: 0.9976\n",
      "Recall: 0.9328703284263611 \n",
      "tp: 84352, fn: 6070, fp: 330, tn: 2522496\n",
      "Epoch loss: 0.014860425585279457\n",
      "EPOCH 47: Accuracy: 0.9975\n",
      "Recall: 0.9327486753463745 \n",
      "tp: 84341, fn: 6081, fp: 349, tn: 2522477\n",
      "Epoch loss: 0.014971087244047337\n",
      "EPOCH 48: Accuracy: 0.9977\n",
      "Recall: 0.9368516802787781 \n",
      "tp: 84712, fn: 5710, fp: 293, tn: 2522533\n",
      "Epoch loss: 0.013948101281563584\n",
      "EPOCH 49: Accuracy: 0.9976\n",
      "Recall: 0.935447096824646 \n",
      "tp: 84585, fn: 5837, fp: 309, tn: 2522517\n",
      "Epoch loss: 0.01440945757862449\n",
      "EPOCH 50: Accuracy: 0.9977\n",
      "Recall: 0.935657262802124 \n",
      "tp: 84604, fn: 5818, fp: 287, tn: 2522539\n",
      "Epoch loss: 0.014516830830440672\n",
      "EPOCH 51: Accuracy: 0.9976\n",
      "Recall: 0.9345070719718933 \n",
      "tp: 84500, fn: 5922, fp: 329, tn: 2522497\n",
      "Epoch loss: 0.014719460724829624\n",
      "EPOCH 52: Accuracy: 0.9976\n",
      "Recall: 0.9354028701782227 \n",
      "tp: 84581, fn: 5841, fp: 311, tn: 2522515\n",
      "Epoch loss: 0.01434542461032731\n",
      "EPOCH 53: Accuracy: 0.9976\n",
      "Recall: 0.9346729516983032 \n",
      "tp: 84515, fn: 5907, fp: 327, tn: 2522499\n",
      "Epoch loss: 0.014699578152379739\n",
      "EPOCH 54: Accuracy: 0.9976\n",
      "Recall: 0.9343854188919067 \n",
      "tp: 84489, fn: 5933, fp: 320, tn: 2522506\n",
      "Epoch loss: 0.014512259341618717\n",
      "EPOCH 55: Accuracy: 0.9976\n",
      "Recall: 0.9340647459030151 \n",
      "tp: 84460, fn: 5962, fp: 359, tn: 2522467\n",
      "Epoch loss: 0.01483574577645168\n",
      "EPOCH 56: Accuracy: 0.9977\n",
      "Recall: 0.9375151991844177 \n",
      "tp: 84772, fn: 5650, fp: 307, tn: 2522519\n",
      "Epoch loss: 0.013660680030787884\n",
      "EPOCH 57: Accuracy: 0.9977\n",
      "Recall: 0.9374377727508545 \n",
      "tp: 84765, fn: 5657, fp: 299, tn: 2522527\n",
      "Epoch loss: 0.013672307171993145\n",
      "EPOCH 58: Accuracy: 0.9977\n",
      "Recall: 0.9384331107139587 \n",
      "tp: 84855, fn: 5567, fp: 316, tn: 2522510\n",
      "Epoch loss: 0.01358793365191026\n",
      "EPOCH 59: Accuracy: 0.9977\n",
      "Recall: 0.9371944665908813 \n",
      "tp: 84743, fn: 5679, fp: 313, tn: 2522513\n",
      "Epoch loss: 0.013873446064010017\n",
      "EPOCH 60: Accuracy: 0.9977\n",
      "Recall: 0.9366968274116516 \n",
      "tp: 84698, fn: 5724, fp: 317, tn: 2522509\n",
      "Epoch loss: 0.0141674717811599\n",
      "EPOCH 61: Accuracy: 0.9977\n",
      "Recall: 0.937869131565094 \n",
      "tp: 84804, fn: 5618, fp: 307, tn: 2522519\n",
      "Epoch loss: 0.013497194499332095\n",
      "EPOCH 62: Accuracy: 0.9978\n",
      "Recall: 0.9386211037635803 \n",
      "tp: 84872, fn: 5550, fp: 286, tn: 2522540\n",
      "Epoch loss: 0.013404697989592427\n",
      "EPOCH 63: Accuracy: 0.9977\n",
      "Recall: 0.9373161196708679 \n",
      "tp: 84754, fn: 5668, fp: 326, tn: 2522500\n",
      "Epoch loss: 0.01389951774263667\n",
      "EPOCH 64: Accuracy: 0.9978\n",
      "Recall: 0.9389971494674683 \n",
      "tp: 84906, fn: 5516, fp: 300, tn: 2522526\n",
      "Epoch loss: 0.013243344192193137\n",
      "EPOCH 65: Accuracy: 0.9978\n",
      "Recall: 0.9387427568435669 \n",
      "tp: 84883, fn: 5539, fp: 295, tn: 2522531\n",
      "Epoch loss: 0.013275793329506038\n",
      "EPOCH 66: Accuracy: 0.9978\n",
      "Recall: 0.9397159814834595 \n",
      "tp: 84971, fn: 5451, fp: 289, tn: 2522537\n",
      "Epoch loss: 0.013026987817386591\n",
      "EPOCH 67: Accuracy: 0.9979\n",
      "Recall: 0.9413970112800598 \n",
      "tp: 85123, fn: 5299, fp: 284, tn: 2522542\n",
      "Epoch loss: 0.012900263026902645\n",
      "EPOCH 68: Accuracy: 0.9978\n",
      "Recall: 0.9394616484642029 \n",
      "tp: 84948, fn: 5474, fp: 308, tn: 2522518\n",
      "Epoch loss: 0.01321288412962354\n",
      "EPOCH 69: Accuracy: 0.9979\n",
      "Recall: 0.9420605897903442 \n",
      "tp: 85183, fn: 5239, fp: 291, tn: 2522535\n",
      "Epoch loss: 0.012674456102069947\n",
      "EPOCH 70: Accuracy: 0.9978\n",
      "Recall: 0.9410431385040283 \n",
      "tp: 85091, fn: 5331, fp: 290, tn: 2522536\n",
      "Epoch loss: 0.012783048980282532\n",
      "EPOCH 71: Accuracy: 0.9979\n",
      "Recall: 0.9424254894256592 \n",
      "tp: 85216, fn: 5206, fp: 286, tn: 2522540\n",
      "Epoch loss: 0.012407308642177326\n",
      "EPOCH 72: Accuracy: 0.9979\n",
      "Recall: 0.941983163356781 \n",
      "tp: 85176, fn: 5246, fp: 293, tn: 2522533\n",
      "Epoch loss: 0.012497615266765022\n",
      "EPOCH 73: Accuracy: 0.9979\n",
      "Recall: 0.9420052766799927 \n",
      "tp: 85178, fn: 5244, fp: 293, tn: 2522533\n",
      "Epoch loss: 0.012616815261519441\n",
      "EPOCH 74: Accuracy: 0.9979\n",
      "Recall: 0.9421379566192627 \n",
      "tp: 85190, fn: 5232, fp: 277, tn: 2522549\n",
      "Epoch loss: 0.012672217733950754\n",
      "EPOCH 75: Accuracy: 0.9979\n",
      "Recall: 0.9418504238128662 \n",
      "tp: 85164, fn: 5258, fp: 308, tn: 2522518\n",
      "Epoch loss: 0.012808770935421921\n",
      "EPOCH 76: Accuracy: 0.9979\n",
      "Recall: 0.943476140499115 \n",
      "tp: 85311, fn: 5111, fp: 279, tn: 2522547\n",
      "Epoch loss: 0.012173801110575682\n",
      "EPOCH 77: Accuracy: 0.9979\n",
      "Recall: 0.9413416981697083 \n",
      "tp: 85118, fn: 5304, fp: 296, tn: 2522530\n",
      "Epoch loss: 0.012469954681028043\n",
      "EPOCH 78: Accuracy: 0.9980\n",
      "Recall: 0.9443055987358093 \n",
      "tp: 85386, fn: 5036, fp: 260, tn: 2522566\n",
      "Epoch loss: 0.011945570191750916\n",
      "EPOCH 79: Accuracy: 0.9978\n",
      "Recall: 0.940689206123352 \n",
      "tp: 85059, fn: 5363, fp: 314, tn: 2522512\n",
      "Epoch loss: 0.012901597145943374\n",
      "EPOCH 80: Accuracy: 0.9979\n",
      "Recall: 0.9428015351295471 \n",
      "tp: 85250, fn: 5172, fp: 281, tn: 2522545\n",
      "Epoch loss: 0.012356767598043274\n",
      "EPOCH 81: Accuracy: 0.9979\n",
      "Recall: 0.9426135420799255 \n",
      "tp: 85233, fn: 5189, fp: 276, tn: 2522550\n",
      "Epoch loss: 0.012452549680441233\n",
      "EPOCH 82: Accuracy: 0.9980\n",
      "Recall: 0.9444051384925842 \n",
      "tp: 85395, fn: 5027, fp: 255, tn: 2522571\n",
      "Epoch loss: 0.011769829993793116\n",
      "EPOCH 83: Accuracy: 0.9979\n",
      "Recall: 0.9426135420799255 \n",
      "tp: 85233, fn: 5189, fp: 296, tn: 2522530\n",
      "Epoch loss: 0.012527620796841164\n",
      "EPOCH 84: Accuracy: 0.9979\n",
      "Recall: 0.9422596096992493 \n",
      "tp: 85201, fn: 5221, fp: 285, tn: 2522541\n",
      "Epoch loss: 0.01228506119847356\n",
      "EPOCH 85: Accuracy: 0.9981\n",
      "Recall: 0.9467939138412476 \n",
      "tp: 85611, fn: 4811, fp: 255, tn: 2522571\n",
      "Epoch loss: 0.01124156023529694\n",
      "EPOCH 86: Accuracy: 0.9980\n",
      "Recall: 0.9439738392829895 \n",
      "tp: 85356, fn: 5066, fp: 275, tn: 2522551\n",
      "Epoch loss: 0.011909808014150196\n",
      "EPOCH 87: Accuracy: 0.9980\n",
      "Recall: 0.9454336166381836 \n",
      "tp: 85488, fn: 4934, fp: 278, tn: 2522548\n",
      "Epoch loss: 0.01183243483366287\n",
      "EPOCH 88: Accuracy: 0.9980\n",
      "Recall: 0.9463405013084412 \n",
      "tp: 85570, fn: 4852, fp: 246, tn: 2522580\n",
      "Epoch loss: 0.011252399009023282\n",
      "EPOCH 89: Accuracy: 0.9980\n",
      "Recall: 0.9456769227981567 \n",
      "tp: 85510, fn: 4912, fp: 315, tn: 2522511\n",
      "Epoch loss: 0.011867750274399909\n",
      "EPOCH 90: Accuracy: 0.9980\n",
      "Recall: 0.9444935917854309 \n",
      "tp: 85403, fn: 5019, fp: 271, tn: 2522555\n",
      "Epoch loss: 0.01168081336428461\n",
      "EPOCH 91: Accuracy: 0.9979\n",
      "Recall: 0.9438742995262146 \n",
      "tp: 85347, fn: 5075, fp: 285, tn: 2522541\n",
      "Epoch loss: 0.01202556722615714\n",
      "EPOCH 92: Accuracy: 0.9980\n",
      "Recall: 0.9454889297485352 \n",
      "tp: 85493, fn: 4929, fp: 262, tn: 2522564\n",
      "Epoch loss: 0.011491001397517268\n",
      "EPOCH 93: Accuracy: 0.9980\n",
      "Recall: 0.946163535118103 \n",
      "tp: 85554, fn: 4868, fp: 275, tn: 2522551\n",
      "Epoch loss: 0.011607823588339419\n",
      "EPOCH 94: Accuracy: 0.9980\n",
      "Recall: 0.9464510679244995 \n",
      "tp: 85580, fn: 4842, fp: 276, tn: 2522550\n",
      "Epoch loss: 0.01125390442463403\n",
      "EPOCH 95: Accuracy: 0.9980\n",
      "Recall: 0.9458538889884949 \n",
      "tp: 85526, fn: 4896, fp: 257, tn: 2522569\n",
      "Epoch loss: 0.011324884698778789\n",
      "current learning rate:0.0007\n",
      "EPOCH 96: Accuracy: 0.9981\n",
      "Recall: 0.9473026394844055 \n",
      "tp: 85657, fn: 4765, fp: 259, tn: 2522567\n",
      "Epoch loss: 0.011000744601402164\n",
      "EPOCH 97: Accuracy: 0.9980\n",
      "Recall: 0.9448585510253906 \n",
      "tp: 85436, fn: 4986, fp: 287, tn: 2522539\n",
      "Epoch loss: 0.011875241216079429\n",
      "EPOCH 98: Accuracy: 0.9980\n",
      "Recall: 0.9455663561820984 \n",
      "tp: 85500, fn: 4922, fp: 264, tn: 2522562\n",
      "Epoch loss: 0.011367590718780029\n",
      "EPOCH 99: Accuracy: 0.9981\n",
      "Recall: 0.9469266533851624 \n",
      "tp: 85623, fn: 4799, fp: 248, tn: 2522578\n",
      "Epoch loss: 0.011134775095622182\n",
      "EPOCH 100: Accuracy: 0.9980\n",
      "Recall: 0.946163535118103 \n",
      "tp: 85554, fn: 4868, fp: 275, tn: 2522551\n",
      "Epoch loss: 0.011504304637259521\n",
      "EPOCH 101: Accuracy: 0.9981\n",
      "Recall: 0.9468492269515991 \n",
      "tp: 85616, fn: 4806, fp: 246, tn: 2522580\n",
      "Epoch loss: 0.011029693009923023\n",
      "EPOCH 102: Accuracy: 0.9980\n",
      "Recall: 0.9462520480155945 \n",
      "tp: 85562, fn: 4860, fp: 277, tn: 2522549\n",
      "Epoch loss: 0.011311133568785313\n",
      "EPOCH 103: Accuracy: 0.9981\n",
      "Recall: 0.946738600730896 \n",
      "tp: 85606, fn: 4816, fp: 279, tn: 2522547\n",
      "Epoch loss: 0.011177795821637335\n",
      "EPOCH 104: Accuracy: 0.9980\n",
      "Recall: 0.9467054605484009 \n",
      "tp: 85603, fn: 4819, fp: 290, tn: 2522536\n",
      "Epoch loss: 0.011213248970387007\n",
      "EPOCH 105: Accuracy: 0.9981\n",
      "Recall: 0.9475680589675903 \n",
      "tp: 85681, fn: 4741, fp: 261, tn: 2522565\n",
      "Epoch loss: 0.010970410209172478\n",
      "EPOCH 106: Accuracy: 0.9981\n",
      "Recall: 0.9487735033035278 \n",
      "tp: 85790, fn: 4632, fp: 245, tn: 2522581\n",
      "Epoch loss: 0.010714200974900428\n",
      "EPOCH 107: Accuracy: 0.9981\n",
      "Recall: 0.9483864307403564 \n",
      "tp: 85755, fn: 4667, fp: 260, tn: 2522566\n",
      "Epoch loss: 0.010782741399760706\n",
      "EPOCH 108: Accuracy: 0.9981\n",
      "Recall: 0.9476122856140137 \n",
      "tp: 85685, fn: 4737, fp: 276, tn: 2522550\n",
      "Epoch loss: 0.01115724155230471\n",
      "EPOCH 109: Accuracy: 0.9980\n",
      "Recall: 0.945311963558197 \n",
      "tp: 85477, fn: 4945, fp: 286, tn: 2522540\n",
      "Epoch loss: 0.011408059755602974\n",
      "EPOCH 110: Accuracy: 0.9981\n",
      "Recall: 0.9484970569610596 \n",
      "tp: 85765, fn: 4657, fp: 260, tn: 2522566\n",
      "Epoch loss: 0.010776808267155645\n",
      "EPOCH 111: Accuracy: 0.9981\n",
      "Recall: 0.9483532905578613 \n",
      "tp: 85752, fn: 4670, fp: 263, tn: 2522563\n",
      "Epoch loss: 0.01100378000994175\n",
      "EPOCH 112: Accuracy: 0.9981\n",
      "Recall: 0.9484749436378479 \n",
      "tp: 85763, fn: 4659, fp: 270, tn: 2522556\n",
      "Epoch loss: 0.010682155852400198\n",
      "EPOCH 113: Accuracy: 0.9981\n",
      "Recall: 0.9492048621177673 \n",
      "tp: 85829, fn: 4593, fp: 248, tn: 2522578\n",
      "Epoch loss: 0.010702057701621649\n",
      "EPOCH 114: Accuracy: 0.9982\n",
      "Recall: 0.9500121474266052 \n",
      "tp: 85902, fn: 4520, fp: 242, tn: 2522584\n",
      "Epoch loss: 0.010270250540958033\n",
      "EPOCH 115: Accuracy: 0.9982\n",
      "Recall: 0.9508416056632996 \n",
      "tp: 85977, fn: 4445, fp: 255, tn: 2522571\n",
      "Epoch loss: 0.010308711936436067\n",
      "EPOCH 116: Accuracy: 0.9981\n",
      "Recall: 0.9488509297370911 \n",
      "tp: 85797, fn: 4625, fp: 252, tn: 2522574\n",
      "Epoch loss: 0.010589048530991104\n",
      "EPOCH 117: Accuracy: 0.9982\n",
      "Recall: 0.9495587348937988 \n",
      "tp: 85861, fn: 4561, fp: 253, tn: 2522573\n",
      "Epoch loss: 0.01052176335862645\n",
      "EPOCH 118: Accuracy: 0.9982\n",
      "Recall: 0.949735701084137 \n",
      "tp: 85877, fn: 4545, fp: 265, tn: 2522561\n",
      "Epoch loss: 0.0104825673099362\n",
      "EPOCH 119: Accuracy: 0.9982\n",
      "Recall: 0.9508305788040161 \n",
      "tp: 85976, fn: 4446, fp: 256, tn: 2522570\n",
      "Epoch loss: 0.010094445821724038\n",
      "EPOCH 120: Accuracy: 0.9982\n",
      "Recall: 0.9508969187736511 \n",
      "tp: 85982, fn: 4440, fp: 233, tn: 2522593\n",
      "Epoch loss: 0.010087865118673042\n",
      "EPOCH 121: Accuracy: 0.9982\n",
      "Recall: 0.950587272644043 \n",
      "tp: 85954, fn: 4468, fp: 262, tn: 2522564\n",
      "Epoch loss: 0.010346740719987821\n",
      "EPOCH 122: Accuracy: 0.9981\n",
      "Recall: 0.9488730430603027 \n",
      "tp: 85799, fn: 4623, fp: 256, tn: 2522570\n",
      "Epoch loss: 0.010902198308167927\n",
      "EPOCH 123: Accuracy: 0.9981\n",
      "Recall: 0.9489946961402893 \n",
      "tp: 85810, fn: 4612, fp: 267, tn: 2522559\n",
      "Epoch loss: 0.010698031928415081\n",
      "EPOCH 124: Accuracy: 0.9982\n",
      "Recall: 0.949425995349884 \n",
      "tp: 85849, fn: 4573, fp: 249, tn: 2522577\n",
      "Epoch loss: 0.010345760049724654\n",
      "EPOCH 125: Accuracy: 0.9982\n",
      "Recall: 0.9497909545898438 \n",
      "tp: 85882, fn: 4540, fp: 252, tn: 2522574\n",
      "Epoch loss: 0.010401677401943664\n",
      "EPOCH 126: Accuracy: 0.9982\n",
      "Recall: 0.9499015808105469 \n",
      "tp: 85892, fn: 4530, fp: 284, tn: 2522542\n",
      "Epoch loss: 0.010773584784919448\n",
      "EPOCH 127: Accuracy: 0.9984\n",
      "Recall: 0.9552652835845947 \n",
      "tp: 86377, fn: 4045, fp: 144, tn: 2522682\n",
      "Epoch loss: 0.008213248174472604\n",
      "EPOCH 128: Accuracy: 0.9985\n",
      "Recall: 0.9575656652450562 \n",
      "tp: 86585, fn: 3837, fp: 159, tn: 2522667\n",
      "Epoch loss: 0.007871263494808519\n",
      "EPOCH 129: Accuracy: 0.9985\n",
      "Recall: 0.9590254426002502 \n",
      "tp: 86717, fn: 3705, fp: 168, tn: 2522658\n",
      "Epoch loss: 0.0076558054864573\n",
      "EPOCH 130: Accuracy: 0.9986\n",
      "Recall: 0.9599544405937195 \n",
      "tp: 86801, fn: 3621, fp: 168, tn: 2522658\n",
      "Epoch loss: 0.007521669373296122\n",
      "EPOCH 131: Accuracy: 0.9986\n",
      "Recall: 0.9607064723968506 \n",
      "tp: 86869, fn: 3553, fp: 167, tn: 2522659\n",
      "Epoch loss: 0.007531187049938704\n",
      "EPOCH 132: Accuracy: 0.9986\n",
      "Recall: 0.9604963660240173 \n",
      "tp: 86850, fn: 3572, fp: 183, tn: 2522643\n",
      "Epoch loss: 0.007722512830550954\n",
      "EPOCH 133: Accuracy: 0.9986\n",
      "Recall: 0.960308313369751 \n",
      "tp: 86833, fn: 3589, fp: 178, tn: 2522648\n",
      "Epoch loss: 0.00758701425244985\n",
      "EPOCH 134: Accuracy: 0.9986\n",
      "Recall: 0.9603304266929626 \n",
      "tp: 86835, fn: 3587, fp: 162, tn: 2522664\n",
      "Epoch loss: 0.00740454483970337\n",
      "EPOCH 135: Accuracy: 0.9986\n",
      "Recall: 0.9605405926704407 \n",
      "tp: 86854, fn: 3568, fp: 190, tn: 2522636\n",
      "Epoch loss: 0.007590405551842954\n",
      "EPOCH 136: Accuracy: 0.9986\n",
      "Recall: 0.9604189395904541 \n",
      "tp: 86843, fn: 3579, fp: 183, tn: 2522643\n",
      "Epoch loss: 0.0075546478150853285\n",
      "EPOCH 137: Accuracy: 0.9986\n",
      "Recall: 0.9615580439567566 \n",
      "tp: 86946, fn: 3476, fp: 175, tn: 2522651\n",
      "Epoch loss: 0.007346619731457582\n",
      "EPOCH 138: Accuracy: 0.9986\n",
      "Recall: 0.9623211026191711 \n",
      "tp: 87015, fn: 3407, fp: 166, tn: 2522660\n",
      "Epoch loss: 0.007208525976381443\n",
      "EPOCH 139: Accuracy: 0.9986\n",
      "Recall: 0.9619893431663513 \n",
      "tp: 86985, fn: 3437, fp: 177, tn: 2522649\n",
      "Epoch loss: 0.007369319125710692\n",
      "EPOCH 140: Accuracy: 0.9986\n",
      "Recall: 0.9622105360031128 \n",
      "tp: 87005, fn: 3417, fp: 185, tn: 2522641\n",
      "Epoch loss: 0.007400926260195883\n",
      "EPOCH 141: Accuracy: 0.9986\n",
      "Recall: 0.9622436761856079 \n",
      "tp: 87008, fn: 3414, fp: 168, tn: 2522658\n",
      "Epoch loss: 0.007395446095320025\n",
      "EPOCH 142: Accuracy: 0.9986\n",
      "Recall: 0.9627745747566223 \n",
      "tp: 87056, fn: 3366, fp: 172, tn: 2522654\n",
      "Epoch loss: 0.007086199981530509\n",
      "EPOCH 143: Accuracy: 0.9986\n",
      "Recall: 0.9617460370063782 \n",
      "tp: 86963, fn: 3459, fp: 174, tn: 2522652\n",
      "Epoch loss: 0.0073069334154230395\n",
      "EPOCH 144: Accuracy: 0.9986\n",
      "Recall: 0.962586522102356 \n",
      "tp: 87039, fn: 3383, fp: 162, tn: 2522664\n",
      "Epoch loss: 0.007193168660543339\n",
      "EPOCH 145: Accuracy: 0.9987\n",
      "Recall: 0.9630178213119507 \n",
      "tp: 87078, fn: 3344, fp: 161, tn: 2522665\n",
      "Epoch loss: 0.00710764141581065\n",
      "EPOCH 146: Accuracy: 0.9986\n",
      "Recall: 0.9628188014030457 \n",
      "tp: 87060, fn: 3362, fp: 189, tn: 2522637\n",
      "Epoch loss: 0.007199280925308935\n",
      "EPOCH 147: Accuracy: 0.9986\n",
      "Recall: 0.9621331095695496 \n",
      "tp: 86998, fn: 3424, fp: 167, tn: 2522659\n",
      "Epoch loss: 0.0071626173142200706\n",
      "EPOCH 148: Accuracy: 0.9987\n",
      "Recall: 0.9637588262557983 \n",
      "tp: 87145, fn: 3277, fp: 172, tn: 2522654\n",
      "Epoch loss: 0.007194707814427414\n",
      "EPOCH 149: Accuracy: 0.9987\n",
      "Recall: 0.9630289077758789 \n",
      "tp: 87079, fn: 3343, fp: 172, tn: 2522654\n",
      "Epoch loss: 0.007039761408046034\n",
      "EPOCH 150: Accuracy: 0.9986\n",
      "Recall: 0.9627192616462708 \n",
      "tp: 87051, fn: 3371, fp: 161, tn: 2522665\n",
      "Epoch loss: 0.007158762369367665\n",
      "EPOCH 151: Accuracy: 0.9986\n",
      "Recall: 0.9627192616462708 \n",
      "tp: 87051, fn: 3371, fp: 183, tn: 2522643\n",
      "Epoch loss: 0.0072637162104520515\n",
      "EPOCH 152: Accuracy: 0.9986\n",
      "Recall: 0.9619230031967163 \n",
      "tp: 86979, fn: 3443, fp: 197, tn: 2522629\n",
      "Epoch loss: 0.0074270759336212445\n",
      "EPOCH 153: Accuracy: 0.9987\n",
      "Recall: 0.9631726741790771 \n",
      "tp: 87092, fn: 3330, fp: 157, tn: 2522669\n",
      "Epoch loss: 0.006917025282061922\n",
      "EPOCH 154: Accuracy: 0.9987\n",
      "Recall: 0.9632501006126404 \n",
      "tp: 87099, fn: 3323, fp: 167, tn: 2522659\n",
      "Epoch loss: 0.007032342508656402\n",
      "EPOCH 155: Accuracy: 0.9986\n",
      "Recall: 0.9627413749694824 \n",
      "tp: 87053, fn: 3369, fp: 173, tn: 2522653\n",
      "Epoch loss: 0.007160371123374437\n",
      "EPOCH 156: Accuracy: 0.9987\n",
      "Recall: 0.9642785787582397 \n",
      "tp: 87192, fn: 3230, fp: 158, tn: 2522668\n",
      "Epoch loss: 0.006865718986052249\n",
      "EPOCH 157: Accuracy: 0.9987\n",
      "Recall: 0.9633164405822754 \n",
      "tp: 87105, fn: 3317, fp: 175, tn: 2522651\n",
      "Epoch loss: 0.007145305569239872\n",
      "EPOCH 158: Accuracy: 0.9987\n",
      "Recall: 0.9639247059822083 \n",
      "tp: 87160, fn: 3262, fp: 165, tn: 2522661\n",
      "Epoch loss: 0.006918527188836407\n",
      "EPOCH 159: Accuracy: 0.9987\n",
      "Recall: 0.963714599609375 \n",
      "tp: 87141, fn: 3281, fp: 173, tn: 2522653\n",
      "Epoch loss: 0.006839270690858559\n",
      "EPOCH 160: Accuracy: 0.9987\n",
      "Recall: 0.9633496403694153 \n",
      "tp: 87108, fn: 3314, fp: 175, tn: 2522651\n",
      "Epoch loss: 0.0070576520727364915\n",
      "EPOCH 161: Accuracy: 0.9987\n",
      "Recall: 0.9643338918685913 \n",
      "tp: 87197, fn: 3225, fp: 156, tn: 2522670\n",
      "Epoch loss: 0.006680567145054185\n",
      "EPOCH 162: Accuracy: 0.9987\n",
      "Recall: 0.9646214246749878 \n",
      "tp: 87223, fn: 3199, fp: 186, tn: 2522640\n",
      "Epoch loss: 0.006980673294267354\n",
      "EPOCH 163: Accuracy: 0.9987\n",
      "Recall: 0.9644666314125061 \n",
      "tp: 87209, fn: 3213, fp: 171, tn: 2522655\n",
      "Epoch loss: 0.006846723962647455\n",
      "EPOCH 164: Accuracy: 0.9987\n",
      "Recall: 0.9638915061950684 \n",
      "tp: 87157, fn: 3265, fp: 147, tn: 2522679\n",
      "Epoch loss: 0.006766361142815234\n",
      "EPOCH 165: Accuracy: 0.9987\n",
      "Recall: 0.9653071165084839 \n",
      "tp: 87285, fn: 3137, fp: 162, tn: 2522664\n",
      "Epoch loss: 0.0067759771506721005\n",
      "EPOCH 166: Accuracy: 0.9987\n",
      "Recall: 0.9643338918685913 \n",
      "tp: 87197, fn: 3225, fp: 148, tn: 2522678\n",
      "Epoch loss: 0.0067589694121706545\n",
      "EPOCH 167: Accuracy: 0.9987\n",
      "Recall: 0.9656057357788086 \n",
      "tp: 87312, fn: 3110, fp: 170, tn: 2522656\n",
      "Epoch loss: 0.0066933471106064325\n",
      "EPOCH 168: Accuracy: 0.9987\n",
      "Recall: 0.9650306105613708 \n",
      "tp: 87260, fn: 3162, fp: 171, tn: 2522655\n",
      "Epoch loss: 0.006732373830200345\n",
      "EPOCH 169: Accuracy: 0.9987\n",
      "Recall: 0.9649089574813843 \n",
      "tp: 87249, fn: 3173, fp: 162, tn: 2522664\n",
      "Epoch loss: 0.0066973747259303695\n",
      "EPOCH 170: Accuracy: 0.9987\n",
      "Recall: 0.9650306105613708 \n",
      "tp: 87260, fn: 3162, fp: 159, tn: 2522667\n",
      "Epoch loss: 0.006660973344277007\n",
      "EPOCH 171: Accuracy: 0.9987\n",
      "Recall: 0.9652296900749207 \n",
      "tp: 87278, fn: 3144, fp: 162, tn: 2522664\n",
      "Epoch loss: 0.006637256454109276\n",
      "EPOCH 172: Accuracy: 0.9987\n",
      "Recall: 0.9652407765388489 \n",
      "tp: 87279, fn: 3143, fp: 180, tn: 2522646\n",
      "Epoch loss: 0.006919478555465391\n",
      "EPOCH 173: Accuracy: 0.9987\n",
      "Recall: 0.965484082698822 \n",
      "tp: 87301, fn: 3121, fp: 155, tn: 2522671\n",
      "Epoch loss: 0.006448291760426289\n",
      "EPOCH 174: Accuracy: 0.9987\n",
      "Recall: 0.9651301503181458 \n",
      "tp: 87269, fn: 3153, fp: 206, tn: 2522620\n",
      "Epoch loss: 0.007039852387988561\n",
      "EPOCH 175: Accuracy: 0.9988\n",
      "Recall: 0.9661697149276733 \n",
      "tp: 87363, fn: 3059, fp: 154, tn: 2522672\n",
      "Epoch loss: 0.0066095474218989085\n",
      "EPOCH 176: Accuracy: 0.9987\n",
      "Recall: 0.9654508829116821 \n",
      "tp: 87298, fn: 3124, fp: 154, tn: 2522672\n",
      "Epoch loss: 0.006553511202839337\n",
      "EPOCH 177: Accuracy: 0.9987\n",
      "Recall: 0.9649200439453125 \n",
      "tp: 87250, fn: 3172, fp: 174, tn: 2522652\n",
      "Epoch loss: 0.006763788195327654\n",
      "EPOCH 178: Accuracy: 0.9987\n",
      "Recall: 0.9654508829116821 \n",
      "tp: 87298, fn: 3124, fp: 160, tn: 2522666\n",
      "Epoch loss: 0.00653068163431451\n",
      "EPOCH 179: Accuracy: 0.9988\n",
      "Recall: 0.9660923480987549 \n",
      "tp: 87356, fn: 3066, fp: 165, tn: 2522661\n",
      "Epoch loss: 0.006468891286762682\n",
      "EPOCH 180: Accuracy: 0.9987\n",
      "Recall: 0.965141236782074 \n",
      "tp: 87270, fn: 3152, fp: 187, tn: 2522639\n",
      "Epoch loss: 0.006882670910705646\n",
      "EPOCH 181: Accuracy: 0.9988\n",
      "Recall: 0.9658490419387817 \n",
      "tp: 87334, fn: 3088, fp: 146, tn: 2522680\n",
      "Epoch loss: 0.006536391967061621\n",
      "EPOCH 182: Accuracy: 0.9987\n",
      "Recall: 0.9654508829116821 \n",
      "tp: 87298, fn: 3124, fp: 167, tn: 2522659\n",
      "Epoch loss: 0.006494498235698305\n",
      "EPOCH 183: Accuracy: 0.9988\n",
      "Recall: 0.9658821821212769 \n",
      "tp: 87337, fn: 3085, fp: 171, tn: 2522655\n",
      "Epoch loss: 0.0066154188015585804\n",
      "EPOCH 184: Accuracy: 0.9987\n",
      "Recall: 0.9647541642189026 \n",
      "tp: 87235, fn: 3187, fp: 158, tn: 2522668\n",
      "Epoch loss: 0.006518486369004164\n",
      "EPOCH 185: Accuracy: 0.9988\n",
      "Recall: 0.9660038352012634 \n",
      "tp: 87348, fn: 3074, fp: 162, tn: 2522664\n",
      "Epoch loss: 0.006619362432624981\n",
      "current learning rate:0.00020999999999999998\n",
      "EPOCH 186: Accuracy: 0.9988\n",
      "Recall: 0.9657716155052185 \n",
      "tp: 87327, fn: 3095, fp: 154, tn: 2522672\n",
      "Epoch loss: 0.006455817286579849\n",
      "current learning rate:6.299999999999999e-05\n",
      "EPOCH 187: Accuracy: 0.9988\n",
      "Recall: 0.9677290916442871 \n",
      "tp: 87504, fn: 2918, fp: 137, tn: 2522689\n",
      "Epoch loss: 0.005823296913457451\n",
      "EPOCH 188: Accuracy: 0.9989\n",
      "Recall: 0.9685364365577698 \n",
      "tp: 87577, fn: 2845, fp: 110, tn: 2522716\n",
      "Epoch loss: 0.005508851976503201\n",
      "EPOCH 189: Accuracy: 0.9989\n",
      "Recall: 0.9687797427177429 \n",
      "tp: 87599, fn: 2823, fp: 111, tn: 2522715\n",
      "Epoch loss: 0.005456276856854392\n",
      "EPOCH 190: Accuracy: 0.9989\n",
      "Recall: 0.969188928604126 \n",
      "tp: 87636, fn: 2786, fp: 117, tn: 2522709\n",
      "Epoch loss: 0.005626529037170908\n",
      "EPOCH 191: Accuracy: 0.9989\n",
      "Recall: 0.9689677357673645 \n",
      "tp: 87616, fn: 2806, fp: 125, tn: 2522701\n",
      "Epoch loss: 0.005533753629962893\n",
      "EPOCH 192: Accuracy: 0.9989\n",
      "Recall: 0.9692441821098328 \n",
      "tp: 87641, fn: 2781, fp: 111, tn: 2522715\n",
      "Epoch loss: 0.0054131669964353766\n",
      "EPOCH 193: Accuracy: 0.9989\n",
      "Recall: 0.9691225290298462 \n",
      "tp: 87630, fn: 2792, fp: 123, tn: 2522703\n",
      "Epoch loss: 0.005419775044952803\n",
      "EPOCH 194: Accuracy: 0.9989\n",
      "Recall: 0.9691004157066345 \n",
      "tp: 87628, fn: 2794, fp: 147, tn: 2522679\n",
      "Epoch loss: 0.005544461158870976\n",
      "EPOCH 195: Accuracy: 0.9989\n",
      "Recall: 0.9707261323928833 \n",
      "tp: 87775, fn: 2647, fp: 126, tn: 2522700\n",
      "Epoch loss: 0.005261670563590412\n",
      "EPOCH 196: Accuracy: 0.9989\n",
      "Recall: 0.9703059196472168 \n",
      "tp: 87737, fn: 2685, fp: 134, tn: 2522692\n",
      "Epoch loss: 0.005311452643781591\n",
      "EPOCH 197: Accuracy: 0.9989\n",
      "Recall: 0.9695096611976624 \n",
      "tp: 87665, fn: 2757, fp: 140, tn: 2522686\n",
      "Epoch loss: 0.0055050247878552965\n",
      "EPOCH 198: Accuracy: 0.9989\n",
      "Recall: 0.9705713391304016 \n",
      "tp: 87761, fn: 2661, fp: 113, tn: 2522713\n",
      "Epoch loss: 0.0052221561405263245\n",
      "EPOCH 199: Accuracy: 0.9989\n",
      "Recall: 0.9702284932136536 \n",
      "tp: 87730, fn: 2692, fp: 114, tn: 2522712\n",
      "Epoch loss: 0.005233419062757681\n",
      "EPOCH 200: Accuracy: 0.9989\n",
      "Recall: 0.9706487655639648 \n",
      "tp: 87768, fn: 2654, fp: 113, tn: 2522713\n",
      "Epoch loss: 0.005199739185878445\n",
      "EPOCH 201: Accuracy: 0.9989\n",
      "Recall: 0.9708920121192932 \n",
      "tp: 87790, fn: 2632, fp: 122, tn: 2522704\n",
      "Epoch loss: 0.005106226510994522\n",
      "EPOCH 202: Accuracy: 0.9989\n",
      "Recall: 0.9706929922103882 \n",
      "tp: 87772, fn: 2650, fp: 112, tn: 2522714\n",
      "Epoch loss: 0.0052320897052952064\n",
      "EPOCH 203: Accuracy: 0.9990\n",
      "Recall: 0.9712901711463928 \n",
      "tp: 87826, fn: 2596, fp: 122, tn: 2522704\n",
      "Epoch loss: 0.005166049171252366\n",
      "EPOCH 204: Accuracy: 0.9990\n",
      "Recall: 0.9713565111160278 \n",
      "tp: 87832, fn: 2590, fp: 109, tn: 2522717\n",
      "Epoch loss: 0.005146295255877379\n",
      "EPOCH 205: Accuracy: 0.9989\n",
      "Recall: 0.9710358381271362 \n",
      "tp: 87803, fn: 2619, fp: 135, tn: 2522691\n",
      "Epoch loss: 0.005401919618888311\n",
      "EPOCH 206: Accuracy: 0.9989\n",
      "Recall: 0.9710026383399963 \n",
      "tp: 87800, fn: 2622, fp: 127, tn: 2522699\n",
      "Epoch loss: 0.0052458631903149165\n",
      "EPOCH 207: Accuracy: 0.9989\n",
      "Recall: 0.9705492258071899 \n",
      "tp: 87759, fn: 2663, fp: 127, tn: 2522699\n",
      "Epoch loss: 0.005368101699233966\n",
      "EPOCH 208: Accuracy: 0.9990\n",
      "Recall: 0.9711242914199829 \n",
      "tp: 87811, fn: 2611, fp: 120, tn: 2522706\n",
      "Epoch loss: 0.005121704085821616\n",
      "EPOCH 209: Accuracy: 0.9989\n",
      "Recall: 0.970615565776825 \n",
      "tp: 87765, fn: 2657, fp: 119, tn: 2522707\n",
      "Epoch loss: 0.0052051774150769295\n",
      "EPOCH 210: Accuracy: 0.9990\n",
      "Recall: 0.9712901711463928 \n",
      "tp: 87826, fn: 2596, fp: 114, tn: 2522712\n",
      "Epoch loss: 0.00521194791789152\n",
      "EPOCH 211: Accuracy: 0.9990\n",
      "Recall: 0.9716219305992126 \n",
      "tp: 87856, fn: 2566, fp: 110, tn: 2522716\n",
      "Epoch loss: 0.005080693177962793\n",
      "EPOCH 212: Accuracy: 0.9989\n",
      "Recall: 0.9708146452903748 \n",
      "tp: 87783, fn: 2639, fp: 125, tn: 2522701\n",
      "Epoch loss: 0.005185280429684889\n",
      "EPOCH 213: Accuracy: 0.9990\n",
      "Recall: 0.9715002775192261 \n",
      "tp: 87845, fn: 2577, fp: 127, tn: 2522699\n",
      "Epoch loss: 0.005187611152263954\n",
      "EPOCH 214: Accuracy: 0.9990\n",
      "Recall: 0.9714560508728027 \n",
      "tp: 87841, fn: 2581, fp: 119, tn: 2522707\n",
      "Epoch loss: 0.005140259637691513\n",
      "EPOCH 215: Accuracy: 0.9990\n",
      "Recall: 0.9716109037399292 \n",
      "tp: 87855, fn: 2567, fp: 130, tn: 2522696\n",
      "Epoch loss: 0.0051838358761823\n",
      "EPOCH 216: Accuracy: 0.9990\n",
      "Recall: 0.9714229106903076 \n",
      "tp: 87838, fn: 2584, fp: 135, tn: 2522691\n",
      "Epoch loss: 0.005191176876843232\n",
      "EPOCH 217: Accuracy: 0.9990\n",
      "Recall: 0.9720864295959473 \n",
      "tp: 87898, fn: 2524, fp: 124, tn: 2522702\n",
      "Epoch loss: 0.005066967956217438\n",
      "EPOCH 218: Accuracy: 0.9990\n",
      "Recall: 0.9713565111160278 \n",
      "tp: 87832, fn: 2590, fp: 131, tn: 2522695\n",
      "Epoch loss: 0.005268113441255489\n",
      "EPOCH 219: Accuracy: 0.9990\n",
      "Recall: 0.9713897109031677 \n",
      "tp: 87835, fn: 2587, fp: 119, tn: 2522707\n",
      "Epoch loss: 0.005023376205613104\n",
      "EPOCH 220: Accuracy: 0.9990\n",
      "Recall: 0.9718652367591858 \n",
      "tp: 87878, fn: 2544, fp: 116, tn: 2522710\n",
      "Epoch loss: 0.00490548039305164\n",
      "EPOCH 221: Accuracy: 0.9990\n",
      "Recall: 0.9716440439224243 \n",
      "tp: 87858, fn: 2564, fp: 126, tn: 2522700\n",
      "Epoch loss: 0.0051172992444380415\n",
      "EPOCH 222: Accuracy: 0.9990\n",
      "Recall: 0.9720975160598755 \n",
      "tp: 87899, fn: 2523, fp: 116, tn: 2522710\n",
      "Epoch loss: 0.00497229545370626\n",
      "EPOCH 223: Accuracy: 0.9990\n",
      "Recall: 0.9719316363334656 \n",
      "tp: 87884, fn: 2538, fp: 121, tn: 2522705\n",
      "Epoch loss: 0.0051129096207540295\n",
      "EPOCH 224: Accuracy: 0.9990\n",
      "Recall: 0.9720864295959473 \n",
      "tp: 87898, fn: 2524, fp: 116, tn: 2522710\n",
      "Epoch loss: 0.005073483371484928\n",
      "EPOCH 225: Accuracy: 0.9990\n",
      "Recall: 0.9725398421287537 \n",
      "tp: 87939, fn: 2483, fp: 105, tn: 2522721\n",
      "Epoch loss: 0.004879441201872248\n",
      "EPOCH 226: Accuracy: 0.9990\n",
      "Recall: 0.9722191691398621 \n",
      "tp: 87910, fn: 2512, fp: 137, tn: 2522689\n",
      "Epoch loss: 0.0051232752540560445\n",
      "EPOCH 227: Accuracy: 0.9990\n",
      "Recall: 0.9726946949958801 \n",
      "tp: 87953, fn: 2469, fp: 119, tn: 2522707\n",
      "Epoch loss: 0.0049543800106167\n",
      "EPOCH 228: Accuracy: 0.9990\n",
      "Recall: 0.9726725816726685 \n",
      "tp: 87951, fn: 2471, fp: 114, tn: 2522712\n",
      "Epoch loss: 0.00493049297320655\n",
      "EPOCH 229: Accuracy: 0.9990\n",
      "Recall: 0.9721085429191589 \n",
      "tp: 87900, fn: 2522, fp: 124, tn: 2522702\n",
      "Epoch loss: 0.0050735973528050595\n",
      "EPOCH 230: Accuracy: 0.9990\n",
      "Recall: 0.9721527695655823 \n",
      "tp: 87904, fn: 2518, fp: 123, tn: 2522703\n",
      "Epoch loss: 0.005003879537684026\n",
      "EPOCH 231: Accuracy: 0.9990\n",
      "Recall: 0.9722080826759338 \n",
      "tp: 87909, fn: 2513, fp: 131, tn: 2522695\n",
      "Epoch loss: 0.004857491705901293\n",
      "EPOCH 232: Accuracy: 0.9990\n",
      "Recall: 0.9720643162727356 \n",
      "tp: 87896, fn: 2526, fp: 122, tn: 2522704\n",
      "Epoch loss: 0.005102695501993372\n",
      "EPOCH 233: Accuracy: 0.9990\n",
      "Recall: 0.9720864295959473 \n",
      "tp: 87898, fn: 2524, fp: 118, tn: 2522708\n",
      "Epoch loss: 0.00499922855895671\n",
      "EPOCH 234: Accuracy: 0.9990\n",
      "Recall: 0.9722965359687805 \n",
      "tp: 87917, fn: 2505, fp: 120, tn: 2522706\n",
      "Epoch loss: 0.005014387925821838\n",
      "EPOCH 235: Accuracy: 0.9990\n",
      "Recall: 0.9730264544487 \n",
      "tp: 87983, fn: 2439, fp: 115, tn: 2522711\n",
      "Epoch loss: 0.0048721560633224565\n",
      "EPOCH 236: Accuracy: 0.9990\n",
      "Recall: 0.9724071621894836 \n",
      "tp: 87927, fn: 2495, fp: 133, tn: 2522693\n",
      "Epoch loss: 0.005070559390062221\n",
      "EPOCH 237: Accuracy: 0.9990\n",
      "Recall: 0.9727057814598083 \n",
      "tp: 87954, fn: 2468, fp: 116, tn: 2522710\n",
      "Epoch loss: 0.005006772307255728\n",
      "EPOCH 238: Accuracy: 0.9990\n",
      "Recall: 0.9728052616119385 \n",
      "tp: 87963, fn: 2459, fp: 128, tn: 2522698\n",
      "Epoch loss: 0.004894054937103418\n",
      "EPOCH 239: Accuracy: 0.9990\n",
      "Recall: 0.9719316363334656 \n",
      "tp: 87884, fn: 2538, fp: 113, tn: 2522713\n",
      "Epoch loss: 0.004908302876735938\n",
      "EPOCH 240: Accuracy: 0.9990\n",
      "Recall: 0.9730043411254883 \n",
      "tp: 87981, fn: 2441, fp: 113, tn: 2522713\n",
      "Epoch loss: 0.004980731981939294\n",
      "EPOCH 241: Accuracy: 0.9990\n",
      "Recall: 0.9725067019462585 \n",
      "tp: 87936, fn: 2486, fp: 123, tn: 2522703\n",
      "Epoch loss: 0.004982432928327712\n",
      "EPOCH 242: Accuracy: 0.9990\n",
      "Recall: 0.9730154275894165 \n",
      "tp: 87982, fn: 2440, fp: 114, tn: 2522712\n",
      "Epoch loss: 0.005008338629656769\n",
      "EPOCH 243: Accuracy: 0.9990\n",
      "Recall: 0.9729490876197815 \n",
      "tp: 87976, fn: 2446, fp: 143, tn: 2522683\n",
      "Epoch loss: 0.004943004307985966\n",
      "EPOCH 244: Accuracy: 0.9990\n",
      "Recall: 0.9730928540229797 \n",
      "tp: 87989, fn: 2433, fp: 110, tn: 2522716\n",
      "Epoch loss: 0.004880265828253421\n",
      "EPOCH 245: Accuracy: 0.9990\n",
      "Recall: 0.9732145071029663 \n",
      "tp: 88000, fn: 2422, fp: 101, tn: 2522725\n",
      "Epoch loss: 0.004789486329144576\n",
      "EPOCH 246: Accuracy: 0.9990\n",
      "Recall: 0.9730817675590515 \n",
      "tp: 87988, fn: 2434, fp: 116, tn: 2522710\n",
      "Epoch loss: 0.004829174675513424\n",
      "EPOCH 247: Accuracy: 0.9990\n",
      "Recall: 0.9727389216423035 \n",
      "tp: 87957, fn: 2465, fp: 108, tn: 2522718\n",
      "Epoch loss: 0.004911596312400736\n",
      "EPOCH 248: Accuracy: 0.9990\n",
      "Recall: 0.9724403619766235 \n",
      "tp: 87930, fn: 2492, fp: 127, tn: 2522699\n",
      "Epoch loss: 0.004932138412819699\n",
      "EPOCH 249: Accuracy: 0.9990\n",
      "Recall: 0.9736347198486328 \n",
      "tp: 88038, fn: 2384, fp: 121, tn: 2522705\n",
      "Epoch loss: 0.004814724371519282\n",
      "EPOCH 250: Accuracy: 0.9990\n",
      "Recall: 0.9733250737190247 \n",
      "tp: 88010, fn: 2412, fp: 115, tn: 2522711\n",
      "Epoch loss: 0.004855684707914538\n",
      "EPOCH 251: Accuracy: 0.9990\n",
      "Recall: 0.9735130667686462 \n",
      "tp: 88027, fn: 2395, fp: 111, tn: 2522715\n",
      "Epoch loss: 0.004850580268152654\n",
      "EPOCH 252: Accuracy: 0.9990\n",
      "Recall: 0.9725288152694702 \n",
      "tp: 87938, fn: 2484, fp: 114, tn: 2522712\n",
      "Epoch loss: 0.004826656058855825\n",
      "EPOCH 253: Accuracy: 0.9990\n",
      "Recall: 0.9729712009429932 \n",
      "tp: 87978, fn: 2444, fp: 128, tn: 2522698\n",
      "Epoch loss: 0.004915036297438675\n",
      "EPOCH 254: Accuracy: 0.9990\n",
      "Recall: 0.9734246134757996 \n",
      "tp: 88019, fn: 2403, fp: 123, tn: 2522703\n",
      "Epoch loss: 0.004778735587708389\n",
      "EPOCH 255: Accuracy: 0.9990\n",
      "Recall: 0.9728052616119385 \n",
      "tp: 87963, fn: 2459, fp: 125, tn: 2522701\n",
      "Epoch loss: 0.0049239197905762205\n",
      "EPOCH 256: Accuracy: 0.9990\n",
      "Recall: 0.9731813073158264 \n",
      "tp: 87997, fn: 2425, fp: 114, tn: 2522712\n",
      "Epoch loss: 0.004867254455978391\n",
      "EPOCH 257: Accuracy: 0.9990\n",
      "Recall: 0.9729933142662048 \n",
      "tp: 87980, fn: 2442, fp: 130, tn: 2522696\n",
      "Epoch loss: 0.004787478543831499\n",
      "EPOCH 258: Accuracy: 0.9990\n",
      "Recall: 0.9730043411254883 \n",
      "tp: 87981, fn: 2441, fp: 121, tn: 2522705\n",
      "Epoch loss: 0.00487747791344205\n",
      "EPOCH 259: Accuracy: 0.9990\n",
      "Recall: 0.9738005995750427 \n",
      "tp: 88053, fn: 2369, fp: 117, tn: 2522709\n",
      "Epoch loss: 0.004759064276896028\n",
      "EPOCH 260: Accuracy: 0.9991\n",
      "Recall: 0.9741324186325073 \n",
      "tp: 88083, fn: 2339, fp: 105, tn: 2522721\n",
      "Epoch loss: 0.00457686457398795\n",
      "EPOCH 261: Accuracy: 0.9990\n",
      "Recall: 0.9735351800918579 \n",
      "tp: 88029, fn: 2393, fp: 122, tn: 2522704\n",
      "Epoch loss: 0.004734307830851469\n",
      "EPOCH 262: Accuracy: 0.9990\n",
      "Recall: 0.9731370806694031 \n",
      "tp: 87993, fn: 2429, fp: 133, tn: 2522693\n",
      "Epoch loss: 0.005002784980984688\n",
      "EPOCH 263: Accuracy: 0.9990\n",
      "Recall: 0.9729048013687134 \n",
      "tp: 87972, fn: 2450, fp: 141, tn: 2522685\n",
      "Epoch loss: 0.005013981711115399\n",
      "EPOCH 264: Accuracy: 0.9990\n",
      "Recall: 0.9733361601829529 \n",
      "tp: 88011, fn: 2411, fp: 116, tn: 2522710\n",
      "Epoch loss: 0.004742346839694821\n",
      "EPOCH 265: Accuracy: 0.9990\n",
      "Recall: 0.9732034206390381 \n",
      "tp: 87999, fn: 2423, fp: 128, tn: 2522698\n",
      "Epoch loss: 0.004909935961590589\n",
      "EPOCH 266: Accuracy: 0.9990\n",
      "Recall: 0.973712146282196 \n",
      "tp: 88045, fn: 2377, fp: 112, tn: 2522714\n",
      "Epoch loss: 0.004803562894748921\n",
      "EPOCH 267: Accuracy: 0.9991\n",
      "Recall: 0.9741766452789307 \n",
      "tp: 88087, fn: 2335, fp: 111, tn: 2522715\n",
      "Epoch loss: 0.00463376050863523\n",
      "EPOCH 268: Accuracy: 0.9990\n",
      "Recall: 0.9732697606086731 \n",
      "tp: 88005, fn: 2417, fp: 134, tn: 2522692\n",
      "Epoch loss: 0.0049007317461117195\n",
      "EPOCH 269: Accuracy: 0.9990\n",
      "Recall: 0.9736789464950562 \n",
      "tp: 88042, fn: 2380, fp: 105, tn: 2522721\n",
      "Epoch loss: 0.004558039327390378\n",
      "EPOCH 270: Accuracy: 0.9991\n",
      "Recall: 0.9742319583892822 \n",
      "tp: 88092, fn: 2330, fp: 120, tn: 2522706\n",
      "Epoch loss: 0.00472013022952062\n",
      "EPOCH 271: Accuracy: 0.9990\n",
      "Recall: 0.9734025001525879 \n",
      "tp: 88017, fn: 2405, fp: 108, tn: 2522718\n",
      "Epoch loss: 0.004595958935677112\n",
      "EPOCH 272: Accuracy: 0.9991\n",
      "Recall: 0.974497377872467 \n",
      "tp: 88116, fn: 2306, fp: 116, tn: 2522710\n",
      "Epoch loss: 0.004645281777700568\n",
      "EPOCH 273: Accuracy: 0.9990\n",
      "Recall: 0.9737342596054077 \n",
      "tp: 88047, fn: 2375, fp: 129, tn: 2522697\n",
      "Epoch loss: 0.004789210634215203\n",
      "EPOCH 274: Accuracy: 0.9991\n",
      "Recall: 0.973878026008606 \n",
      "tp: 88060, fn: 2362, fp: 106, tn: 2522720\n",
      "Epoch loss: 0.004669260724573936\n",
      "EPOCH 275: Accuracy: 0.9990\n",
      "Recall: 0.973645806312561 \n",
      "tp: 88039, fn: 2383, fp: 131, tn: 2522695\n",
      "Epoch loss: 0.0048048912348318045\n",
      "EPOCH 276: Accuracy: 0.9990\n",
      "Recall: 0.9737895727157593 \n",
      "tp: 88052, fn: 2370, fp: 113, tn: 2522713\n",
      "Epoch loss: 0.004717834705219029\n",
      "EPOCH 277: Accuracy: 0.9991\n",
      "Recall: 0.9740992188453674 \n",
      "tp: 88080, fn: 2342, fp: 116, tn: 2522710\n",
      "Epoch loss: 0.004568134667808015\n",
      "EPOCH 278: Accuracy: 0.9991\n",
      "Recall: 0.9742650985717773 \n",
      "tp: 88095, fn: 2327, fp: 114, tn: 2522712\n",
      "Epoch loss: 0.004668937212963469\n",
      "EPOCH 279: Accuracy: 0.9991\n",
      "Recall: 0.9741876721382141 \n",
      "tp: 88088, fn: 2334, fp: 119, tn: 2522707\n",
      "Epoch loss: 0.00470745623767519\n",
      "EPOCH 280: Accuracy: 0.9991\n",
      "Recall: 0.9740660190582275 \n",
      "tp: 88077, fn: 2345, fp: 129, tn: 2522697\n",
      "Epoch loss: 0.0047575707555030326\n",
      "EPOCH 281: Accuracy: 0.9991\n",
      "Recall: 0.9744862914085388 \n",
      "tp: 88115, fn: 2307, fp: 118, tn: 2522708\n",
      "Epoch loss: 0.0045814135777743955\n",
      "EPOCH 282: Accuracy: 0.9991\n",
      "Recall: 0.9739554524421692 \n",
      "tp: 88067, fn: 2355, fp: 124, tn: 2522702\n",
      "Epoch loss: 0.004792857395445265\n",
      "current learning rate:6.299999999999999e-05\n",
      "EPOCH 283: Accuracy: 0.9991\n",
      "Recall: 0.9739996790885925 \n",
      "tp: 88071, fn: 2351, fp: 114, tn: 2522712\n",
      "Epoch loss: 0.004495853389695259\n",
      "current learning rate:1.8899999999999995e-05\n",
      "EPOCH 284: Accuracy: 0.9991\n",
      "Recall: 0.975581169128418 \n",
      "tp: 88214, fn: 2208, fp: 109, tn: 2522717\n",
      "Epoch loss: 0.004417819697524823\n",
      "EPOCH 285: Accuracy: 0.9991\n",
      "Recall: 0.9752604365348816 \n",
      "tp: 88185, fn: 2237, fp: 117, tn: 2522709\n",
      "Epoch loss: 0.004447421851521577\n",
      "EPOCH 286: Accuracy: 0.9991\n",
      "Recall: 0.9746190309524536 \n",
      "tp: 88127, fn: 2295, fp: 95, tn: 2522731\n",
      "Epoch loss: 0.0043555055671502165\n",
      "EPOCH 287: Accuracy: 0.9991\n",
      "Recall: 0.9747406840324402 \n",
      "tp: 88138, fn: 2284, fp: 97, tn: 2522729\n",
      "Epoch loss: 0.004246476876639198\n",
      "EPOCH 288: Accuracy: 0.9991\n",
      "Recall: 0.9748070240020752 \n",
      "tp: 88144, fn: 2278, fp: 99, tn: 2522727\n",
      "Epoch loss: 0.004316061493016628\n",
      "EPOCH 289: Accuracy: 0.9991\n",
      "Recall: 0.974663257598877 \n",
      "tp: 88131, fn: 2291, fp: 89, tn: 2522737\n",
      "Epoch loss: 0.004274491258985567\n",
      "EPOCH 290: Accuracy: 0.9991\n",
      "Recall: 0.975006103515625 \n",
      "tp: 88162, fn: 2260, fp: 109, tn: 2522717\n",
      "Epoch loss: 0.004301034277408102\n",
      "EPOCH 291: Accuracy: 0.9991\n",
      "Recall: 0.9752715229988098 \n",
      "tp: 88186, fn: 2236, fp: 97, tn: 2522729\n",
      "Epoch loss: 0.004156238991147867\n",
      "EPOCH 292: Accuracy: 0.9991\n",
      "Recall: 0.975006103515625 \n",
      "tp: 88162, fn: 2260, fp: 95, tn: 2522731\n",
      "Epoch loss: 0.004309759672859254\n",
      "EPOCH 293: Accuracy: 0.9991\n",
      "Recall: 0.9758355021476746 \n",
      "tp: 88237, fn: 2185, fp: 72, tn: 2522754\n",
      "Epoch loss: 0.00406456492801283\n",
      "EPOCH 294: Accuracy: 0.9991\n",
      "Recall: 0.9754705429077148 \n",
      "tp: 88204, fn: 2218, fp: 85, tn: 2522741\n",
      "Epoch loss: 0.004146020173988901\n",
      "EPOCH 295: Accuracy: 0.9991\n",
      "Recall: 0.9754263162612915 \n",
      "tp: 88200, fn: 2222, fp: 101, tn: 2522725\n",
      "Epoch loss: 0.004186155991203613\n",
      "EPOCH 296: Accuracy: 0.9991\n",
      "Recall: 0.9755590558052063 \n",
      "tp: 88212, fn: 2210, fp: 97, tn: 2522729\n",
      "Epoch loss: 0.004391218751919277\n",
      "EPOCH 297: Accuracy: 0.9991\n",
      "Recall: 0.9754042029380798 \n",
      "tp: 88198, fn: 2224, fp: 103, tn: 2522723\n",
      "Epoch loss: 0.004310756155717053\n",
      "EPOCH 298: Accuracy: 0.9991\n",
      "Recall: 0.974663257598877 \n",
      "tp: 88131, fn: 2291, fp: 82, tn: 2522744\n",
      "Epoch loss: 0.004090806552983359\n",
      "EPOCH 299: Accuracy: 0.9991\n",
      "Recall: 0.9757912755012512 \n",
      "tp: 88233, fn: 2189, fp: 91, tn: 2522735\n",
      "Epoch loss: 0.004193258624924216\n",
      "EPOCH 300: Accuracy: 0.9991\n",
      "Recall: 0.9758687019348145 \n",
      "tp: 88240, fn: 2182, fp: 102, tn: 2522724\n",
      "Epoch loss: 0.004195608018910612\n",
      "EPOCH 301: Accuracy: 0.9991\n",
      "Recall: 0.9756696224212646 \n",
      "tp: 88222, fn: 2200, fp: 95, tn: 2522731\n",
      "Epoch loss: 0.004278363646475196\n",
      "EPOCH 302: Accuracy: 0.9991\n",
      "Recall: 0.9753599762916565 \n",
      "tp: 88194, fn: 2228, fp: 100, tn: 2522726\n",
      "Epoch loss: 0.004181513230419528\n",
      "EPOCH 303: Accuracy: 0.9991\n",
      "Recall: 0.975990355014801 \n",
      "tp: 88251, fn: 2171, fp: 94, tn: 2522732\n",
      "Epoch loss: 0.004139059667524241\n",
      "EPOCH 304: Accuracy: 0.9991\n",
      "Recall: 0.9758908152580261 \n",
      "tp: 88242, fn: 2180, fp: 91, tn: 2522735\n",
      "Epoch loss: 0.004072084613716439\n",
      "EPOCH 305: Accuracy: 0.9991\n",
      "Recall: 0.9754374027252197 \n",
      "tp: 88201, fn: 2221, fp: 95, tn: 2522731\n",
      "Epoch loss: 0.004255447920576413\n",
      "EPOCH 306: Accuracy: 0.9991\n",
      "Recall: 0.9761562347412109 \n",
      "tp: 88266, fn: 2156, fp: 91, tn: 2522735\n",
      "Epoch loss: 0.004103224330352179\n",
      "EPOCH 307: Accuracy: 0.9991\n",
      "Recall: 0.9753488898277283 \n",
      "tp: 88193, fn: 2229, fp: 102, tn: 2522724\n",
      "Epoch loss: 0.004153780858225823\n",
      "EPOCH 308: Accuracy: 0.9991\n",
      "Recall: 0.9763774275779724 \n",
      "tp: 88286, fn: 2136, fp: 87, tn: 2522739\n",
      "Epoch loss: 0.004049289872486482\n",
      "EPOCH 309: Accuracy: 0.9991\n",
      "Recall: 0.9759461283683777 \n",
      "tp: 88247, fn: 2175, fp: 99, tn: 2522727\n",
      "Epoch loss: 0.0041494207008472474\n",
      "EPOCH 310: Accuracy: 0.9991\n",
      "Recall: 0.9759019017219543 \n",
      "tp: 88243, fn: 2179, fp: 115, tn: 2522711\n",
      "Epoch loss: 0.004296468188281851\n",
      "EPOCH 311: Accuracy: 0.9991\n",
      "Recall: 0.9760235548019409 \n",
      "tp: 88254, fn: 2168, fp: 93, tn: 2522733\n",
      "Epoch loss: 0.004135578812383185\n",
      "EPOCH 312: Accuracy: 0.9992\n",
      "Recall: 0.9764548540115356 \n",
      "tp: 88293, fn: 2129, fp: 91, tn: 2522735\n",
      "Epoch loss: 0.0041726792801327\n",
      "EPOCH 313: Accuracy: 0.9991\n",
      "Recall: 0.9750282168388367 \n",
      "tp: 88164, fn: 2258, fp: 94, tn: 2522732\n",
      "Epoch loss: 0.004127339916219818\n",
      "EPOCH 314: Accuracy: 0.9991\n",
      "Recall: 0.9763110876083374 \n",
      "tp: 88280, fn: 2142, fp: 93, tn: 2522733\n",
      "Epoch loss: 0.004162473958791697\n",
      "EPOCH 315: Accuracy: 0.9991\n",
      "Recall: 0.9761009216308594 \n",
      "tp: 88261, fn: 2161, fp: 98, tn: 2522728\n",
      "Epoch loss: 0.0039997103529709295\n",
      "EPOCH 316: Accuracy: 0.9991\n",
      "Recall: 0.9759461283683777 \n",
      "tp: 88247, fn: 2175, fp: 103, tn: 2522723\n",
      "Epoch loss: 0.0041524075997335\n",
      "EPOCH 317: Accuracy: 0.9992\n",
      "Recall: 0.9764548540115356 \n",
      "tp: 88293, fn: 2129, fp: 83, tn: 2522743\n",
      "Epoch loss: 0.004026009551202732\n",
      "EPOCH 318: Accuracy: 0.9991\n",
      "Recall: 0.9763442277908325 \n",
      "tp: 88283, fn: 2139, fp: 93, tn: 2522733\n",
      "Epoch loss: 0.00416592345522829\n",
      "EPOCH 319: Accuracy: 0.9992\n",
      "Recall: 0.976775586605072 \n",
      "tp: 88322, fn: 2100, fp: 76, tn: 2522750\n",
      "Epoch loss: 0.003917311241992218\n",
      "EPOCH 320: Accuracy: 0.9991\n",
      "Recall: 0.9758908152580261 \n",
      "tp: 88242, fn: 2180, fp: 93, tn: 2522733\n",
      "Epoch loss: 0.0040611669149864344\n",
      "EPOCH 321: Accuracy: 0.9991\n",
      "Recall: 0.9763774275779724 \n",
      "tp: 88286, fn: 2136, fp: 100, tn: 2522726\n",
      "Epoch loss: 0.004150818003802108\n",
      "EPOCH 322: Accuracy: 0.9991\n",
      "Recall: 0.9762115478515625 \n",
      "tp: 88271, fn: 2151, fp: 96, tn: 2522730\n",
      "Epoch loss: 0.004133919518993205\n",
      "EPOCH 323: Accuracy: 0.9991\n",
      "Recall: 0.9762889742851257 \n",
      "tp: 88278, fn: 2144, fp: 96, tn: 2522730\n",
      "Epoch loss: 0.004186855362590292\n",
      "EPOCH 324: Accuracy: 0.9991\n",
      "Recall: 0.9765986204147339 \n",
      "tp: 88306, fn: 2116, fp: 116, tn: 2522710\n",
      "Epoch loss: 0.004113809300384405\n",
      "EPOCH 325: Accuracy: 0.9992\n",
      "Recall: 0.9763553142547607 \n",
      "tp: 88284, fn: 2138, fp: 77, tn: 2522749\n",
      "Epoch loss: 0.003934550741959296\n",
      "EPOCH 326: Accuracy: 0.9991\n",
      "Recall: 0.9762004613876343 \n",
      "tp: 88270, fn: 2152, fp: 99, tn: 2522727\n",
      "Epoch loss: 0.004192905728583429\n",
      "EPOCH 327: Accuracy: 0.9991\n",
      "Recall: 0.9756032824516296 \n",
      "tp: 88216, fn: 2206, fp: 91, tn: 2522735\n",
      "Epoch loss: 0.00419714189496557\n",
      "EPOCH 328: Accuracy: 0.9991\n",
      "Recall: 0.9764548540115356 \n",
      "tp: 88293, fn: 2129, fp: 93, tn: 2522733\n",
      "Epoch loss: 0.004110054547030739\n",
      "EPOCH 329: Accuracy: 0.9991\n",
      "Recall: 0.9765543937683105 \n",
      "tp: 88302, fn: 2120, fp: 103, tn: 2522723\n",
      "Epoch loss: 0.00401641163817347\n",
      "current learning rate:1.8899999999999995e-05\n",
      "EPOCH 330: Accuracy: 0.9992\n",
      "Recall: 0.9768972396850586 \n",
      "tp: 88333, fn: 2089, fp: 112, tn: 2522714\n",
      "Epoch loss: 0.004192896603768492\n",
      "EPOCH 331: Accuracy: 0.9991\n",
      "Recall: 0.9761562347412109 \n",
      "tp: 88266, fn: 2156, fp: 96, tn: 2522730\n",
      "Epoch loss: 0.00404770707063447\n",
      "EPOCH 332: Accuracy: 0.9992\n",
      "Recall: 0.9765211939811707 \n",
      "tp: 88299, fn: 2123, fp: 95, tn: 2522731\n",
      "Epoch loss: 0.0040557945001090156\n",
      "EPOCH 333: Accuracy: 0.9991\n",
      "Recall: 0.9763110876083374 \n",
      "tp: 88280, fn: 2142, fp: 105, tn: 2522721\n",
      "Epoch loss: 0.004120820349779018\n",
      "EPOCH 334: Accuracy: 0.9992\n",
      "Recall: 0.976974606513977 \n",
      "tp: 88340, fn: 2082, fp: 110, tn: 2522716\n",
      "Epoch loss: 0.004174775112059761\n",
      "EPOCH 335: Accuracy: 0.9991\n",
      "Recall: 0.9761341214179993 \n",
      "tp: 88264, fn: 2158, fp: 102, tn: 2522724\n",
      "Epoch loss: 0.003989517355699393\n",
      "EPOCH 336: Accuracy: 0.9992\n",
      "Recall: 0.97728431224823 \n",
      "tp: 88368, fn: 2054, fp: 93, tn: 2522733\n",
      "Epoch loss: 0.003844891270693044\n",
      "EPOCH 337: Accuracy: 0.9992\n",
      "Recall: 0.9773064255714417 \n",
      "tp: 88370, fn: 2052, fp: 93, tn: 2522733\n",
      "Epoch loss: 0.003965594516928398\n",
      "EPOCH 338: Accuracy: 0.9992\n",
      "Recall: 0.9765875339508057 \n",
      "tp: 88305, fn: 2117, fp: 100, tn: 2522726\n",
      "Epoch loss: 0.004128041647115159\n",
      "EPOCH 339: Accuracy: 0.9992\n",
      "Recall: 0.9766981601715088 \n",
      "tp: 88315, fn: 2107, fp: 96, tn: 2522730\n",
      "Epoch loss: 0.004043924871901106\n",
      "EPOCH 340: Accuracy: 0.9991\n",
      "Recall: 0.9761120080947876 \n",
      "tp: 88262, fn: 2160, fp: 112, tn: 2522714\n",
      "Epoch loss: 0.004158955389602481\n",
      "EPOCH 341: Accuracy: 0.9992\n",
      "Recall: 0.9766096472740173 \n",
      "tp: 88307, fn: 2115, fp: 92, tn: 2522734\n",
      "Epoch loss: 0.003977347725731733\n",
      "EPOCH 342: Accuracy: 0.9992\n",
      "Recall: 0.9766981601715088 \n",
      "tp: 88315, fn: 2107, fp: 91, tn: 2522735\n",
      "Epoch loss: 0.0040593296176541175\n",
      "EPOCH 343: Accuracy: 0.9992\n",
      "Recall: 0.9766870737075806 \n",
      "tp: 88314, fn: 2108, fp: 89, tn: 2522737\n",
      "Epoch loss: 0.003847134870476656\n",
      "EPOCH 344: Accuracy: 0.9992\n",
      "Recall: 0.9765875339508057 \n",
      "tp: 88305, fn: 2117, fp: 98, tn: 2522728\n",
      "Epoch loss: 0.004054062738990905\n",
      "EPOCH 345: Accuracy: 0.9991\n",
      "Recall: 0.9765543937683105 \n",
      "tp: 88302, fn: 2120, fp: 118, tn: 2522708\n",
      "Epoch loss: 0.0043126753022581146\n",
      "EPOCH 346: Accuracy: 0.9992\n",
      "Recall: 0.9768087267875671 \n",
      "tp: 88325, fn: 2097, fp: 97, tn: 2522729\n",
      "Epoch loss: 0.004068503395475806\n",
      "EPOCH 347: Accuracy: 0.9992\n",
      "Recall: 0.9768308401107788 \n",
      "tp: 88327, fn: 2095, fp: 97, tn: 2522729\n",
      "Epoch loss: 0.004024976007969718\n",
      "current learning rate:5.669999999999998e-06\n",
      "EPOCH 348: Accuracy: 0.9992\n",
      "Recall: 0.9767645001411438 \n",
      "tp: 88321, fn: 2101, fp: 87, tn: 2522739\n",
      "Epoch loss: 0.0038925098639556724\n",
      "current learning rate:5.669999999999998e-06\n",
      "EPOCH 349: Accuracy: 0.9992\n",
      "Recall: 0.9767866134643555 \n",
      "tp: 88323, fn: 2099, fp: 87, tn: 2522739\n",
      "Epoch loss: 0.003876655931599317\n",
      "current learning rate:5.669999999999998e-06\n",
      "EPOCH 350: Accuracy: 0.9992\n",
      "Recall: 0.9767976999282837 \n",
      "tp: 88324, fn: 2098, fp: 95, tn: 2522731\n",
      "Epoch loss: 0.00396214562931155\n",
      "current learning rate:5.669999999999998e-06\n",
      "EPOCH 351: Accuracy: 0.9992\n",
      "Recall: 0.977074146270752 \n",
      "tp: 88349, fn: 2073, fp: 66, tn: 2522760\n",
      "Epoch loss: 0.0037079503747242644\n",
      "current learning rate:5.669999999999998e-06\n",
      "EPOCH 352: Accuracy: 0.9992\n",
      "Recall: 0.9765433073043823 \n",
      "tp: 88301, fn: 2121, fp: 85, tn: 2522741\n",
      "Epoch loss: 0.003879540004622597\n",
      "current learning rate:5.669999999999998e-06\n",
      "EPOCH 353: Accuracy: 0.9992\n",
      "Recall: 0.9771294593811035 \n",
      "tp: 88354, fn: 2068, fp: 85, tn: 2522741\n",
      "Epoch loss: 0.0038867948790661825\n",
      "current learning rate:5.669999999999998e-06\n",
      "EPOCH 354: Accuracy: 0.9992\n",
      "Recall: 0.9773727655410767 \n",
      "tp: 88376, fn: 2046, fp: 97, tn: 2522729\n",
      "Epoch loss: 0.003966997846109621\n",
      "EPOCH 355: Accuracy: 0.9992\n",
      "Recall: 0.9771626591682434 \n",
      "tp: 88357, fn: 2065, fp: 105, tn: 2522721\n",
      "Epoch loss: 0.0039253468175643475\n",
      "EPOCH 356: Accuracy: 0.9992\n",
      "Recall: 0.9765875339508057 \n",
      "tp: 88305, fn: 2117, fp: 86, tn: 2522740\n",
      "Epoch loss: 0.003984297476460731\n",
      "EPOCH 357: Accuracy: 0.9992\n",
      "Recall: 0.977074146270752 \n",
      "tp: 88349, fn: 2073, fp: 88, tn: 2522738\n",
      "Epoch loss: 0.004004261676829891\n",
      "EPOCH 358: Accuracy: 0.9992\n",
      "Recall: 0.9772732257843018 \n",
      "tp: 88367, fn: 2055, fp: 85, tn: 2522741\n",
      "Epoch loss: 0.003858850865174511\n",
      "EPOCH 359: Accuracy: 0.9992\n",
      "Recall: 0.9772511124610901 \n",
      "tp: 88365, fn: 2057, fp: 96, tn: 2522730\n",
      "Epoch loss: 0.004003642279165541\n",
      "EPOCH 360: Accuracy: 0.9992\n",
      "Recall: 0.9768972396850586 \n",
      "tp: 88333, fn: 2089, fp: 98, tn: 2522728\n",
      "Epoch loss: 0.004023476802970416\n",
      "EPOCH 361: Accuracy: 0.9992\n",
      "Recall: 0.9776492714881897 \n",
      "tp: 88401, fn: 2021, fp: 83, tn: 2522743\n",
      "Epoch loss: 0.003862504724511781\n",
      "EPOCH 362: Accuracy: 0.9992\n",
      "Recall: 0.9770188927650452 \n",
      "tp: 88344, fn: 2078, fp: 97, tn: 2522729\n",
      "Epoch loss: 0.003930932799406017\n",
      "EPOCH 363: Accuracy: 0.9992\n",
      "Recall: 0.9771957993507385 \n",
      "tp: 88360, fn: 2062, fp: 96, tn: 2522730\n",
      "Epoch loss: 0.004005609836301994\n",
      "EPOCH 364: Accuracy: 0.9992\n",
      "Recall: 0.9767976999282837 \n",
      "tp: 88324, fn: 2098, fp: 100, tn: 2522726\n",
      "Epoch loss: 0.003958189388029832\n",
      "EPOCH 365: Accuracy: 0.9992\n",
      "Recall: 0.9775054454803467 \n",
      "tp: 88388, fn: 2034, fp: 87, tn: 2522739\n",
      "Epoch loss: 0.003853092503580294\n",
      "EPOCH 366: Accuracy: 0.9992\n",
      "Recall: 0.9771957993507385 \n",
      "tp: 88360, fn: 2062, fp: 85, tn: 2522741\n",
      "Epoch loss: 0.0038588198233391437\n",
      "EPOCH 367: Accuracy: 0.9992\n",
      "Recall: 0.9770962595939636 \n",
      "tp: 88351, fn: 2071, fp: 97, tn: 2522729\n",
      "Epoch loss: 0.003978052834108352\n",
      "EPOCH 368: Accuracy: 0.9992\n",
      "Recall: 0.9773285388946533 \n",
      "tp: 88372, fn: 2050, fp: 93, tn: 2522733\n",
      "Epoch loss: 0.003856217571236609\n",
      "EPOCH 369: Accuracy: 0.9992\n",
      "Recall: 0.9773285388946533 \n",
      "tp: 88372, fn: 2050, fp: 84, tn: 2522742\n",
      "Epoch loss: 0.0038141406837546633\n",
      "EPOCH 370: Accuracy: 0.9992\n",
      "Recall: 0.9764879941940308 \n",
      "tp: 88296, fn: 2126, fp: 88, tn: 2522738\n",
      "Epoch loss: 0.003999466559854059\n",
      "EPOCH 371: Accuracy: 0.9992\n",
      "Recall: 0.9768972396850586 \n",
      "tp: 88333, fn: 2089, fp: 101, tn: 2522725\n",
      "Epoch loss: 0.004049536645347486\n",
      "current learning rate:1.7009999999999995e-06\n",
      "EPOCH 372: Accuracy: 0.9992\n",
      "Recall: 0.977074146270752 \n",
      "tp: 88349, fn: 2073, fp: 85, tn: 2522741\n",
      "Epoch loss: 0.003944807298233124\n",
      "current learning rate:5.102999999999998e-07\n",
      "EPOCH 373: Accuracy: 0.9992\n",
      "Recall: 0.9766207337379456 \n",
      "tp: 88308, fn: 2114, fp: 95, tn: 2522731\n",
      "Epoch loss: 0.0039030762634632036\n",
      "current learning rate:5.102999999999998e-07\n",
      "EPOCH 374: Accuracy: 0.9992\n",
      "Recall: 0.9772953391075134 \n",
      "tp: 88369, fn: 2053, fp: 90, tn: 2522736\n",
      "Epoch loss: 0.0038906382033703584\n",
      "current learning rate:5.102999999999998e-07\n",
      "EPOCH 375: Accuracy: 0.9992\n",
      "Recall: 0.9773838520050049 \n",
      "tp: 88377, fn: 2045, fp: 107, tn: 2522719\n",
      "Epoch loss: 0.003930778290956357\n",
      "current learning rate:5.102999999999998e-07\n",
      "EPOCH 376: Accuracy: 0.9992\n",
      "Recall: 0.9770188927650452 \n",
      "tp: 88344, fn: 2078, fp: 92, tn: 2522734\n",
      "Epoch loss: 0.003940314832606045\n",
      "current learning rate:5.102999999999998e-07\n",
      "EPOCH 377: Accuracy: 0.9992\n",
      "Recall: 0.9769524931907654 \n",
      "tp: 88338, fn: 2084, fp: 87, tn: 2522739\n",
      "Epoch loss: 0.003910856756186814\n",
      "current learning rate:5.102999999999998e-07\n",
      "EPOCH 378: Accuracy: 0.9992\n",
      "Recall: 0.9770078063011169 \n",
      "tp: 88343, fn: 2079, fp: 88, tn: 2522738\n",
      "Epoch loss: 0.0037978549735043286\n",
      "current learning rate:5.102999999999998e-07\n",
      "EPOCH 379: Accuracy: 0.9991\n",
      "Recall: 0.9763553142547607 \n",
      "tp: 88284, fn: 2138, fp: 84, tn: 2522742\n",
      "Epoch loss: 0.004043611710693879\n",
      "current learning rate:5.102999999999998e-07\n",
      "EPOCH 380: Accuracy: 0.9992\n",
      "Recall: 0.977350652217865 \n",
      "tp: 88374, fn: 2048, fp: 106, tn: 2522720\n",
      "Epoch loss: 0.0039000212626400708\n",
      "current learning rate:5.102999999999998e-07\n",
      "EPOCH 381: Accuracy: 0.9992\n",
      "Recall: 0.9772511124610901 \n",
      "tp: 88365, fn: 2057, fp: 81, tn: 2522745\n",
      "Epoch loss: 0.0038847601209343104\n",
      "Early stopping triggered at epoch 381.\n",
      "Time of training: 6673.99 seconds\n",
      "Max recall: 0.9776\n"
     ]
    }
   ],
   "source": [
    "model, loss_batch, loss_epoch, learniung_rate_list = network_training_detect(model, train_loader, criterion, optimizer, num_epochs, code_word, path_to_save, interpolate = False, sequence = True, add_noise = True, patience = patience, std = std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3132adf0-1e39-49c1-98e5-e2b7678b9455",
   "metadata": {},
   "source": [
    "Evaluation of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4581c6c5-e099-470d-a134-1b80f3cf6267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1279d47c190>]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510b93eb-1f2c-4ec5-809e-203c142a91c5",
   "metadata": {},
   "source": [
    "We can see that training went smoothly with training loss decreasing close to 0 assymptotycally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e03b3d08-73da-40dc-956b-7ef5486b19be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x127525e76d0>]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(learniung_rate_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6a6622-cf55-41cc-8be8-b79db949a2c3",
   "metadata": {},
   "source": [
    "Here you can see how learning rate scheduler changed the learning rate several times which caused corresponding speedups that we see on the loss function plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0fe238df-db26-4a4d-b000-7d0c90d60a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_correctly_classified(pred, label, original_input, temp_threshold, slack):\n",
    "    \"\"\"\n",
    "    Check if the predicted peaks are correctly classified based on temperature difference\n",
    "    and spatial proximity, ensuring unique matches.\n",
    "    \"\"\"\n",
    "    count_targets = (label == 1).sum().item()  # Count of target peaks\n",
    "    count_detections = (pred == 1).sum().item()  # Count of detected peaks\n",
    "    #if (count_targets + 1 < count_detections) or (count_targets > count_detections):\n",
    "    #    return False, False  # Early return if the counts don't match\n",
    "\n",
    "    # Initialize a mask to track matched predictions\n",
    "    matched_preds = torch.zeros_like(pred, dtype=torch.bool)\n",
    "    \n",
    "    # This will store the minimum distance match for each target\n",
    "    best_match = {}\n",
    "\n",
    "    for i in range(label.shape[0]):  \n",
    "        for j in range(label.shape[1]):\n",
    "            if label[i, j] == 1:  # Target peak\n",
    "                min_distance = float('inf')\n",
    "                best_idx = None\n",
    "                min_i, max_i = max(0, i-slack), min(label.shape[0], i+slack+1)\n",
    "                min_j, max_j = max(0, j-slack), min(label.shape[1], j+slack+1)\n",
    "                \n",
    "                for mi in range(min_i, max_i):\n",
    "                    for mj in range(min_j, max_j):\n",
    "                        if pred[mi, mj] == 1 and not matched_preds[mi, mj]:\n",
    "                            #distance to choose the closest prediction only\n",
    "                            distance = (mi - i)**2 + (mj - j)**2\n",
    "                            temp_diff = abs(original_input[mi, mj] - original_input[i, j])\n",
    "                            if temp_diff <= temp_threshold and distance < min_distance:\n",
    "                                min_distance = distance\n",
    "                                best_idx = (mi, mj)\n",
    "                \n",
    "                if best_idx:\n",
    "                    matched_preds[best_idx[0], best_idx[1]] = True\n",
    "                    best_match[(i, j)] = best_idx\n",
    "\n",
    "    # Check if all targets have been matched uniquely\n",
    "    targets_detected = len(best_match)\n",
    "    correctly_classified = (targets_detected == count_targets and count_targets == count_detections)\n",
    "    return correctly_classified\n",
    "\n",
    "\n",
    "\n",
    "def evaluation_of_detection_joint(net, test_loader, device, margin_of_error=0, interpolate=False, temp_threshold=0.5, sequence = False, slack = 1):\n",
    "    net.eval()\n",
    "\n",
    "    # Initialize metrics\n",
    "    total_pixels = 0\n",
    "    correct_pixels = 0\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "    image_level_correct_relaxed = 0\n",
    "    image_level_correct = 0\n",
    "    misclassified_samples = []\n",
    "    labels = []\n",
    "    mis_idxs = []\n",
    "    tolerables = 0\n",
    "    tolearable = False\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(test_loader):\n",
    "            if sequence:\n",
    "                original_inputs = inputs[:,-1,:,:,:].clone().squeeze(1)\n",
    "            else:\n",
    "                original_inputs = inputs.clone().squeeze(1)\n",
    "            if interpolate:\n",
    "                inputs = F.interpolate(inputs, size=(16, 16), mode='bilinear')\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = net(inputs)\n",
    "            outputs = torch.sigmoid(outputs).squeeze(1)\n",
    "\n",
    "            # Threshold outputs to get binary predictions\n",
    "            preds = (outputs > 0.5).to(device)#1,8,8\n",
    "\n",
    "            for idx in range(preds.size(0)):  # Loop through batch\n",
    "                pred = preds[idx]  #8,8\n",
    "                label = labels[idx] #8,8\n",
    "                original_input = original_inputs[idx].to(device) #8,8\n",
    "\n",
    "                # Image-level accuracy with margin of error\n",
    "                diff = torch.abs(pred.float() - label.float())\n",
    "                if torch.all(diff <= margin_of_error):\n",
    "                    image_level_correct += 1\n",
    "\n",
    "                # Image-level accuracy with slack, allowing the predicted location be a little bit different than target one\n",
    "                correctly_classified = is_correctly_classified(pred, label, original_input, temp_threshold, slack)\n",
    "                if correctly_classified:\n",
    "                    image_level_correct_relaxed += 1\n",
    "                else:\n",
    "                    misclassified_samples.append((i, original_input.cpu(), pred.cpu(), label.cpu()))\n",
    "            # Update pixel-level metrics\n",
    "            correct_pixels += torch.sum(preds == labels).item()\n",
    "            total_pixels += labels.numel()\n",
    "\n",
    "            # Update true positives, false positives, and false negatives\n",
    "            true_positives += torch.sum((preds == 1) & (labels == 1)).item()\n",
    "            false_positives += torch.sum((preds == 1) & (labels == 0)).item()\n",
    "            false_negatives += torch.sum((preds == 0) & (labels == 1)).item()\n",
    "\n",
    "    pixel_accuracy = correct_pixels / total_pixels\n",
    "    pixel_recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives else 0\n",
    "    pixel_precision = true_positives / (true_positives + false_positives) if true_positives + false_positives else 0\n",
    "    image_level_accuracy = image_level_correct / len(test_loader.dataset)\n",
    "    image_level_accuracy_relaxed = image_level_correct_relaxed / len(test_loader.dataset)\n",
    "    \n",
    "    print(f\"Pixel-Level Accuracy: {pixel_accuracy}\")\n",
    "    print(f\"Pixel-Level Recall: {pixel_recall}\")\n",
    "    print(f\"Pixel-Level Precision: {pixel_precision}\")\n",
    "    print(f\"Image-Level Accuracy: {image_level_accuracy}\")\n",
    "    print(f\"Image-Level Accuracy Relaxed: {image_level_accuracy_relaxed}\")\n",
    "\n",
    "    return {\n",
    "        'pixel_accuracy': pixel_accuracy,\n",
    "        'pixel_recall': pixel_recall,\n",
    "        'pixel_precision': pixel_precision,\n",
    "        'image_level_accuracy': image_level_accuracy,\n",
    "        'image_level_accuracy_relaxed': image_level_accuracy_relaxed,\n",
    "        'misclassified_samples': misclassified_samples\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0addd728-2b3a-4254-af0c-1e016aa71fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixel-Level Accuracy: 0.9988999619446295\n",
      "Pixel-Level Recall: 0.9795500024403339\n",
      "Pixel-Level Precision: 0.9842577607768133\n",
      "Image-Level Accuracy: 0.95100371039863\n",
      "Image-Level Accuracy Relaxed: 0.9669869660355818\n"
     ]
    }
   ],
   "source": [
    "results_dict = evaluation_of_detection_joint(model, test_loader, device, margin_of_error=0, interpolate=False, temp_threshold=0.5, sequence = True, slack = 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
